<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="panliang's blog."><meta name="keywords" content="Linux,Kubernetes,Docker,Microservices"><title>Kubernetes 排错指南：集群【转载】 | 每天进步一点点……</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kubernetes 排错指南：集群【转载】</h1><a id="logo" href="/.">每天进步一点点……</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Kubernetes 排错指南：集群【转载】</h1><div class="post-meta"><a href="/2018/05/07/troubleshooting-cluster.html#comments" class="comment-count"></a><p><span class="date">May 07, 2018</span><span><a href="/categories/Kubernetes/" class="category">Kubernetes</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>本章介绍集群状态异常的排错方法，包括 Kubernetes 主要组件以及必备扩展（如 kube-dns）等，而有关网络的异常排错请参考<a href="network.md">网络异常排错方法</a>。</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>排查集群状态异常问题通常从 Node 和 Kubernetes 服务 的状态出发，定位出具体的异常服务，再进而寻找解决方法。集群状态异常可能的原因比较多，常见的有</p>
<ul>
<li>虚拟机或物理机宕机</li>
<li>网络分区</li>
<li>Kubernetes 服务未正常启动</li>
<li>数据丢失或持久化存储不可用（一般在公有云或私有云平台中）</li>
<li>操作失误（如配置错误）</li>
</ul>
<p>按照不同的组件来说，具体的原因可能包括</p>
<ul>
<li>kube-apiserver 无法启动会导致<ul>
<li>集群不可访问</li>
<li>已有的 Pod 和服务正常运行（依赖于 Kubernetes API 的除外）</li>
</ul>
</li>
<li>etcd 集群异常会导致<ul>
<li>kube-apiserver 无法正常读写集群状态，进而导致 Kubernetes API 访问出错</li>
<li>kubelet 无法周期性更新状态</li>
</ul>
</li>
<li>kube-controller-manager/kube-scheduler 异常会导致<ul>
<li>复制控制器、节点控制器、云服务控制器等无法工作，从而导致 Deployment、Service 等无法工作，也无法注册新的 Node 到集群中来</li>
<li>新创建的 Pod 无法调度（总是 Pending 状态）</li>
</ul>
</li>
<li>Node 本身宕机或者 Kubelet 无法启动会导致<ul>
<li>Node 上面的 Pod 无法正常运行</li>
<li>已在运行的 Pod 无法正常终止</li>
</ul>
</li>
<li>网络分区会导致 Kubelet 等与控制平面通信异常以及 Pod 之间通信异常</li>
</ul>
<p>为了维持集群的健康状态，推荐在部署集群时就考虑以下</p>
<ul>
<li>在云平台上开启 VM 的自动重启功能</li>
<li>为 Etcd 配置多节点高可用集群，使用持久化存储（如 AWS EBS 等），定期备份数据</li>
<li>为控制平面配置高可用，比如多 kube-apiserver 负载均衡以及多节点运行 kube-controller-manager、kube-scheduler 以及 kube-dns 等</li>
<li>尽量使用复制控制器和 Service，而不是直接管理 Pod</li>
<li>跨地域的多 Kubernetes 集群</li>
</ul>
<h2 id="查看-Node-状态"><a href="#查看-Node-状态" class="headerlink" title="查看 Node 状态"></a>查看 Node 状态</h2><p>一般来说，可以首先查看 Node 的状态，确认 Node 本身是不是 Ready 状态</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes</span><br><span class="line">kubectl describe node &lt;node-name&gt;</span><br></pre></td></tr></table></figure>
<p>如果是 NotReady 状态，则可以执行 <code>kubectl describe node &lt;node-name&gt;</code> 命令来查看当前 Node 的事件。这些事件通常都会有助于排查 Node 发生的问题。</p>
<h2 id="SSH-登录-Node"><a href="#SSH-登录-Node" class="headerlink" title="SSH 登录 Node"></a>SSH 登录 Node</h2><p>在排查 Kubernetes 问题时，通常需要 SSH 登录到具体的 Node 上面查看 kubelet、docker、iptables 等的状态和日志。在使用云平台时，可以给相应的 VM 绑定一个公网 IP；而在物理机部署时，可以通过路由器上的端口映射来访问。但更简单的方法是使用 SSH Pod （不要忘记替换成你自己的 nodeName）：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ssh.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ssh</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ssh</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">LoadBalancer</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">22</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">22</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ssh</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ssh</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">ssh</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ssh</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">alpine</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">alpine</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">22</span></span><br><span class="line"><span class="attr">        stdin:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        tty:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      nodeName:</span> <span class="string">&lt;node-name&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f ssh.yaml</span><br><span class="line">$ kubectl get svc ssh</span><br><span class="line">NAME      TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE</span><br><span class="line">ssh       LoadBalancer   10.0.99.149   52.52.52.52   22:32008/TCP   5m</span><br></pre></td></tr></table></figure>
<p>接着，就可以通过 ssh 服务的外网 IP 来登录 Node，如 <code>ssh user@52.52.52.52</code>。</p>
<p>在使用完后， 不要忘记删除 SSH 服务 <code>kubectl delete -f ssh.yaml</code>。</p>
<h2 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h2><p>一般来说，Kubernetes 的主要组件有两种部署方法</p>
<ul>
<li>直接使用 systemd 等启动控制节点的各个服务</li>
<li>使用 Static Pod 来管理和启动控制节点的各个服务</li>
</ul>
<p>使用 systemd 等管理控制节点服务时，查看日志必须要首先 SSH 登录到机器上，然后查看具体的日志文件。如</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">journalctl -l -u kube-apiserver</span><br><span class="line">journalctl -l -u kube-controller-manager</span><br><span class="line">journalctl -l -u kube-scheduler</span><br><span class="line">journalctl -l -u kubelet</span><br><span class="line">journalctl -l -u kube-proxy</span><br></pre></td></tr></table></figure>
<p>或者直接查看日志文件</p>
<ul>
<li>/var/log/kube-apiserver.log</li>
<li>/var/log/kube-scheduler.log</li>
<li>/var/log/kube-controller-manager.log</li>
<li>/var/log/kubelet.log</li>
<li>/var/log/kube-proxy.log</li>
</ul>
<p>而对于使用 Static Pod 部署集群控制平面服务的场景，可以参考下面这些查看日志的方法。</p>
<h3 id="kube-apiserver-日志"><a href="#kube-apiserver-日志" class="headerlink" title="kube-apiserver 日志"></a>kube-apiserver 日志</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PODNAME=$(kubectl -n kube-system get pod -l component=kube-apiserver -o jsonpath=<span class="string">'&#123;.items[0].metadata.name&#125;'</span>)</span><br><span class="line">kubectl -n kube-system logs <span class="variable">$PODNAME</span> --tail 100</span><br></pre></td></tr></table></figure>
<h3 id="kube-controller-manager-日志"><a href="#kube-controller-manager-日志" class="headerlink" title="kube-controller-manager 日志"></a>kube-controller-manager 日志</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PODNAME=$(kubectl -n kube-system get pod -l component=kube-controller-manager -o jsonpath=<span class="string">'&#123;.items[0].metadata.name&#125;'</span>)</span><br><span class="line">kubectl -n kube-system logs <span class="variable">$PODNAME</span> --tail 100</span><br></pre></td></tr></table></figure>
<h3 id="kube-scheduler-日志"><a href="#kube-scheduler-日志" class="headerlink" title="kube-scheduler 日志"></a>kube-scheduler 日志</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PODNAME=$(kubectl -n kube-system get pod -l component=kube-scheduler -o jsonpath=<span class="string">'&#123;.items[0].metadata.name&#125;'</span>)</span><br><span class="line">kubectl -n kube-system logs <span class="variable">$PODNAME</span> --tail 100</span><br></pre></td></tr></table></figure>
<h3 id="kube-dns-日志"><a href="#kube-dns-日志" class="headerlink" title="kube-dns 日志"></a>kube-dns 日志</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PODNAME=$(kubectl -n kube-system get pod -l k8s-app=kube-dns -o jsonpath=<span class="string">'&#123;.items[0].metadata.name&#125;'</span>)</span><br><span class="line">kubectl -n kube-system logs <span class="variable">$PODNAME</span> -c kubedns</span><br></pre></td></tr></table></figure>
<h3 id="Kubelet-日志"><a href="#Kubelet-日志" class="headerlink" title="Kubelet 日志"></a>Kubelet 日志</h3><p>查看 Kubelet 日志需要首先 SSH 登录到 Node 上。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -l -u kubelet</span><br></pre></td></tr></table></figure>
<h3 id="Kube-proxy-日志"><a href="#Kube-proxy-日志" class="headerlink" title="Kube-proxy 日志"></a>Kube-proxy 日志</h3><p>Kube-proxy 通常以 DaemonSet 的方式部署</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get pod -l component=kube-proxy</span><br><span class="line">NAME               READY     STATUS    RESTARTS   AGE</span><br><span class="line">kube-proxy-42zpn   1/1       Running   0          1d</span><br><span class="line">kube-proxy-7gd4p   1/1       Running   0          3d</span><br><span class="line">kube-proxy-87dbs   1/1       Running   0          4d</span><br><span class="line">$ kubectl -n kube-system logs kube-proxy-42zpn</span><br></pre></td></tr></table></figure>
<h2 id="Kube-dns-Dashboard-CrashLoopBackOff"><a href="#Kube-dns-Dashboard-CrashLoopBackOff" class="headerlink" title="Kube-dns/Dashboard CrashLoopBackOff"></a>Kube-dns/Dashboard CrashLoopBackOff</h2><p>由于 Dashboard 依赖于 kube-dns，所以这个问题一般是由于 kube-dns 无法正常启动导致的。查看 kube-dns 的日志</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns</span><br><span class="line">$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c dnsmasq</span><br><span class="line">$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c sidecar</span><br></pre></td></tr></table></figure>
<p>可以发现如下的错误日志</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Waiting <span class="keyword">for</span> services and endpoints to be initialized from apiserver...</span><br><span class="line">skydns: failure to forward request <span class="string">"read udp 10.240.0.18:47848-&gt;168.63.129.16:53: i/o timeout"</span></span><br><span class="line">Timeout waiting <span class="keyword">for</span> initialization</span><br></pre></td></tr></table></figure>
<p>这说明 kube-dns pod 无法转发 DNS 请求到上游 DNS 服务器。解决方法为</p>
<ul>
<li>如果使用的 Docker 版本大于 1.12，则在每个 Node 上面运行 <code>iptables -P FORWARD ACCEPT</code></li>
<li>等待一段时间，如果还未恢复，则检查 Node 网络是否正确配置，比如是否可以正常请求上游DNS服务器、是否有安全组禁止了 DNS 请求等</li>
</ul>
<p>如果错误日志中不是转发 DNS 请求超时，而是访问 kube-apiserver 超时，比如</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">E0122 06:56:04.774977       1 reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout</span><br><span class="line">I0122 06:56:04.775358       1 dns.go:174] Waiting <span class="keyword">for</span> services and endpoints to be initialized from apiserver...</span><br><span class="line">E0122 06:56:04.775574       1 reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Service: Get https://10.0.0.1:443/api/v1/services?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout</span><br><span class="line">I0122 06:56:05.275295       1 dns.go:174] Waiting <span class="keyword">for</span> services and endpoints to be initialized from apiserver...</span><br><span class="line">I0122 06:56:05.775182       1 dns.go:174] Waiting <span class="keyword">for</span> services and endpoints to be initialized from apiserver...</span><br><span class="line">I0122 06:56:06.275288       1 dns.go:174] Waiting <span class="keyword">for</span> services and endpoints to be initialized from apiserver...</span><br></pre></td></tr></table></figure>
<p>这说明 Pod 网络（一般是多主机之间）访问异常，包括 Pod-&gt;Node、Node-&gt;Pod 以及 Node-Node 等之间的往来通信异常。可能的原因比较多，具体的排错方法可以参考<a href="network.md">网络异常排错指南</a>。</p>
<h2 id="Kubelet-failed-to-initialize-top-level-QOS-containers"><a href="#Kubelet-failed-to-initialize-top-level-QOS-containers" class="headerlink" title="Kubelet: failed to initialize top level QOS containers"></a>Kubelet: failed to initialize top level QOS containers</h2><p>重启 kubelet 时报错 <code>Failed to start ContainerManager failed to initialise top level QOS containers</code>（参考 <a href="https://github.com/kubernetes/kubernetes/issues/43856" target="_blank" rel="noopener">#43856</a>），临时解决方法是：</p>
<ol>
<li>在 docker.service 配置中增加 <code>--exec-opt native.cgroupdriver=systemd</code> 选项。</li>
<li>重启主机</li>
</ol>
<p>该问题已于2017年4月27日修复（v1.7.0+， <a href="https://github.com/kubernetes/kubernetes/pull/44940" target="_blank" rel="noopener">#44940</a>）。更新集群到新版本即可解决这个问题。</p>
<h2 id="Kubelet-一直报-FailedNodeAllocatableEnforcement-事件"><a href="#Kubelet-一直报-FailedNodeAllocatableEnforcement-事件" class="headerlink" title="Kubelet 一直报 FailedNodeAllocatableEnforcement 事件"></a>Kubelet 一直报 FailedNodeAllocatableEnforcement 事件</h2><p>当 NodeAllocatable 特性未开启时（即 kubelet 设置了 <code>--cgroups-per-qos=false</code> ），查看 node 的事件会发现每分钟都会有 <code>Failed to update Node Allocatable Limits</code> 的警告信息：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node node1</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                            Age                  From                               Message</span><br><span class="line">  ----     ------                            ----                 ----                               -------</span><br><span class="line">  Warning  FailedNodeAllocatableEnforcement  2m (x1001 over 16h)  kubelet, aks-agentpool-22604214-0  Failed to update Node Allocatable Limits <span class="string">""</span>: failed to <span class="built_in">set</span> supported cgroup subsystems <span class="keyword">for</span> cgroup : Failed to <span class="built_in">set</span> config <span class="keyword">for</span> supported subsystems : failed to write 7285047296 to memory.limit_in_bytes: write /var/lib/docker/overlay2/5650a1aadf9c758946073fefa1558446ab582148ddd3ee7e7cb9d269fab20f72/merged/sys/fs/cgroup/memory/memory.limit_in_bytes: invalid argument</span><br></pre></td></tr></table></figure>
<p>如果 NodeAllocatable 特性确实不需要，那么该警告事件可以忽略。但根据 Kubernetes 文档 <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/" target="_blank" rel="noopener">Reserve Compute Resources for System Daemons</a>，最好开启该特性：</p>
<blockquote>
<p>Kubernetes nodes can be scheduled to <code>Capacity</code>. Pods can consume all the available capacity on a node by default. This is an issue because nodes typically run quite a few system daemons that power the OS and Kubernetes itself. Unless resources are set aside for these system daemons, pods and system daemons compete for resources and lead to resource starvation issues on the node.</p>
<p>The <code>kubelet</code> exposes a feature named <code>Node Allocatable</code> that helps to reserve compute resources for system daemons. Kubernetes recommends cluster administrators to configure <code>Node Allocatable</code> based on their workload density on each node.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">     Node Capacity</span><br><span class="line">---------------------------</span><br><span class="line">|     kube-reserved       |</span><br><span class="line">|-------------------------|</span><br><span class="line">|     system-reserved     |</span><br><span class="line">|-------------------------|</span><br><span class="line">|    eviction-threshold   |</span><br><span class="line">|-------------------------|</span><br><span class="line">|                         |</span><br><span class="line">|      allocatable        |</span><br><span class="line">|   (available for pods)  |</span><br><span class="line">|                         |</span><br><span class="line">|                         |</span><br><span class="line">---------------------------</span><br></pre></td></tr></table></figure>
</blockquote>
<p>开启方法为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubelet --cgroups-per-qos=<span class="literal">true</span> --enforce-node-allocatable=pods ...</span><br></pre></td></tr></table></figure>
<h2 id="Kube-proxy-error-looking-for-path-of-conntrack"><a href="#Kube-proxy-error-looking-for-path-of-conntrack" class="headerlink" title="Kube-proxy: error looking for path of conntrack"></a>Kube-proxy: error looking for path of conntrack</h2><p>kube-proxy 报错，并且 service 的 DNS 解析异常</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-proxy[2241]: E0502 15:55:13.889842    2241 conntrack.go:42] conntrack returned error: error looking <span class="keyword">for</span> path of conntrack: <span class="built_in">exec</span>: <span class="string">"conntrack"</span>: executable file not found <span class="keyword">in</span> <span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<p>解决方式是安装 <code>conntrack-tools</code> 包后重启 kube-proxy 即可。</p>
<h2 id="Dashboard-中无资源使用图表"><a href="#Dashboard-中无资源使用图表" class="headerlink" title="Dashboard 中无资源使用图表"></a>Dashboard 中无资源使用图表</h2><p>正常情况下，Dashboard 首页应该会显示资源使用情况的图表，如</p>
<p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1525688792/troubleshooting-cluster.png" alt=""></p>
<p>如果没有这些图表，则需要首先检查 Heapster 是否正在运行（因为Dashboard 需要访问 Heapster 来查询资源使用情况）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n kube-system get pods -l k8s-app=heapster</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">heapster-86b59f68f6-h4vt6   2/2       Running   0          5d</span><br></pre></td></tr></table></figure>
<p>如果查询结果为空，说明 Heapster 还未部署，可以参考 <a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">https://github.com/kubernetes/heapster</a> 来部署。</p>
<p>但如果 Heapster 处于正常状态，那么需要查看 dashboard 的日志，确认是否还有其他问题</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get pods -l k8s-app=kubernetes-dashboard</span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">kubernetes-dashboard-665b4f7df-dsjpn   1/1       Running   0          5d</span><br><span class="line"></span><br><span class="line">$ kubectl -n kube-system logs kubernetes-dashboard-665b4f7df-dsjpn</span><br></pre></td></tr></table></figure>
<h2 id="HPA-不自动扩展-Pod"><a href="#HPA-不自动扩展-Pod" class="headerlink" title="HPA 不自动扩展 Pod"></a>HPA 不自动扩展 Pod</h2><p>查看 HPA 的事件，发现</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe hpa php-apache</span><br><span class="line">Name:                                                  php-apache</span><br><span class="line">Namespace:                                             default</span><br><span class="line">Labels:                                                &lt;none&gt;</span><br><span class="line">Annotations:                                           &lt;none&gt;</span><br><span class="line">CreationTimestamp:                                     Wed, 27 Dec 2017 14:36:38 +0800</span><br><span class="line">Reference:                                             Deployment/php-apache</span><br><span class="line">Metrics:                                               ( current / target )</span><br><span class="line">  resource cpu on pods  (as a percentage of request):  &lt;unknown&gt; / 50%</span><br><span class="line">Min replicas:                                          1</span><br><span class="line">Max replicas:                                          10</span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status  Reason                   Message</span><br><span class="line">  ----           ------  ------                   -------</span><br><span class="line">  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target<span class="string">'s current scale</span></span><br><span class="line"><span class="string">  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)</span></span><br><span class="line"><span class="string">Events:</span></span><br><span class="line"><span class="string">  Type     Reason                   Age                  From                       Message</span></span><br><span class="line"><span class="string">  ----     ------                   ----                 ----                       -------</span></span><br><span class="line"><span class="string">  Warning  FailedGetResourceMetric  3m (x2231 over 18h)  horizontal-pod-autoscaler  unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)</span></span><br></pre></td></tr></table></figure>
<p>这说明 <a href="../addons/metrics.md">metrics-server</a> 未部署，可以参考 <a href="../addons/metrics.md">这里</a> 部署。</p>
<h2 id="Node-存储空间不足"><a href="#Node-存储空间不足" class="headerlink" title="Node 存储空间不足"></a>Node 存储空间不足</h2><p>Node 存储空间不足一般是容器镜像未及时清理导致的，比如短时间内运行了很多使用较大镜像的容器等。Kubelet 会自动清理未使用的镜像，但如果想要立即清理，可以使用 <a href="https://github.com/spotify/docker-gc" target="_blank" rel="noopener">spotify/docker-gc</a>：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v /etc:/etc:ro spotify/docker-gc</span><br></pre></td></tr></table></figure>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/" target="_blank" rel="noopener">Troubleshoot Clusters</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/aks/aks-ssh#configure-ssh-access" target="_blank" rel="noopener">SSH into Azure Container Service (AKS) cluster nodes</a></li>
<li><a href="https://github.com/kubernetes/dashboard/wiki/FAQ" target="_blank" rel="noopener">Kubernetes dashboard FAQ</a></li>
</ul>
</div><div class="tags"><a href="/tags/Kubernetes-排错指南/">Kubernetes 排错指南</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2018/05/08/troubleshooting-pod.html" class="pre">Kubernetes 排错指南：POD【转载】</a><a href="/2018/05/07/troubleshooting-index.html" class="next">Kubernetes 排错指南：开篇【转载】</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#概述"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#查看-Node-状态"><span class="toc-text">查看 Node 状态</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSH-登录-Node"><span class="toc-text">SSH 登录 Node</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#查看日志"><span class="toc-text">查看日志</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kube-apiserver-日志"><span class="toc-text">kube-apiserver 日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kube-controller-manager-日志"><span class="toc-text">kube-controller-manager 日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kube-scheduler-日志"><span class="toc-text">kube-scheduler 日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kube-dns-日志"><span class="toc-text">kube-dns 日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kubelet-日志"><span class="toc-text">Kubelet 日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kube-proxy-日志"><span class="toc-text">Kube-proxy 日志</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kube-dns-Dashboard-CrashLoopBackOff"><span class="toc-text">Kube-dns/Dashboard CrashLoopBackOff</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kubelet-failed-to-initialize-top-level-QOS-containers"><span class="toc-text">Kubelet: failed to initialize top level QOS containers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kubelet-一直报-FailedNodeAllocatableEnforcement-事件"><span class="toc-text">Kubelet 一直报 FailedNodeAllocatableEnforcement 事件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kube-proxy-error-looking-for-path-of-conntrack"><span class="toc-text">Kube-proxy: error looking for path of conntrack</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dashboard-中无资源使用图表"><span class="toc-text">Dashboard 中无资源使用图表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HPA-不自动扩展-Pod"><span class="toc-text">HPA 不自动扩展 Pod</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Node-存储空间不足"><span class="toc-text">Node 存储空间不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文档"><span class="toc-text">参考文档</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/12/comparison-between-calico-and-flannel.html">calico与flannel对比</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/08/troubleshooting-pod.html">Kubernetes 排错指南：POD【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/troubleshooting-cluster.html">Kubernetes 排错指南：集群【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/troubleshooting-index.html">Kubernetes 排错指南：开篇【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/18/top-5-kubernetes-best-practices.html">五大 Kubernetes 最佳实践【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/02/kubernetes-19-conflict-with-centos7.html">Kubernetes 1.9 与 CentOS 7 内核兼容问题【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/performance-tuning-overview.html">性能调优概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/31/simplifying-microservices-with-istio-in-kubernetes-3.html">使用Istio简化微服务系列三：如何才能做“金丝雀部署”，并通过Istio增加流量？【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/31/simplifying-microservices-with-istio-in-kubernetes-2.html">使用Istio简化微服务系列二：如何通过HTTPS与外部服务进行通信？【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/31/simplifying-microservices-with-istio-in-kubernetes-1.html">使用Istio简化微服务系列一：如何用Isito解决Spring Cloud Netflix部署微服务的挑战?【转载】</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Jenkins/">Jenkins</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kubernetes/">Kubernetes</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/微服务/">微服务</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/亲和性/" style="font-size: 15px;">亲和性</a> <a href="/tags/微服务入门系列/" style="font-size: 15px;">微服务入门系列</a> <a href="/tags/Pipeline/" style="font-size: 15px;">Pipeline</a> <a href="/tags/微服务拆分/" style="font-size: 15px;">微服务拆分</a> <a href="/tags/Jenkins/" style="font-size: 15px;">Jenkins</a> <a href="/tags/Kubernetes/" style="font-size: 15px;">Kubernetes</a> <a href="/tags/Dubbo-Service-Mesh-微服务-ZooKeeper/" style="font-size: 15px;">Dubbo,Service Mesh,微服务,ZooKeeper</a> <a href="/tags/性能调优/" style="font-size: 15px;">性能调优</a> <a href="/tags/Istio简化微服务系列/" style="font-size: 15px;">Istio简化微服务系列</a> <a href="/tags/Spring-Cloud/" style="font-size: 15px;">Spring Cloud</a> <a href="/tags/Kubernetes-排错指南/" style="font-size: 15px;">Kubernetes 排错指南</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Cloudpanl.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?8c06a0cc670571138e4a33bce0d3167a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>