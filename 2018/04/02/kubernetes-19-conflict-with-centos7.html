<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="panliang's blog."><meta name="keywords" content="Linux,Kubernetes,Docker,Microservices"><title>Kubernetes 1.9 与 CentOS 7 内核兼容问题【转载】 | 每天进步一点点……</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kubernetes 1.9 与 CentOS 7 内核兼容问题【转载】</h1><a id="logo" href="/.">每天进步一点点……</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Kubernetes 1.9 与 CentOS 7 内核兼容问题【转载】</h1><div class="post-meta"><a href="/2018/04/02/kubernetes-19-conflict-with-centos7.html#comments" class="comment-count"></a><p><span class="date">Apr 02, 2018</span><span><a href="/categories/Kubernetes/" class="category">Kubernetes</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>生产环境发现不定时 Java 应用出现 coredump 故障，测试环境不定时出现写入 /cgroup/memory 报 no space left on device 的故障，导致整个 kubernetes node 节点无法使用。甚至会随着堆积的 cgroup 越来越多，docker ps 执行异常，直到把内存吃光，机器挂死。</p>
<p>典型报错：</p>
<pre><code>kubelet.ns-k8s-node001.root.log.ERROR.20180214-113740.15702:1593018:E0320 04:59:09.572336 15702 remote_runtime.go:92] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod &quot;osp-xxx-com-ljqm19-54bf7678b8-bvz9s&quot;: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused &quot;process_linux.go:258: applying cgroup configuration for process caused \&quot;mkdir /sys/fs/cgroup/memory/kubepods/burstable/podf1bd9e87-1ef2-11e8-afd3-fa163ecf2dce/8710c146b3c8b52f5da62e222273703b1e3d54a6a6270a0ea7ce1b194f1b5053: no space left on device\&quot;&quot;
</code></pre><p>或者</p>
<pre><code>Mar 26 18:36:59 ns-k8s-node-s0054 kernel: SLUB: Unable to allocate memory on node -1 (gfp=0x8020)
Mar 26 18:36:59 ns-k8s-noah-node001 kernel: cache: ip6_dst_cache(1995:6b6bc0c9f30123084a409d89a300b017d26ee5e2c3ac8a02c295c378f3dbfa5f), object size: 448, buffer size: 448, default order: 2, min order: 0
</code></pre><p>该问题发生前后，进行过 kubernetes 1.6 到 1.9 的升级工作。排查了好久才定位到问题与 kubernetes 、操作系统内核有关。</p>
<h1 id="1-对比测试结果"><a href="#1-对比测试结果" class="headerlink" title="1. 对比测试结果"></a>1. 对比测试结果</h1><p>使用同样的测试方法，结果为：</p>
<ul>
<li>使用初次部署 k8s 1.6 版本测试，没有出现 cgroup memory 遗漏问题；</li>
<li>从 k8s 1.6 升级到 1.9 后，测试没有出现 cgroup memory 遗漏问题；</li>
<li>重启 kubelet 1.9 node 节点重启，再次测试，出现 cgroup memory 遗漏问题。</li>
</ul>
<p>对比 k8s 1.6 和 1.9 创建的 POD 基础容器和业务容器，runc、libcontainerd、docker inspect 的容器 json 参数都一致，没有差异。</p>
<p>为什么同样是 k8s 1.9（其他 docker、kernel 版本一致）的情况下，结果不一样呢？重启的影响是？</p>
<h1 id="2-问题重现"><a href="#2-问题重现" class="headerlink" title="2.问题重现"></a>2.问题重现</h1><p>对于 cgroup memory 报 no space left on device ，是由于 cgroup memory 存在 64k（65535 个）大小的限制。</p>
<p>采用下面的测试方式，可以发现在删除 pod 后，会出现 cgroup memory 遗漏的问题。该测试方法通过留空 99 个 系统 cgroup memory 位置，来判断引起问题的原因是由于 pod container 导致的。</p>
<p>1）填满系统 cgroup memory</p>
<pre><code># uname -r  
3.10.0-514.10.2.el7.x86_64  
# kubelet --version  
Kubernetes 1.9.0  
# mkdir /sys/fs/cgroup/memory/test  
# for i in `seq 1 65535`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done  
# cat /proc/cgroups |grep memory  
memory  11      65535   1 
</code></pre><p>把系统 cgroup memory 填到 65535 个。</p>
<p>2）腾空 99 个 cgroup memory</p>
<pre><code># for i in `seq 1 100`;do rmdir /sys/fs/cgroup/memory/test/test-${i} 2&gt;/dev/null 1&gt;&amp;2; done   
# mkdir /sys/fs/cgroup/memory/stress/  
# for i in `seq 1 100`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done   
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-100’: No space left on device  
# for i in `seq 1 100`;do rmdir /sys/fs/cgroup/memory/test/test-${i}; done  
# cat /proc/cgroups |grep memory  
memory  11      65436   1 
</code></pre><p>在写入第 100 个的时候提示无法写入，证明写入了 99 个。</p>
<p>3）创建一个 pod 到这个 node 上，查看占用的 cgroup memory 情况</p>
<p>每创建一个 pod ，会占用 3 个 cgroup memory 目录：</p>
<pre><code># ll /sys/fs/cgroup/memory/kubepods/pod0f6c3c27-3186-11e8-afd3-fa163ecf2dce/  
total 0  
drwxr-xr-x 2 root root 0 Mar 27 14:14 6d1af9898c7f8d58066d0edb52e4d548d5a27e3c0d138775e9a3ddfa2b16ac2b  
drwxr-xr-x 2 root root 0 Mar 27 14:14 8a65cb234767a02e130c162e8d5f4a0a92e345bfef6b4b664b39e7d035c63d1  
</code></pre><p>这时再次创建 100 个 cgroup memory ，因为 pod 占用了 3 个，会出现 4 个无法成功：</p>
<pre><code># for i in `seq 1 100`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done      
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-97’: No space left on device &amp;lt;-- 3 directory used by pod  
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-98’: No space left on device  
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-99’: No space left on device  
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-100’: No space left on device  
# cat /proc/cgroups   
memory  11      65439   1 
</code></pre><p>写入到的 cgroup memory 增加到 65439 个。</p>
<p>4）删掉测试 pod ，看看 3 个占用的 cgroup memory 是否有释放</p>
<p>看到的结果：</p>
<pre><code># cat /proc/cgroups   
memory  11      65436   1  
# for i in `seq 1 100`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done      
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-97’: No space left on device  
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-98’: No space left on device  
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-99’: No space left on device  
mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-100’: No space left on device  
</code></pre><p>可以看到，虽然 cgroup memory 减少到 65536 ，似乎 3 个位置释放了。但实际上测试结果发现，并不能写入，结果还是 pod 占用时的无法写入 97-100 。</p>
<p>这就说明，cgroup memory 数量减少，但被 pod container 占用的空间没有释放。</p>
<p>反复验证后，发现随着 pod 发布和变更的增加，该问题会越来越严重，直到把整台机器的 cgroup memory 用完。</p>
<pre><code>$ cat /proc/cgroups   
#subsys_name    hierarchy       num_cgroups     enabled  
cpuset  10      229418  1                 -- 但 cpuset 数量很恐怖  
cpu     6       118     1  
cpuacct 6       118     1  
memory  7       109     1                  -- 看上去不多，实际没有释放空间的  
devices 3       229504  1  
freezer 5       32      1  
net_cls 4       32      1  
blkio   9       118     1  
perf_event      11      32      1  
hugetlb 2       32      1  
pids    8       32      1  
net_prio        4       32      1  
</code></pre><h1 id="3-问题根源"><a href="#3-问题根源" class="headerlink" title="3.问题根源"></a>3.问题根源</h1><p>经过大量的测试和对比分析，在两个 k8s 1.9 环境中 kubelet 创建的 /sys/fs/cgroup/memory/kubepods 差异，发现：</p>
<p>没问题的： </p>
<pre><code>[root@k8s-node01 kubepods]# cat memory.kmem.slabinfo   
cat: memory.kmem.slabinfo: Input/output error  
</code></pre><p>有问题的：  </p>
<pre><code>[root@k8s-node01 kubepods]# cat memory.kmem.slabinfo   
slabinfo - version: 2.1  
# name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt; 
</code></pre><p>也就是说，在有问题的环境下，cgroup kernel memory 特性被激活了。</p>
<p>关于 cgroup kernel memory，在 kernel-doc 中有如下描述：</p>
<pre><code># vim /usr/share/doc/kernel-doc-3.10.0/Documentation/cgroups/memory.txt
2.7 Kernel Memory Extension (CONFIG_MEMCG_KMEM) With the Kernel memory extension, the Memory Controller is able to limit the amount of kernel memory used by the system. Kernel memory is fundamentally different than user memory, since it can&apos;t be swapped out, which makes it possible to DoS the system by consuming too much of this precious resource. Kernel memory won&apos;t be accounted at all until limit on a group is set. This allows for existing setups to continue working without disruption. The limit cannot be set if the cgroup have children, or if there are already tasks in the cgroup. Attempting to set the limit under those conditions will return -EBUSY. When use_hierarchy == 1 and a group is accounted, its children will automatically be accounted regardless of their limit value. After a group is first limited, it will be kept being accounted until it is removed. The memory limitation itself, can of course be removed by writing -1 to memory.kmem.limit_in_bytes. In this case, kmem will be accounted, but not limited.
</code></pre><p>这是一个 cgroup memory 的扩展，用于限制对 kernel memory 的使用。但该特性在大于 4.0 版本中是个实验特性，若使用 docker run 运行，就会提示：</p>
<pre><code># docker run -d --name test001 --kernel-memory 100M registry.vclound.com:5000/hyphenwang/sshdserver:v1 
WARNING: You specified a kernel memory limit on a kernel older than 4.0. Kernel memory limits are experimental on older kernels, it won&apos;t work as expected and can cause your system to be unstable.

# cat /sys/fs/cgroup/memory/docker/eceb6dfba2c64a783f33bd5e54cecb32d5e64647439b4932468650257ea06206/memory.kmem.limit_in_bytes 
104857600

# cat /sys/fs/cgroup/memory/docker/eceb6dfba2c64a783f33bd5e54cecb32d5e64647439b4932468650257ea06206/memory.kmem.slabinfo   
slabinfo - version: 2.1  
# name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;  
ip6_dst_cache          0      0    448   18    2 : tunables    0    0    0 : slabdata      0      0      0  
UDPv6                  0      0   1216   26    8 : tunables    0    0    0 : slabdata      0      0      0  
kmalloc-8192           0      0   8192    4    8 : tunables    0    0    0 : slabdata      0      0      0  
signal_cache          28     28   1152   28    8 : tunables    0    0    0 : slabdata      1      1      0  
sighand_cache         15     15   2112   15    8 : tunables    0    0    0 : slabdata      1      1      0  
sysfs_dir_cache       36     36    112   36    1 : tunables    0    0    0 : slabdata      1      1      0  
task_struct            8      8   4016    8    8 : tunables    0    0    0 : slabdata      1      1      0  
......  
</code></pre><p>经过反复验证，<strong>当使用 docker run –kernel-memory 参数启动的容器，在删除后也不会释放 cgroup memory 占用的位置 ，存在同样的问题。</strong></p>
<p>基于该现象，对比 kubernetes 1.6 和 kubernetes 1.9 对 cgroup kernel memory 设置的差异。</p>
<p>在k8s 1.9 vendor库中，少了if d.config.KernelMemory != 0 { 这行代码的判断， 而在k8s的Github，1.6.4版本在官方的runc/libcontainerd增加了这行代码，但在1.9.0删掉了，导致默认就使用kmem 。</p>
<pre><code>$ git diff remotes/origin/vip_v1.6.4.3 remotes/origin/vip_v1.9.0 -- vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go  
@@ -29,14 +35,18 @@ func (s *MemoryGroup) Apply(d *cgroupData) (err error) {  
        path, err := d.path(&quot;memory&quot;)  
        if err != nil &amp;&amp; !cgroups.IsNotFound(err) {  
                return err  
+       } else if path == &quot;&quot; {  
+               return nil  
        }  
        if memoryAssigned(d.config) {  
-               if path != &quot;&quot; {  
+               if _, err := os.Stat(path); os.IsNotExist(err) {  
                        if err := os.MkdirAll(path, 0755); err != nil {  
                                return err  
                        }  
-               }  
-               if d.config.KernelMemory != 0 { // 删除了这行的判断，使得 1.9 默认就 enable cgroup kernel memory 特性  
+                       // Only enable kernel memory accouting when this cgroup  
+                       // is created by libcontainer, otherwise we might get  
+                       // error when people use `cgroupsPath` to join an existed  
+                       // cgroup whose kernel memory is not initialized.  
                        if err := EnableKernelMemoryAccounting(path); err != nil {  
                                return err  
                        }  
</code></pre><p>这就引发了 cgroup memory 也不能释放的问题（与通过 docker run –kernel-memory 打开的情况一样。）</p>
<p>其实根据 libcontainerd 推荐的 kernel 4.3 以上版本，打开 cgroup kernel memory 应该也是没问题的。可惜我们使用的kernel版本是3.10.0-514.16.1.el7.x86_64，kernel memory在这个版本是不稳定的。因此让我们踩了这个坑。</p>
<p>经验证，CentOS 7.4（3.10.0-693.11.1.el7）存在同样的问题。</p>
<h1 id="4-寻根问底"><a href="#4-寻根问底" class="headerlink" title="4.寻根问底"></a>4.寻根问底</h1><p>附上同事对该问题在 kernel 中追踪的结果。</p>
<p>memcg是Linux内核中用于管理cgroup中kernel 内存的模块，整个生命周期应该是跟随cgroup的，但是当在3.10.0-514.10.2.el7版本中，一旦开启了kmem_limit就可能会导致不能彻底删除memcg和对应的cssid。</p>
<p>在创建过程中，如果我们设定了kmem_limit就会激活memcg</p>
<p><img src="http://dockone.io/uploads/article/20180402/e1c3e73bd35576a25951807b56187acd.jpg" alt=""></p>
<p>memcg和cssid的释放的调用链很长，其中在这个过程中，mem_cgroup_css_free的过程中会调用kmem_cgroup_destroy</p>
<p><img src="http://dockone.io/uploads/article/20180402/388d74c145785290a63ba62e5b3a30b2.jpg" alt=""></p>
<p>而kmem_cgroup_destroy函数会检查memcg是否还占用memory，如果发现还占用，就不会调用mem_cgroup_put函数：</p>
<p><img src="http://dockone.io/uploads/article/20180402/2b9efefbe7e25318909fa7d85f681d94.jpg" alt=""></p>
<p>而mem_cgroup_put函数会将memcg的refcnt减去1，并查看是否等于0，如果为0即表示没有其他东西引用memcg，memcg可以放心释放</p>
<p><img src="http://dockone.io/uploads/article/20180402/569c6ace25ecadc6b4af1ba1e46979a2.jpg" alt=""></p>
<p>可惜当开启了kmem_limit后，这个refcnt不会等于0，导致永远无法调用到free_rcu去释放cssid。</p>
<h1 id="5-解决问题"><a href="#5-解决问题" class="headerlink" title="5.解决问题"></a>5.解决问题</h1><p>经过以上分析，造成该故障的原因，是由于 kubelet 1.9 激活了 cgroup kernel-memory 特性，而在 CentOS 7.3 kernel 3.10.0-514.10.1.el7.x86_64 中对该特性支持不好。</p>
<p>导致删除容器后，仍有对 cgroup memory 的占用，没有执行 free_css_id() 函数的操作。</p>
<p>解决方法，就是修改 k8s 1.9 代码，再次禁止设置 cgroup kernel-memory 配置，保持关闭状态。</p>
<p>也可以将 kernel 升至 4.3 以上版本。</p>
<h1 id="6-疑问"><a href="#6-疑问" class="headerlink" title="6.疑问"></a>6.疑问</h1><p><strong>1）为什么同样是 k8s 1.9 的版本，在不同的测试中没有问题</strong></p>
<p>原因是第一次的 k8s 1.9 是在原来 k8s 1.6 的环境中，通过升级 kubelet 版本来测试的。<br>而 /sys/fs/cgroup/kubepods 是由 k8s 1.6 创建的，升级到 k8s 1.9 后，启动服务时，判断到该路径已经存在，就没有再创建新的。</p>
<p>所以，也就没有激活 cgroup kernel-memory 特性。</p>
<p>接下来创建的 POD 会继承该路径的 cgroup memory 属性，也没有激活 cgroup kernel-memoy ，所以没有引发问题。</p>
<p>相反，在重启 k8s 1.9 node 后，kubelet 新建了 /sys/fs/cgroup/kubepods ，激活了 cgroup kernel-memoy ，导致后续的 POD 在删除时也有问题。</p>
<p><strong>2）cgroup kernel memory 激活后可以关闭吗？</strong></p>
<p>按 kernel 的说明，以及 kernel 代码，激活的方式，就是传递非 -1 的值（0 也是激活）来激活。而且最小单位是 4096 PageSize 的大小。</p>
<p>在激活 cgroup kernel memory 后，是不能关闭的，只能通过设置 -1 关闭限制，但还是会继续计数。</p>
<pre><code>After a group is first limited, it will be kept being accounted until it is removed. The memory limitation itself, can of course be removed by writing -1 to memory.kmem.limit_in_bytes. In this case, kmem will be accounted, but not limited.
</code></pre><p>另外，在 CentOS 7.3 kernel 中，激活 cgroup kernel memory 后，即使通过 -1 设置不限制使用，但还是会频繁出现 SLUB 的报警，以及 Java crash 的问题。</p>
<pre><code>Mar 26 18:36:59 ns-k8s-noah-s0054 kernel: SLUB: Unable to allocate memory on node -1 (gfp=0x8020)
Mar 26 18:36:59 ns-k8s-noah-s0054 kernel: cache: ip6_dst_cache(1995:6b6bc0c9f30123084a409d89a300b017d26ee5e2c3ac8a02c295c378f3dbfa5f), object size: 448, buffer size: 448, default order: 2, min order: 0
</code></pre><p><strong>3）判断 cgroup kernel memory 是否激活的方式</strong></p>
<p>查看对应 POD container 下的 memory.kmem.slabinfo。</p>
<p>开启：</p>
<pre><code># cat memory.kmem.slabinfo   
cat: memory.kmem.slabinfo: Input/output error  
</code></pre><p>关闭：</p>
<pre><code># cat memory.kmem.slabinfo   
slabinfo - version: 2.1  
# name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;  
</code></pre><p>需要注意的是，如果给 cgroup memory.kmem.limit_in_bytes 设置 -1 ，其结果为：</p>
<pre><code># cat memory.kmem.limit_in_bytes 
9223372036854771712
</code></pre><p>这个值在关闭时和不限制 kernel memory 时是一样的，不能作为 kernel memory 是否激活的判断条件。</p>
<h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6.参考资料"></a>6.参考资料</h1><p>/usr/share/doc/kernel-doc-3.10.0/Documentation/cgroups/memory.txt</p>
<p><a href="https://github.com/kubernetes/kubernetes/issues/61937" target="_blank" rel="noopener">application crash due to k8s 1.9.x open the kernel memory accounting by default #61937</a></p>
<p><a href="https://github.com/moby/moby/issues/29638" target="_blank" rel="noopener">Docker leaking cgroups causing no space left on device? #29638</a></p>
</div><div class="tags"><a href="/tags/Kubernetes/">Kubernetes</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2018/04/01/performance-tuning-overview.html" class="next">性能调优概述</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-对比测试结果"><span class="toc-text">1. 对比测试结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-问题重现"><span class="toc-text">2.问题重现</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-问题根源"><span class="toc-text">3.问题根源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-寻根问底"><span class="toc-text">4.寻根问底</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-解决问题"><span class="toc-text">5.解决问题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-疑问"><span class="toc-text">6.疑问</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-参考资料"><span class="toc-text">6.参考资料</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/04/02/kubernetes-19-conflict-with-centos7.html">Kubernetes 1.9 与 CentOS 7 内核兼容问题【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/01/performance-tuning-overview.html">性能调优概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/31/simplifying-microservices-with-istio-in-kubernetes-3.html">使用Istio简化微服务系列三：如何才能做“金丝雀部署”，并通过Istio增加流量？【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/31/simplifying-microservices-with-istio-in-kubernetes-2.html">使用Istio简化微服务系列二：如何通过HTTPS与外部服务进行通信？【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/31/simplifying-microservices-with-istio-in-kubernetes-1.html">使用Istio简化微服务系列一：如何用Isito解决Spring Cloud Netflix部署微服务的挑战?【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/30/Microservices-split.html">微服务拆分那点事【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/30/Container-7-regulations.html">应用容器化和与Kubernetes适配的7条军规及最佳实践【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/28/Service-deployment.html">微服务入门系列(五)：服务部署【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/28/Database-Service-Segmentation.html">微服务入门系列(四)：数据库的服务化切分【转载】</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/28/Distributed-transaction-solutions.html">微服务入门系列(三)：微服务架构下的分布式事务解决方案【转载】</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Jenkins/">Jenkins</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kubernetes/">Kubernetes</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/微服务/">微服务</a><span class="category-list-count">11</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/微服务入门系列/" style="font-size: 15px;">微服务入门系列</a> <a href="/tags/微服务拆分/" style="font-size: 15px;">微服务拆分</a> <a href="/tags/Pipeline/" style="font-size: 15px;">Pipeline</a> <a href="/tags/Jenkins/" style="font-size: 15px;">Jenkins</a> <a href="/tags/亲和性/" style="font-size: 15px;">亲和性</a> <a href="/tags/Kubernetes/" style="font-size: 15px;">Kubernetes</a> <a href="/tags/Dubbo-Service-Mesh-微服务-ZooKeeper/" style="font-size: 15px;">Dubbo,Service Mesh,微服务,ZooKeeper</a> <a href="/tags/Istio简化微服务系列/" style="font-size: 15px;">Istio简化微服务系列</a> <a href="/tags/性能调优/" style="font-size: 15px;">性能调优</a> <a href="/tags/Spring-Cloud/" style="font-size: 15px;">Spring Cloud</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Cloudpanl.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?8c06a0cc670571138e4a33bce0d3167a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>