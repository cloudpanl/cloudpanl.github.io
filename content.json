[{"title":"Kubernetes中的亲和性【转载】","date":"2018-03-24T02:12:08.000Z","path":"2018/03/24/Affinity-in-Kubernetes.html","text":"现实中应用的运行对于kubernetes在亲和性上提出了一些要求，可以归类到以下几个方面： Pod固定调度到某些节点之上 Pod不会调度到某些节点之上 Pod的多副本调度到相同的节点之上 Pod的多副本调度到不同的节点之上 实践下面我们将通过例子的方式来说明在kubernetes需要去设置亲和性实现上面要求． Pod调动到某些节点上Pod的定义中通过nodeSelector指定label标签，pod将会只调度到具有该标签的node之上 apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd 这个例子中pod只会调度到具有disktype=ssd的node上面． 节点亲和性/反亲和性Affinity/anti-affinity node 相对于nodeSelector机制更加的灵活和丰富 表达的语法：支持In,NotIn,Exists,DoesNotExist,Gt,Lt． 支持soft(preference)和hard(requirement),hard表示pod sheduler到某个node上，则必须满足亲和性设置．soft表示scheduler的时候，无法满足节点的时候，会选择非nodeSelector匹配的节点． nodeAffinity的基础上添加多个nodeSelectorTerms字段，调度的时候Node只需要nodeSelectorTerms中的某一个符合条件就符合nodeAffinity的规则．在nodeSelectorTerms中添加matchExpressions，需要可以调度的Node是满足matchExpressions中表示的所有规则． apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: k8s.gcr.io/pause:2.0 Pod间的亲和性和反亲和性基于已经运行在Node 上pod的labels来决定需要新创建的Pods是否可以调度到node节点上，配置的时候可以指定那个namespace中的pod需要满足pod的亲和性．可以通过topologyKey来指定topology domain, 可以指定为node／cloud provider zone／cloud provider region的范围． 表达的语法：支持In, NotIn, Exists, DoesNotExist Pod的亲和性和反亲和性可以分成 requiredDuringSchedulingIgnoredDuringExecution #硬要求 preferredDuringSchedulingIgnoredDuringExecution ＃软要求 类似上面node的亲和策略类似，requiredDuringSchedulingIgnoredDuringExecution亲和性可以用于约束不同服务的pod在同一个topology domain的Nod上．preferredDuringSchedulingIgnoredDuringExecution反亲和性可以将服务的pod分散到不同的topology domain的Node上． topologyKey可以设置成如下几种类型 kubernetes.io/hostname ＃Node failure-domain.beta.kubernetes.io/zone ＃Zone failure-domain.beta.kubernetes.io/region #Region 可以设置node上的label的值来表示node的name,zone,region等信息，pod的规则中指定topologykey的值表示指定topology范围内的node上运行的pod满足指定规则 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname containers: - name: with-pod-affinity image: k8s.gcr.io/pause:2.0 利用社区官方的例子来进一步的说明，例子中指定了pod的亲和性和反亲和性，preferredDuringSchedulingIgnoredDuringExecution指定的规则是pod将会调度到的node尽量会满足如下条件： node上具有failure-domain.beta.kubernetes.io/zone，并且具有相同failure-domain.beta.kubernetes.io/zone的值的node上运行有一个pod,它符合label为securtity=S1. preferredDuringSchedulingIgnoredDuringExecution规则表示将不会调度到node上运行有security=S2的pod．如果这里我们将topologyKey＝failure-domain.beta.kubernetes.io/zone，那么pod将不会调度到node满足的条件是：node上具有failure-domain.beta.kubernetes.io/zone相同的ｖalue,并且这些相同zone下的node上运行有security=S2的pod. Notice:对于topologyKey字段具有如下约束 对于亲和性以及RequiredDuringScheduling的反亲和性，topologyKey需要指定 对于RequiredDuringScheduling的反亲和性，LimitPodHardAntiAffinityTopology的准入控制限制topologyKey为kubernetes.io/hostname,可以通过修改或者disable解除该约束 对于PreferredDuringScheduling的反亲和性，空的topologyKey表示kubernetes.io/hostname, failure-domain.beta.kubernetes.io/zone and failure-domain.beta.kubernetes.io/region的组合． topologyKey在遵循其他约束的基础上可以设置成其他的key. 规则中可以指定匹配pod所在namespace,如果定义了但是为空，它表示所有namespace范围内的pod. 常用的场景一些更加常用的场景见例子所示 例子一 apiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: redis-server image: redis:3.2-alpine 创建了一个Deployment,副本数为３，指定了反亲和规则如上所示，pod的label为app:store,那么pod调度的时候将不会调度到node上已经运行了label为app:store的pod了，这样就会使得Deployment的三副本分别部署在不同的host的node上． 例子二 apiVersion: apps/v1 kind: Deployment metadata: name: web-server spec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: &quot;kubernetes.io/hostname&quot; podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: web-app image: nginx:1.12-alpine 在一个例子中基础之上，要求pod的亲和性满足requiredDuringSchedulingIgnoredDuringExecution中topologyKey=”kubernetes.io/hostname”,并且node上需要运行有app=store的label. 运行完例子一，例子二，那么pod的分布如下所示 $kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE redis-cache-1450370735-6dzlj 1/1 Running 0 8m 10.192.4.2 kube-node-3 redis-cache-1450370735-j2j96 1/1 Running 0 8m 10.192.2.2 kube-node-1 redis-cache-1450370735-z73mh 1/1 Running 0 8m 10.192.3.1 kube-node-2 web-server-1287567482-5d4dz 1/1 Running 0 7m 10.192.2.3 kube-node-1 web-server-1287567482-6f7v5 1/1 Running 0 7m 10.192.4.3 kube-node-3 web-server-1287567482-s330j 1/1 Running 0 7m 10.192.3.2 kube-node-2 例子三 apiVersion: apps/v1beta1 # for versions before 1.6.0 use extensions/v1beta1 kind: Deployment metadata: name: web-server spec: replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: web-app image: hub.easystack.io/library/nginx:1.9.0 在一些应用中，pod副本之间需要共享cache,需要将pod运行在一个节点之上 web-server-77bfb4575f-bhxvg 1/1 Running 0 11s 10.233.66.79 hzc-slave2 app=web-store,pod-template-hash=3369601319 web-server-77bfb4575f-mkfd9 1/1 Running 0 11s 10.233.66.80 hzc-slave2 app=web-store,pod-template-hash=3369601319 web-server-77bfb4575f-wgjq6 1/1 Running 0 11s 10.233.66.78 hzc-slave2 app=web-store,pod-template-hash=3369601319 参考https://github.com/davidkbainbridge/demo-affinityhttps://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-featurehttps://medium.com/kokster/scheduling-in-kubernetes-part-2-pod-affinity-c2b217312ae1","tags":[{"name":"Kubernetes 亲和性","slug":"Kubernetes-亲和性","permalink":"http://yoursite.com/tags/Kubernetes-亲和性/"}]},{"title":"Pipeline语法","date":"2018-03-23T09:23:48.000Z","path":"2018/03/23/Pipeline-syntax.html","text":"介绍本文基于入门介绍，仅仅是一个语法参考。至于如何在特定的例子中运用Pipeline语法，请参考Jenkins Pipeline。从插件Pipeline plugin的2.5版本开始，Pipeline支持两种格式的语法。对于它们之间的区别请参考语法对比。 正如在入门介绍里说的，流水线最主要的就是”步骤“。基本上，就是步骤来告诉Jenkins该干什么，它是申明式和脚本式流水线语法的基础。 你可以在流水线步骤参考中，找到一份可用的步骤列表。 申明Pipeline申明式流水线是最近添加到Jenkins流水线功能中的，这种语法更加简单。 所有合法的申明式流水线必须在 pipeline 代码块中，例如： pipeline { /* insert Declarative Pipeline here */ } 在申明式流水线中，基本的语句和表达式是遵循 Groovy语法 ，但是有以下几个例外： 流水线的顶层必须是一个代码块： pipeline { } 不需分号作为语句的分隔符。每个语句单独占一行 只能包括段落、步骤、或者赋值语句 属性引用语句被当作无参数的方法调用。例如：input会当作方法input() 段落在申明式流水线中，通常包括一个或者多个指令或者步骤。 代理agent代理指定了整个流水线或者特定的阶段的运行环境。它必须在pipeline块的顶层定义，而在阶段中是可选的。 参数为了支持多种情况的流水线使用场景，代理（agent）支持几种不同类型的参数。这些参数既可以在顶层的pipeline块也可以在每个阶段中使用。 any在任意可用的代理上执行流水线。例如： agent any none当在顶层的pipeline块中使用时，不会有全局的代理分配给整个流水线，每个阶段都需要包含个人的代理。例如：agent none label根据Jenkins环境中提供的标签，确定一个可用的代理来chiding流水线或者阶段。例如：agent { label ‘my-defined-label’ } nodeagent { node { label ‘labelName’ } } 和 agent { label ‘labelName’ }一样，但是 node 允许增加选项（例如 customWorkspace） docker在指定的容器里执行流水线或者阶段，容器会被动态分配到预先配置好的基于Docker的流水线节点，或者通过label参数来匹配。docker也有一个可选参数args，该参数会直接传递给docker run来执行；还有一个alwaysPull 选项，及时镜像名已经存在了依然会强制执行docker pull。 例如： agent { docker &apos;maven:3-alpine&apos; } 或者： agent { docker { image &apos;maven:3-alpine&apos; label &apos;my-defined-label&apos; args &apos;-v /tmp:/tmp&apos; } } dockerfile由码线中的Dockerfile构建出来的容器，执行流水线或者阶段。为了使用该特性，Jenkinsfile 必须是在多分支流水线或者从SCM中加载。约定Dockerfile 在码线的根目录中，可以是agent { dockerfile true } 。如果Dockerfile 在另外一个目录中，可以使用参数dir ：agent { dockerfile { dir ‘someSubDir’ } } 。如果Dockerfile 有其他的名称，你可以通过参数filename 指定文件名称。你可以通过参数additionalBuildArgs 给命令docker build … 传递额外的选项，例如agent { dockerfile { additionalBuildArgs ‘–build-arg foo=bar’ } } 。例如：一个码线有文件build/Dockerfile.build，并需要一个构建参数version： agent { // Equivalent to &quot;docker build -f Dockerfile.build --build-arg version=1.0.2 ./build/ dockerfile { filename &apos;Dockerfile.build&apos; dir &apos;build&apos; label &apos;my-defined-label&apos; additionalBuildArgs &apos;--build-arg version=1.0.2&apos; } 通用选项Common Options有一些选项可以在多种代理实现中使用。没有指定的话，就不是必须的。 label字符串。可以在流水线或者 stage上。 该选项可以在 node，docker 和 dockerfile中使用，但对于 node是必须的。 customWorkspace字符串。指定工作空间，而不使用默认的。可以是相对于节点上的根工作空间，也可以是绝对路径。例如： agent { node { label &apos;my-defined-label&apos; customWorkspace &apos;/some/other/path&apos; } } 该选项可以用在 node， docker 和 dockerfile。 reuseNode布尔值，默认为false。如果为true，则在相同的工作空间中运行，而不是每次创建新的。该选项可以在 docker 和 dockerfile中使用，而且只有在 agent 配置到单独的 stage中才能使用。 Jenkinsfile (Declarative Pipeline) pipeline { agent { docker &apos;maven:3-alpine&apos; } stages { stage(&apos;Example Build&apos;) { steps { sh &apos;mvn -B clean verify&apos; } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent none stages { stage(&apos;Example Build&apos;) { agent { docker &apos;maven:3-alpine&apos; } steps { echo &apos;Hello, Maven&apos; sh &apos;mvn --version&apos; } } stage(&apos;Example Test&apos;) { agent { docker &apos;openjdk:8-jre&apos; } steps { echo &apos;Hello, JDK&apos; sh &apos;java -version&apos; } } } } post 阶段stages 步骤steps 指令Directive 环境environment 选项options 参数parameters 触发器triggers 阶段stage 工具tools 输入input 条件when 指令when 允许流水线根据条件来决定是否要执行特定的阶段。指令when 必须至少包含一个条件。如果指令when 包含多个条件，所有的条件都必须为true才可以会执行该阶段。这和allOf 条件是类似的（请参考下面的例子）。 更复杂的结构可以使用嵌套：not， allOf，或 anyOf。可以嵌套任意深度。 内置条件：分支branch当匹配分支名称时执行，例如： when { branch ‘master’ }。这只有在多分支流水线中才可以使用。 环境environment当指定的环境变量值和给定的一样时执行，例如： when { environment name: ‘DEPLOY_TO’, value: ‘production’ } 表达式expression当Groovy表达式为true时，例如： when { expression { return params.DEBUG_BUILD } } not当嵌套条件值为false时执行。必须包含一个条件。例如： when { not { branch ‘master’ } } allOf当嵌套条件为true时执行。必须至少包含一个。例如： when { allOf { branch ‘master’; environment name: ‘DEPLOY_TO’, value: ‘production’ } } anyOf当任意一个表达式为true时。必须至少包含一个。例如： when { anyOf { branch ‘master’; branch ‘staging’ } } 在进入阶段的代理节点之前计算when表达式 默认情况下，when 条件是在进入阶段的代理之后计算。然而，通过增加选项beforeAgent 可以改变。如果把选项beforeAgent 设置为true，就会首先计算when 条件，只有在值为true时才会进入。 示例1： Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 示例2： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 示例3： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { allOf { branch &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例4： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; anyOf { environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例5： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { expression { BRANCH_NAME ==~ /(production|staging)/ } anyOf { environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例6： pipeline { agent none stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { agent { label &quot;some-label&quot; } when { beforeAgent true branch &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 并发parallel阶段是可以并行执行的。注意，在阶段内必须要只能有一个steps 或 parallel。任何包含parallel 的阶段不能包括agent 或 tools，也不包括steps。 另外，当有一个任务失败后，你可以强制整个并行失败。只要设置参数failFast 为true就可以。 示例： pipeline { agent any stages { stage(&apos;Non-Parallel Stage&apos;) { steps { echo &apos;This stage will be executed first.&apos; } } stage(&apos;Parallel Stage&apos;) { when { branch &apos;master&apos; } failFast true parallel { stage(&apos;Branch A&apos;) { agent { label &quot;for-branch-a&quot; } steps { echo &quot;On Branch A&quot; } } stage(&apos;Branch B&apos;) { agent { label &quot;for-branch-b&quot; } steps { echo &quot;On Branch B&quot; } } } } } } 步骤脚本script 可以在申明时的流水线中执行脚本时步骤。大多数情况下 script 是用不到的。 示例： pipeline { agent any stages { stage(&apos;Example&apos;) { steps { echo &apos;Hello World&apos; script { def browsers = [&apos;chrome&apos;, &apos;firefox&apos;] for (int i = 0; i &amp;lt; browsers.size(); ++i) { echo &quot;Testing the ${browsers[i]} browser&quot; } } } } } } 脚本化流水线脚本化流水线可以使用普通的Groovy语法，因此，它可以实现很强大的功能。 在Jenkins流水线刚被开发出来时，采用Groovy作为基础。Jenkins已经很长时间内采用嵌入式的Groovy引擎提供了高级脚本功能给管理员和普通用户。也就是说，基于Groovy脚本的流水线指的就是脚本化流水线。 流程控制步骤语法对比参考https://jenkins.io/doc/book/pipeline/syntax/","tags":[{"name":"Pipeline","slug":"Pipeline","permalink":"http://yoursite.com/tags/Pipeline/"}]},{"title":"Jenkins Pipeline","date":"2018-03-22T05:02:36.000Z","path":"2018/03/22/Jenkins-Pipeline.html","text":"介绍本文介绍如何在Jenkins中使用pipeline插件。 Jenkins安装启动后，还需要安装一些插件才可以使用pipeline（流水线）的特性。你可以在系统管理–插件管理–可选插件中搜索Pipeline进行按钮；要提醒一下的是，Jenkins会自己查找依赖的插件，所以你可能看到安装的插件不只一个。Pipeline插件的wiki地址是https://wiki.jenkins.io/display/JENKINS/Pipeline+Plugin。另外，你可以通过这里https://plugins.jenkins.io/workflow-aggregator，查看该插件的依赖关系，并找到Pipeline插件在Github上的托管地址。 什么是PipelineJenkins Pipeline是一套插件，支持实现和持续集成作为流水线应用到Jenkins。Pipeline提供了一套可扩展的工具。 Pipeline大致可以分为：节点、阶段、步骤。步骤是具体的功能表达式，例如：执行shell命令等。阶段，你可以理解为步骤的集合。而节点则是包含阶段，它规定了这些阶段（步骤）都会在哪些slave上运行。 节点，可以是一个普通的slave，也可以运行在Docker容器中。 为什么要用Pipeline根本上来说，Jenkins是一个支持很多自动化模式的引擎。Pipeline增加了一套强大的工具到Jenkins中，支持用户从简单持续集成到全面的持续集成。通过模块化一些列相关的任务，用户可以利用很多Pipeline的特性。 代码：Pipelines通过代码来实现，并通常可以由版本控制系统（svn、git等）来管理。 可暂停：Pipelines可以暂停（停止），并且可以在运行之前接收人工输入或者等待同意。 Pipeline表达式Step是一个单一任务，告诉Jenkins该做什么。例如，在step中执行shell命令make。当一个插件扩展了Pipeline DSL，就意味着可以使用新的step。 Node大多数工作是在一个或者多个节点（node）中完成的。 语法Jenkins的流水线（pipeline）采用groovy语法来编写。逻辑判断、循环、异常等功能都是具备的，另外，熟悉groovy的人就明白这和Java的写法有一定的相似。 下面我介绍一些流水线的步骤（或者函数），首先介绍的是在插件workflow-basic-steps-plugin中的。我们从插件的名称上也能看到，这些流水线步骤大多是基础、简单的。首先，给出我研究时的版本信息： &lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt;` &lt;artifactId&gt;workflow-basic-steps&lt;/artifactId&gt; &lt;version&gt;2.7-SNAPSHOT&lt;/version&gt; 以便各位依据本文可以进一步学习Jenkins流水线插件的源码。 环境变量node { echo env.JENKINS_HOME sh &apos;echo $JENKINS_HOME&apos; echo env.JOB_NAME echo env.NODE_NAME echo env.NODE_LABELS echo env.WORKSPACE echo env.JENKINS_URL echo env.BUILD_URL env.SUREN_VER = &apos;12&apos; echo env.SUREN_VER } 上面的示例中，给出了如何使用内置的环境变量和自定义环境变量的做法 node() { env.JDK_HOME = &quot;${tool &apos;8u131&apos;}&quot; env.PATH=&quot;${env.JDK_HOME}/bin:${env.PATH}&quot; echo env.JDK_HOME echo env.PATH sh &apos;java -version&apos; } node(&apos;bimpm_deploytodev&apos;) { def pass_bin = &apos;/opt/pass/bin&apos; env.PASS_BIN = pass_bin stage(&apos;Clean&apos;) { sh &apos;rm -rfv $PASS_BIN&apos; } } 工具node() { tool name: &apos;JDK8_Linux&apos;, type: &apos;jdk&apos; tool name: &apos;maven339_linux_dir&apos;, type: &apos;maven&apos; echo &apos;hello&apos; } 上面的pipeline指定需要工具jdk和maven的名称（在Global Tool Configuration中配置）。 对应的实现类为ToolStep，该类被final关键字所修饰，因此是不能做扩展的了。 属性node { echo &apos;hello&apos; } properties([ buildDiscarder( logRotator( artifactDaysToKeepStr: &apos;&apos;, artifactNumToKeepStr: &apos;&apos;, daysToKeepStr: &apos;5&apos;, numToKeepStr: &apos;10&apos; ) ), pipelineTriggers([ cron(&apos;H 3,12,17 * * *&apos;) ]) ]) 拷贝成品node { stage(&apos;Copy&apos;) { step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true, projectName: &apos;BIM_PMJF/BIM-PMJF-BUILD/BIM_PMJF_DISCOVERY&apos;, selector: [$class: &apos;StatusBuildSelector&apos;, stable: false], target: &apos;/opt/pass/bin&apos;]) } } 我们通常会在一个Job里实现工程构建，在另外的Job里做程序的部署，这时候就可以用到Jenkins的成品特性。它可以实现在多个slave之间拷贝成品。实现类为ArtifactArchiverStep。 循环node(&apos;suren&apos;) { def dev_path = &apos;/opt/suren/bin&apos; def services = [ [ &apos;name&apos;: &apos;admin&apos;, &apos;project&apos;: &apos;admin&apos;, &apos;port&apos;: &apos;7002&apos;, &apos;jarName&apos;: &apos;admin&apos; ] ]; stage(&apos;Copy Artifact&apos;) { for(service in services){ step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true, projectName: service.project, selector: [$class: &apos;StatusBuildSelector&apos;, stable: false], target: dev_path + &apos;/&apos; + service.name ]) } } stage(&apos;Stop Service&apos;) { for(service in services){ sh &apos;fuser -n tcp -k &apos; + service.port + &apos; &gt; redirection &amp;&apos; } } stage(&apos;Start Service&apos;) { for(service in services){ sh &apos;cd &apos; + pass_bin + &apos;/&apos; + service.name + &apos; &amp;&amp; nohup nice java -server -Xms128m -Xmx384m \\ -jar &apos; + service.jarName + &apos;.jar \\ --server.port=&apos; + service.port + &apos; $&gt; initServer.log 2&gt;&amp;1 &amp;&apos; } } } 上面的例子，展示了如何在jenkins pipeline中调用循环语句，实现批量操作。 参数化构建properties([[$class: &apos;JobRestrictionProperty&apos;], parameters([run(description: &apos;&apos;, filter: &apos;ALL&apos;, name: &apos;Name&apos;, projectName: &apos;Project&apos;)]), pipelineTriggers([])] ) 为了能让我们的流水线定义更加具有通用性，除了可以在流水线中使用系统预定义的变量外，可以使用由用户动态输入的变量值。当流水线Job加入参数化后，在执行任务时候就必须有用户输入一系列值才可以执行。 并行node { stage(&apos;Start Service&apos;) { parallel &apos;test&apos;: { echo &apos;test&apos; }, &apos;deply&apos;: { echo &apos;deply&apos; } } parallel &apos;one&apos; : { stage(&apos;one&apos;) { echo &apos;one&apos; } }, &apos;two&apos; : { stage(&apos;two&apos;) { echo &apos;two&apos; } } } parallel &apos;one&apos;: { node{ stage(&apos;one&apos;) { echo &apos;one&apos; } } }, &apos;two&apos;: { node { stage(&apos;two&apos;) { echo &apos;two&apos; } } } Jenkins的流水线同时支持节点（node）、阶段（stage）和步骤（step）之间的并行执行。如果多个节点并发执行的话，并发数量会少于当前可用的执行器（exector）数量。 超时node { stage(&apos;stage2&apos;) { timeout(time: 600, unit: &apos;SECONDS&apos;) { sleep 20 echo &apos;2&apos; } } } 遇到可能执行时间会比较长的情况，可以通过超时来约定最长的执行时间。 对应的实现类为TimeoutStep。 下面介绍的函数在插件workflow-durable-task-step-plugin中，版本信息如下： &lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt; &lt;artifactId&gt;workflow-durable-task-step&lt;/artifactId&gt; &lt;version&gt;2.18-SNAPSHOT&lt;/version&gt; 工作空间当你希望在一个流水线中，对多个工程（例如git工程）做构建以及部署等操作，如果不切换工作空间的话就会发生代码错乱的问题。你可以参考下面的示例代码来解决这个问题： node{ stage(&apos;suren&apos;){ ws(&apos;suren-a-work&apos;) { pwd } ws(&apos;suren-b-work&apos;) { pwd } } } 实现类为WorkspaceStep，使用final修饰，无法扩展。 执行节点Jenkins里可能会配置很多节点（node），而不一定所有的节点都满足你的构建环境要求，这时候就需要来指定节点了： node(&apos;local&apos;) { echo &apos;hello&apos; } properties([ buildDiscarder( logRotator( artifactDaysToKeepStr: &apos;&apos;, artifactNumToKeepStr: &apos;&apos;, daysToKeepStr: &apos;5&apos;, numToKeepStr: &apos;10&apos; ) ), pipelineTriggers([ cron(&apos;H 3,12,17 * * *&apos;) ]) ]) 上面的pipeline指定了运行节点的label为local。 实现类为ExecutorStep，使用final修饰，无法扩展。 异常捕获node{ stage(&apos;suren&apos;){ try{ trigger }catch(error){ echo error.getMessage() } } } 这里调用了一个不存在的流水线函数，然后使用catch来捕获并打印错误信息。 stashnode { stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true) } node(&apos;jenkins-slave&apos;) { unstash(name: &apos;test&apos;) } pipeline的文件存储（stash）这个功能，可以在流水线需要运行在多个节点（node）的情况下使用。stash和unstash会把存储的文件从一个节点转移到另一个节点上。上面给出的例子中，把所有的xml文件从master转移到了当前执行任务的slaver节点上。 node(&apos;jenkins-slave&apos;){ checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]]) } node { def path = JENKINS_HOME + &apos;/jobs/&apos; + JOB_NAME + &apos;/builds/&apos; + BUILD_ID echo path dir(path){ stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true) } } node(&apos;jenkins-slave&apos;) { unstash(name: &apos;test&apos;) } 敏感信息我们可以利用Jenkins的Credentials机制，在Pipeline中传递密码等敏感信息，例如： pipeline { agent any stages{ stage(&apos;test&apos;) { steps{ withCredentials([usernamePassword(credentialsId: &apos;aaa&apos;, passwordVariable: &apos;passwd&apos;, usernameVariable: &apos;user&apos;)]) { sh &apos;&apos;&apos;echo $user $passwd&apos;&apos;&apos; } } } } } 文件读取很多情况下，我们需要读取文件内容。 获取pom.xml版本号，获取groupId（需要的插件Pipeline Utility Steps）： node { stage(&apos;test&apos;){ checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]]) pom = readMavenPom file: &apos;pom.xml&apos; echo pom.version echo pom.groupId echo pom.artifactId } } 使用Docker在docker容器中执行任务： pipeline { agent { docker { image &apos;eclipse/mysql&apos; args &apos;-e MYSQL_ROOT_PASSWORD=root&apos; } } stages { stage(&apos;test&apos;) { steps { sh &apos;mysql&apos; } } } } withDockerContainer withDockerServer dockerFingerprintRun withDockerRegistry dockerFingerprintFrom 其他插件另外有一些比较好的Jenkins流水线插件，给出推荐： 支持从SCM加载库文件 https://github.com/suren-jenkins/workflow-remote-loader-plugin 远程调试安装环境： sudo apt-get install -y npm jenkins-pipeline --file test.groovy --url http://localhost:8080/jenkins/job/MyJob --credentials admin:123456 参考https://github.com/spring-cloud/spring-cloud-pipelines","tags":[{"name":"Pipeline","slug":"Pipeline","permalink":"http://yoursite.com/tags/Pipeline/"}]},{"title":"初探Jenkins X","date":"2018-03-21T13:42:25.000Z","path":"2018/03/21/Preliminary-Jenkins-X.html","text":"Jenkins 于 3月21日 发布了名为Jenkins X的项目，这一项目对开发人员和云端的 CI/CD 环境之间的交互过程进行了审视和反思，结合自动化、工具链以及 DevOps 最佳实践。为开发团队提供了新的生产效率增长点。 Jenkins X是什么？“X”注定是一个不平凡的名字，Jenkins X 对于整个Jenkins生态而言也是不平凡的存在。 简而言之，Jenkins X 是一个高度集成化的CI/CD平台，基于Jenkins和Kubernetes实现，旨在解决微服务体系架构下的云原生应用的持续交付的问题，简化整个云原生应用的开发、运行和部署过程。 你猜的没错，Jenkins X 只能在Kubernetes集群上运行，这有并不意外。Kubernetes已然成为了容器编排的一枝独秀，各大厂商纷纷转向Kubernetes，发布了自己的公有云、操作系统或PaaS平台。 另外，微服务和云原生应用解决方案也日臻成熟，以Spring Boot为代表的一系列体系框架也开始走到舞台中央。 与此同时，随着应用架构的细分和服务间的解耦，服务具备了独立发布的能力，这也使得微服务架构下的持续交付成为业界所关注的热门领域，我们需要更加灵活的CI/CD自动化解决方案，以应对越发快速的交付需求。 注：Jenkins的企业版CloudBees，已经加入CNCF（云原生计算）基金会 看到这里，你是不是觉得Jenkins X 就是个基于Kubernetes的持续交付平台呢？ 那你就大错特错了，因为Jenkins X想要实现的远非如此而已！ 试想如下场景： 越来越多的工具和实践，工程师们需要会写Kubernetes YAML，Dockerfile，Jenkinsfile，对微服务、云原生、Kubernetes和Jenkins非常熟悉。 臣妾做不到呀！ 而在Jenkins X的世界中，这一切都是通过命令完成。 可以说Jenkins X重新思考了未来云原生应用下研发工程师和CI/CD的交付方式，通过整合工具，自动化和DevOps最佳实践，改善了研发过程中的复杂环节，让研发可以专注于价值创造，其他的事情通通交给Jenkins X来帮你解决。 神奇吗？ 的确，在第一次看到项目演示的时候，我也惊叹世界的变化如此之快，在Jenkins X的设计中，整合了Helm，Draft，GitOps，以及Nexus，chartmuseum，monocular等诸多新系统和工具，从而实现自动构建编译环境，生成容器镜像，流水线，自动化部署，并通过简单的Review实现不同环境间的自动发布。 这一切都被完美的封装在简单的jx命令之后。同时你也无需担心对内部实现细节的失控，因为一切都被妥善的版本控制，可以自定义和修改，可以说Jenkins X为你实现了自动化的CI/CD和DevOps最佳实践，持续交付不再是难事，进而提升生产力，实现促进企业的业务成功！ Jenkins X 部分新特性1. 自动化一切：自动化CI/CD流水线 选择项目类型自动生成Jenkinsfile定义流水线 自动生成Dockerfile并打包容器镜像 自动创建Helm Chart并运行在Kubernetes集群 自动关联代码库和流水线，作为代码变更自动触发（基于Webhook实现） 自动版本号自动归档 2. Review代码一键部署应用：基于GitOps的环境部署 所有的环境，应用列表，版本，配置信息统一放在代码库中进行版本控制 通过Pull Request实现研发和运维的协同，完成应用部署升级（Promotion） 可自动部署和手动部署，在必要的时候增加手工Review 当然这些都封装在jx命令中实现 3. 自动生成预览环境和信息同步反馈 预览环境用于代码Review环节中临时创建 同Pull Request工作流程集成并实现信息同步和有效通知 验证完毕后自动清理 提交和应用状态自动同步到Github注释 自动生成release notes信息供验证 Jenkins X 核心组件 JenkinsJenkins X不是一个全新的Jenkins。 他依然使用Jenkins作为持续交付的核心引擎，实际上Jenkins X作为Jenkins的一个子项目存在，专注于云原生应用的CI/CD实现，同时也帮助Jenkins自身完成云原生应用的转型，毕竟现在越来越多的人在诟病单体应用的设计和文件存储系统。 在之前同Jenkins创始人和核心骨干的交流中，我们也了解到Jenkins已经开始着手改变。 HELMHelm是用于管理Kubernetes资源对象的工具，类似APT，YUM和HOMEBREW，他通过将Kubernetes的资源对象打包成Chart的形式，完成复杂应用的部署和版本控制，是目前业界流行的解决方案 DRAFTDraft是自动化应用构建和运行在Kubernetes上面的工具，具有语言识别能力，能够自动生成构建脚本，依赖，环境并打包成docker镜像并部署在Kubernetes集群上，加快代码开发节奏，而无需关心基础设施层面的技术实现 GitOpsGitOps是weaveworks推出的天才的应用部署解决方案，他将Git作为整个应用部署的单一可信数据源（SSOT），通过类似代码开发的Pull Request流程完成应用部署的Review和自动化实现，并且将部署配置信息纳入版本控制。 Jenkins X 安装试用先决条件工具 helm kubectl git Kubernetes 集群 互联网连接 Tiller 公网 IP github 账号 安装 jxhttp://jenkins-x.io/getting-started/install/ 提供了几种系统下的安装说明： OS X：brew tap jenkins-x/jx &amp;&amp; brew tap jenkins-x/jx Linux：curl -L https://github.com/jenkins-x/jx/releases/download/v1.1.10/jx-darwin-amd64.tar.gz | tar xzv &amp;&amp; mv jx /usr/local/bin jx installjx create cluster 支持多种公有云的创建。 配置好集群和对应的 kubeconfig 访问之后，就可以使用jx install进行安装了。 过程中几个需要注意的点： 如果 Tiller 的 SA 权限不足，会导致安装失败，可设置相应的 ClusterRole 进行解决。 安装过程会修改 kubeconfig 文件，因此建议做好备份。 为完整体验功能，建议听从安装器建议，安装 Ingress Controller。 Jenkins X 的环境管理以及代码拉取等功能需要和 Github 进行交互，因此会提问 GitHub 的 Token。 安装过程相对较长，可以使用watch kubectl get pods -n jx查看进程状况。 最后步骤会显示管理密码，注意复制保存。 安装完成JX 会为用户建立三个环境分别是，Dev、Staging 以及 Production。 运行命令jx console，会打开浏览器进入 Jenkins 登录页面。 登录之后我们会看到正在进行构建，如果是一个排队状态，可能是因为正在创建 Worker Pod，可以使用kubectl查询具体情况。 构建完成，会看到这一示例中包含了拉取、构建、Helm、环境等几个步骤，可以作为工作的基础环节来进行使用。 应用接下来就可以做几个善后工作 jx 支持插件，可以通过jx get addons查看支持的插件列表，进行安装。 根据实际工作需要，对缺省环境进行调整，安装所需软件。 对 Jenkins X 中的软件、集群进行安全加固。 使用import或者create spring/create quickstart，进行项目工作。 最后要注意的一点是，Jenkins X 目前的升级频率非常高。不建议生产使用。","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://yoursite.com/tags/Jenkins/"}]},{"title":"浅谈服务治理、微服务与Service Mesh（二） Spring Cloud从入门到精通到放弃【转载】","date":"2018-03-19T14:36:59.000Z","path":"2018/03/19/spring-cloud-start-to-give-up.html","text":"作为本系列文章的第二篇(第一篇链接请戳：浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生)，本文主要为大家介绍下微服务概念中非常火热的Spring Cloud开发框架。由于网上关于Spring Cloud的文章多如牛毛，为了让大家阅读后能有不一样的收获，因此本文将用一个相对轻松的叙述方式来为大家讲解一下Spring Cloud框架和微服务。虽然不可能通过一篇文章让大家对Spring Cloud做到从“入门到精通到放弃”，但是希望大家通过阅读本文能对Spring Cloud和微服务有一个更加清晰的认识和了解，为后面学习Service Mesh做好一个铺垫。 Spring Cloud 之“出身名门望族”作为当下最火热的微服务框架，Spring Cloud的名字可以说是无人不知、无人不晓，凭借之前Spring Framework的良好群众基础和Cloud这个具有时代感的名字，Spring Cloud一出现便被大家认知。 提到Spring Cloud，便会让人想起刚刚发布了2.0版本的Spring Boot。Spring Boot和Spring Cloud都是出自Pivotal公司，Spring Boot和Spring Cloud虽然火热，但是了解Pivotal公司的人在国内却是不多。实际上Pivotal公司在云计算、大数据、虚拟化等领域都有所建树，这里先给大家简单八卦下Pivotal的情况。 Pivotal公司是由EMC和VMware联合成立的一家公司，GE（通用电气）也对Pivotal进行了股权收购，同时GE也是Pivotal的一个重要大客户。除了Spring Framework、Spring Boot和Spring Cloud之外，我们日常开发中经常使用的Reids、RabbitMQ、Greenplum、Gemfire、Cloud Foundry等，目前都是归属于Pivotal公司的产品。其中Gemfire也是被中国铁路总公司12306使用的分布式内存数据库，也就是说你过年回家买不到火车票，这个锅Pivotal的Gemfire也会跟着一起背（开个小玩笑，哈哈）。 Spring Cloud 之“入门”Spring Cloud作为一个微服务的开发框架，其包括了很多的组件，包括：Spring Cloud Netflix（Eureka、Hystrix、Zuul、Archaius）、Spring Cloud Config、Spring Cloud Bus、Spring Cloud Cluster、Spring Cloud Consul、Spring Cloud Security、Spring Cloud Sleuth、Spring Cloud Data Flow、Spring Cloud Stream、Spring Cloud Task、Spring Cloud ZooKeeper、Spring Cloud Connectors、Spring Cloud Starters、Spring Cloud CLI等。 在上述组件中，Spring Cloud Netflix是一套微服务的核心框架，由互联网流媒体播放商Netflix开源后并入Spring Cloud大家庭，它提供了的微服务最基础的功能：服务发现（Service Discovery）、动态路由（Dynamic Routing）、负载均衡（Load Balancing）和边缘服务器（Edge Server）等。 Spring Boot是Spring的一套快速配置脚手架，可以基于Spring Boot快速开发单个微服务。Spring Boot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、生产级别的Spring应用。由于Spring Cloud是基于Spring Boot进行的开发，因此使用Spring Cloud就必须使用到Spring Boot。 下图是一个常见的关于Spring Cloud的架构图。下面此图为例，对Spring Cloud最常用的几个组件做一个简单的介绍： Eureka：服务注册中心，一个基于REST的服务，用于定位服务，以实现微服务架构中服务发现和故障转移。 Hystrix：熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。 Turbine：Turbine是聚合服务器发送事件流数据的一个工具，用来监控集群下Hystrix的Metrics情况。 Zuul：API网关，Zuul是在微服务中提供动态路由、监控、弹性、安全等边缘服务的框架。 Ribbon：提供微服务中的负载均衡功能，有多种负载均衡策略可供选择，可配合服务发现和断路器使用。 Feign：Feign是一种声明式、模板化的HTTP客户端。 Spring Cloud Config：配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。 Spring Cloud Security：基于Spring Security的安全工具包，为微服务的应用程序添加安全控制。 Spring Cloud Sleuth：日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。 除了上面介绍的基础组件外，常见的Spring Cloud组件还有非常多种，涉及到了微服务以及应用开发的方方面面： Spring Cloud Starters：Spring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。 Archaius：配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。 Consul：封装了Consul操作，Consul是一个服务发现与配置工具，与Docker容器可以无缝集成。 Spring Cloud Stream：数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 Spring Cloud CLI：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 Spring Cloud Task：提供云端计划任务管理、任务调度。 Spring Cloud Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与 Spring Cloud Config 联合实现热部署。 Spring Cloud Data Flow：大数据操作工具，作为Spring XD的替代产品，它是一个混合计算模型，结合了流数据与批量数据的处理方式。 Spring Cloud ZooKeeper：操作ZooKeeper的工具包，用于使用ZooKeeper方式的服务发现和配置管理。 Spring Cloud Connectors：便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。 Spring Cloud之“精通”Spring Cloud虽然集成了众多组件，可以构建一个完整的微服务应用，但是其中的各个组件却并非完美无缺，很多组件在实际应用中都存在诸多不足和缺陷。因此，需要我们对其中的一些组件进行替换和修改，方能构建一个强大、灵活、健壮的微服务架构应用。 配置中心Spring Cloud Config可以说是Spring Cloud家族中实现最Low的一个组件，直接采用了本地存储/SVN/Git的方式进行存储。同时，Spring Cloud Config也缺乏一个完整的可视化管理查询后台，当存在比较复杂的权限管理和版本管理需求时，Spring Cloud Config会显得非常力不从心。如果需要在配置修改后，能自动进行配置信息推送的话，使用Spring Cloud Config也无法满足要求，需要自行编写代码进行实现。 目前开源社区中，已经有了很多的开源配置中心实现方案，同时很多公司也自研了自己的配置中心方案。包括淘宝的统一配置中心Diamond（已经多年未更新）、百度的分布式配置管理平台Disconf、携程的开源分布式配置中心Apollo、360的分布式配置管理工具QConf等等。目前，笔者公司采用的是自己公司自研的配置中心，没有采用开源实现的主要原因是因为需要同时适配Spring Cloud和Dubbo等多种场景的应用。 注册中心作为Spring Cloud的服务注册中心，从分布式CAP理论来看，Eureka采用是AP型设计，强调的是注册中心的高可用性。和Dubbo常用的服务注册中心ZooKeeper相比，ZooKeeper则是采用的CP型设计，强调的是注册中心数据的一致性。 Eureka的设计确实简单易用，但是默认没有实现对注册中心数据的持久化。同时，在极端场景下，也会出现多个Eureka注册中心节点数据不一致，甚至服务注册数据丢失的情况。当然，从分布式CAP理论来看，理论上是没办法做到同时兼顾CAP三点的。目前也有一些互联网公司对Eureka进行了改造，支持了数据的持久化，但是尚不能完整的支持CAP的全部要求。 API网关API网关可以说是微服务需求最多，也是最有难点的一个组件。Spring Cloud中集成的Zuul应该说更多的是实现了服务的路由功能，对于负载均衡等其他功能，需要结合Ribbon等组件来实现。对于很多个性化的需求，需要开发者自己来进行编码实现。 和大部分基于Java的Web应用类似，Zuul也采用了Servlet架构，因此Zuul处理每个请求的方式是针对每个请求是用一个线程来处理。同时，由于Zuul是基于JVM的实现，因此性能也会在高并发访问场景下成为瓶颈。虽然网上一些文章评测Zuul和Nginx性能接近，但是在性能要求较高的场景下，JVM的内存管理和垃圾回收问题，仍然是一个很大的问题。所以在实际的应用场景中，通常会采用在多个Zuul几点前面再添加一层Nginx或者OpenResty来进行代理。 为了解决Zuul的性能问题，Netflix将自己的网关服务Zuul进行了升级，新的Zuul 2将HTTP请求的处理方式从同步变成了异步，并且新增诸如HTTP/2、websocket等功能。但是遗憾的是，开源版本的Zuul 2一直处于难产状态中，始终没有和大家正式见面。 熔断器微服务中对于服务的限流、降级、熔断的需求是多种多样的，需要在API网关和各个具体服务接口中分别进行控制，才能满足复杂场景下微服务架构的应用需求。 单独使用Spring Cloud中的Hystrix无法完整的满足上述的复杂需求，需要结合API网关，并通过Kubernetes对资源、进程和命名空间来提供隔离，并通过部分自定义编码方能实现对全部服务的限流、降级、熔断等需求。 监控系统无论是Spring Cloud中集成的Spring Cloud Sleuth，还是集成经典的ELK，都只是对日志级别的追踪和监控。在大中型微服务应用架构中，尤其是基于JVM的项目，还需要添加APM的监控机制，才能保证及时发现各种潜在的性能问题。 APM整体上主要完成3点功能：1.日志追踪、2.监控报警、3.性能统计。目前，国内外商业版本的APM方案已经有很多，开源版本的APM方案也开始丰富起来。国内开源的APM方案主要有：大众点评的CAT和Apache孵化中的SkyWalking。这里给大家重点推荐下SkyWalking，SkyWalking是针对分布式系统的应用性能监控系统，特别针对微服务、Cloud Native和容器化（Docker、Kubernetes、Mesos）架构，项目的关注度和发展速度都很快，中文文档资料也比较齐全。 Spring Cloud之“放弃”Spring Cloud可以说是一个完美的微服务入门框架，如果你是在一个中小型项目中应用Spring Cloud，那么你不需要太多的改造和适配，就可以实现微服务的基本功能。但是如果是在大型项目中实践微服务，可能会发现需要处理的问题还是比较多，尤其是项目中老代码比较多，没办法全部直接升级到Spring Boot框架下开发的话，你会非常希望能有一个侵入性更低的方案来实施微服务架构。在这种场景下，Service Mesh将会成为你的最佳选择，经过一段时间的发展，目前Service Mesh这个概念已经开始逐步被大家了解和认知。同时，一些Service Mesh的实现方案也逐步成熟和落地，例如Istio、Linkerd、Envoy等。在本系列文章的下一篇中，将为大家对Service Mesh概念做一个系统的介绍。但是在了解Service Mesh概念之前，还是建议大家先对微服务和Spring Cloud这些概念和框架有一个深入的了解，这样才能体会到应用Service Mesh的价值和意义。 Spring Cloud与Dubbo网上关于Spring Cloud和Dubbo对比的文章很多，大多数对比结果都是Spring Cloud压倒性优势战胜Dubbo，下表是对Dubbo和Spring Cloud做的一个基础功能的对比： 实际上，Dubbo的关注点在于服务治理，并不能算是一个真正的微服务框架。包括目前在开发中的Dubbo 3.0，也不能完整覆盖微服务的各项功能需求。而Spring Cloud一方面是针对微服务而设计，另外一方面Spring Cloud是通过集成各种组件的方式来实现微服务，因此理论上可以集成目前业内的绝大多数的微服务相关组件，从而实现微服务的全部功能。 而对Dubbo而言，如果一定要应用到微服务的使用场景中的话，上表中欠缺的大多数功能都可以通过集成第三方应用和组件的方式来实现，跟Spring Cloud相比主要的缺陷在于集成过程中的便利性和兼容性等问题。 Spring Cloud与Docker虽然网上也有很多文章写到如何使用Docker来实现微服务，但是事实上单独使用Docker是没办法完整的实现微服务的所有功能的。在实际上微服务架构中，Spring Cloud和Docker更多的是一种协作的关系，而不是一种竞争的关系。通过Docker容器化技术，可以更好的解决引入Spring Cloud微服务后带来的部署和运维的复杂性。 Spring Cloud生态圈中的Pivotal Cloud Foundry（PCF）作为PaaS实现，也提供一些类似于Docker的功能支持，但是无论上功能上还是易用性上和Docker还是存在比较大的差异。Pivotal Cloud Foundry和Docker之间的关系更多的是一种兼容关系，而不是竞争关系，Pivotal Cloud Foundry的主要竞争对手是Red Hat的OpenShift。目前，Pivotal Cloud Foundry支持的IaaS包括：AWS、AZURE、GCP、vSphere、OpenStack等。 Spring Cloud与Kubernetes网上也有一些“Spring Cloud与Kubernetes哪个更好”，“当已经有了Kubernetes之后，还需要使用Spring Cloud么”之类的文章。首先说笔者并不认为Spring Cloud与Kubernetes是竞争关系，但是也不否认二者确实在诸多功能上存在一些重合。下图是对Spring Cloud与Kubernetes在微服务架构中的一些基础功能上的对比： 通过对比可以看出，Spring Cloud和Kubernetes确实存在一些功能上的重合，但是二者的定位其实差别很大。Spring Cloud是一个基于Java语言的微服务开发框架，而Kubernetes是一个针对容器应用的自动化部署、伸缩和管理的开源系统，它兼容多种语言且提供了创建、运行、伸缩以及管理分布式系统的原语。Spring Cloud更多的是面向有Spring开发经验的Java语言开发者，而Kubernetes不是一个针对开发者的平台，它的目的是供有DevOps思想的IT人员使用。 为了区分Spring Cloud和Kubernetes两个项目的范围，下面这张图列出了几乎是端到端的微服务架构需求，从最底层的硬件，到最上层的DevOps和自服务经验，并且列出了如何关联到Spring Cloud和Kubernetes平台。 总结通过Spring Cloud、Docker和Kubernetes的组合，可以构建更加完整和强大的微服务架构程序。通过三者的整合，使用Spring Boot提供应用的打包，Docker和Kubernetes提供应用的部署和调度。Spring Cloud通过Hystrix线程池提供应用内的隔离，而Kubernetes通过资源、进程和命名空间来提供隔离。Spring Cloud为每个微服务提供健康终端，而Kubernetes执行健康检查，且把流量导到健康服务。Spring Cloud外部化配置并更新它们，而Kubernetes分发配置到每个微服务。 对于一名开发人员或者架构师来说，想要精通微服务设计与开发，能够在大中型项目中应用微服务架构，单纯掌握Spring Cloud是远远不够的，Docker和Kubernetes等都是需要学习和掌握的内容。同时，由于采用微服务架构后带来了分布式的相关问题，对于分布式系统理论也必须有一定的了解。当然，最重要的还是对系统业务的深入理解，对整体业务进行合理的规划和拆分，才能真正行之有效的应用微服务架构，构建高效、健壮、灵活、可扩展的微服务应用。 参考链接： https://springcloud.cc https://www.kubernetes.org.cn https://my.oschina.net/u/3677020/blog/1570248 http://blog.csdn.net/rickiyeat/article/details/60792925 http://www.uml.org.cn/wfw/201711271.asp https://projects.spring.io/spring-cloud/","tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生【转载】","date":"2018-03-19T09:44:57.000Z","path":"2018/03/19/dubbo-past-and-present.html","text":"本系列文章将为大家介绍当下最流行的服务治理、微服务等相关内容，从服务治理、SOA、微服务到最新的服务网格（Service Mesh）进行综合介绍和分析。易商阜极自2017年便积极引进微服务的先进理念，运用在项目实践中，为项目集成带来了显著效果。本文将以Dubbo为例，向为大家介绍SOA、服务治理等概念，以及Dubbo的基础知识和最新发展情况。 SOA与服务治理SOA（面向服务的体系结构）概念由来已久，在10多年前便开始进入到我们广大软件开发者的视线中。SOA是一种粗粒度、松耦合服务架构，服务之间通过简单、精确定义接口进行通讯，不涉及底层编程接口和通讯模型。SOA可以看作是B/S模型、Web Service技术之后的自然延伸。 服务治理，也称为SOA治理，是指用来管理SOA的采用和实现的过程。以下是在2006年时IBM对于服务治理要点的总结： 服务定义（服务的范围、接口和边界） 服务部署生命周期（各个生命周期阶段） 服务版本治理（包括兼容性） 服务迁移（启用和退役） 服务注册中心（依赖关系） 服务消息模型（规范数据模型） 服务监视（进行问题确定） 服务所有权（企业组织） 服务测试（重复测试） 服务安全（包括可接受的保护范围） 限于当时的技术发展水平，广大软件设计与开发人员对于SOA和服务治理的技术认知还主要停留在Web Service和ESB总线等技术和规范上，并没有真正在软件开发中得以充分落地。 Dubbo开源直到2011年10月27日，阿里巴巴开源了自己的SOA服务化治理方案的核心框架Dubbo，服务治理和SOA的设计理念开始逐渐在国内软件行业中落地，并被广泛应用。Dubbo作为阿里巴巴内部的SOA服务化治理方案的核心框架，在2012年时已经每天为2000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用，其中既有当当网、网易考拉等互联网公司，也有中国人寿、青岛海尔等传统企业。 Dubbo简介Dubbo是一个高性能服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，使得应用可通过高性能RPC实现服务的输出和输入功能，和Spring框架可以无缝集成。 作为一个分布式服务框架，以及SOA治理方案，Dubbo其功能主要包括：高性能NIO通讯及多协议集成，服务动态寻址与路由，软负载均衡与容错，依赖分析与服务降级等。Dubbo最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。 Dubbo包含远程通讯、集群容错和自动发现三个核心部分。提供透明化的远程方法调用，实现像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入。同时具备软负载均衡及容错机制，可在内网替代F5等硬件负载均衡器，降低成本，减少单点。可以实现服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者。 下图来自从Dubbo官网，描述了服务注册中心、服务提供方、服务消费方、服务监控中心之间的调用关系，具体如下图所示： 节点角色说明： 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 Dubbo总体架构Dubbo框架设计共划分了10层，最上面的Service层是留给实际使用Dubbo开发分布式服务的开发者实现业务逻辑的接口层。图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。 各层说明： Config配置层：对外配置接口，以ServiceConfig、ReferenceConfig为中心，可以直接初始化配置类，也可以通过Spring解析配置生成配置类。 Proxy服务代理层：服务接口透明代理，生成服务的客户端Stub和服务器端Skeleton，以ServiceProxy为中心，扩展接口为ProxyFactory。 Registry注册中心层：封装服务地址的注册与发现，以服务URL为中心，扩展接口为RegistryFactory、Registry、RegistryService。 Cluster路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以Invoker为中心，扩展接口为Cluster、Directory、Router、LoadBalance。 Monitor监控层：RPC调用次数和调用时间监控，以Statistics为中心，扩展接口为MonitorFactory、Monitor、MonitorService。 Protocol远程调用层：封将RPC调用，以Invocation、Result为中心，扩展接口为Protocol、Invoker、Exporter。 Exchange信息交换层：封装请求响应模式，同步转异步，以Request、Response为中心，扩展接口为Exchanger、ExchangeChannel、ExchangeClient、ExchangeServer。 Transport网络传输层：抽象MINA和Netty为统一接口，以Message为中心，扩展接口为Channel、Transporter、Client、Server、Codec。 Serialize数据序列化层：可复用的一些工具，扩展接口为Serialization、ObjectInput、ObjectOutput、ThreadPool。 模块分包 各模块说明： dubbo-common公共逻辑模块：包括Util类和通用模型。 dubbo-remoting远程通讯模块：相当于Dubbo协议的实现，如果RPC用 RMI协议则不需要使用此包。 dubbo-rpc远程调用模块：抽象各种协议，以及动态代理，只包含一对一的调用，不关心集群的管理。 dubbo-cluster集群模块：将多个服务提供方伪装为一个提供方，包括：负载均衡、容错、路由等，集群的地址列表可以是静态配置的，也可以是由注册中心下发。 dubbo-registry注册中心模块：基于注册中心下发地址的集群方式，以及对各种注册中心的抽象。 dubbo-monitor监控模块：统计服务调用次数、调用时间的、调用链跟踪的服务。 dubbo-config配置模块：是Dubbo对外的API，用户通过Config使用Dubbo，隐藏Dubbo所有细节。 dubbo-container容器模块：是一个Standlone的容器，以简单的Main加载Spring启动，因为服务通常不需要Tomcat/JBoss等Web容器的特性，没必要用Web容器去加载服务。 协议支持 Dubbo协议（默认协议） Hessian协议 HTTP协议 RMI协议 WebService协议 Thrift协议 Memcached协议 Redis协议 注册中心（1）Multicast注册中心：Multicast注册中心不需要启动任何中心节点，只要广播地址一样，就可以互相发现。组播受网络结构限制，只适合小规模应用或开发阶段使用。组播地址段：224.0.0.0 - 239.255.255.255。 （2）ZooKeeper注册中心（推荐）：ZooKeeper是Apacahe子项目，是一个树型的目录服务，支持变更推送，适合作为Dubbo服务的注册中心，可用于生产环境。 对上图流程说明如下： 服务提供者（Provider）启动时，向/dubbo/com.foo.BarService/providers目录下写入URL。 服务消费者（Consumer）启动时，订阅/dubbo/com.foo.BarService/providers目录下的URL，向/dubbo/com.foo.BarService/consumers目录下写入自己的URL。 监控中心（Monitor）启动时，订阅/dubbo/com.foo.BarService目录下的所有提供者和消费者URL。 （3）Redis注册中心：阿里内部并没有采用Redis做为注册中心，而是使用自己实现的基于数据库的注册中心，即：Redis注册中心并没有在阿里内部长时间运行的可靠性保障，此Redis桥接实现只为开源版本提供，其可靠性依赖于Redis本身的可靠性。 （4）Simple注册中心：Simple注册中心本身就是一个普通的Dubbo服务，可以减少第三方依赖，使整体通讯方式一致。只是简单实现，不支持集群，可作为自定义注册中心的参考，但不适合直接用于生产环境。 远程通信与信息交换远程通信需要指定通信双方所约定的协议，在保证通信双方理解协议语义的基础上，还要保证高效、稳定的消息传输。Dubbo继承了当前主流的网络通信框架，主要包括如下几个： Mina Netty（默认） Grizzly 停止维护从2012年10月23日Dubbo 2.5.3发布后，在Dubbo开源将满一周年之际，阿里基本停止了对Dubbo的主要升级。只在之后的2013年和2014年更新过2次对Dubbo 2.4的维护版本，然后停止了所有维护工作。Dubbo对Srping的支持也停留在了Spring 2.5.6版本上。 分支出现在阿里停止维护和升级Dubbo期间，当当网开始维护自己的Dubbo分支版本Dubbox，支持了新版本的Spring，并对外开源了Dubbox。同时，网易考拉也维护了自己的独立分支Dubbok，可惜并未对外开源。 重获新生经过多年漫长的等待，随着微服务的火热兴起，在国内外开发者对阿里不再升级维护Dubbo的吐槽声中，阿里终于开始重新对Dubbo的升级和维护工作。在2017年9月7日 ，阿里发布了Dubbo的2.5.4版本，距离上一个版本2.5.3发布已经接近快5年时间了。在随后的几个月中，阿里Dubbo开发团队以差不多每月一版本的速度开始快速升级迭代，修补了Dubbo老版本多年来存在的诸多bug，并对Spring等组件的支持进行了全面升级。 分支合并在2018年1月8日，Dubbo 2.6.0版本发布，新版本将之前当当网开源的Dubbo分支Dubbox进行了合并，实现了Dubbo版本的统一整合。 Dubbo与Spring Cloud阿里巴巴负责主导了 Dubbo 重启维护的研发工程师刘军在接受采访时表示：当前由于 RPC 协议、注册中心元数据不匹配等问题，在面临微服务基础框架选型时Dubbo与Spring Cloud是只能二选一，这也是为什么大家总是拿Dubbo和Spring Cloud做对比的原因之一。Dubbo之后会积极寻求适配到Spring Cloud生态，比如作为Spring Cloud的二进制通信方案来发挥Dubbo的性能优势，或者Dubbo通过模块化以及对http的支持适配到Spring Cloud。 未来展望2018年1月8日，Dubbo创始人之一梁飞在Dubbo交流群里透露了Dubbo 3.0正在动工的消息。Dubbo 3.0内核与Dubbo 2.0完全不同，但兼容Dubbo 2.0。Dubbo 3.0将以Streaming为内核，不再是Dubbo时代的RPC，但是RPC会在Dubbo 3.0中变成远程Streaming对接的一种可选形态。Dubbo 3.0将支持可选Service Mesh，多加一层IPC，这主要是为了兼容老系统，而内部则会优先尝试内嵌模式。代理模式Ops可独立升级框架，减少业务侵入，而内嵌模式可以带业务测试、部署节点少、稳定性检测方便。同时，可以将Dubbo 3.0启动为独立进程，由dubbo-mesh进行IPC，路由、负载均衡和熔断机制将由独立进程控制。 总结从 Dubbo 新版本的路线规划上可以看出，新版本的Dubbo在原有服务治理的功能基础上，将全面拥抱微服务和Service Mesh。同时，考虑到在阿里云已经有了Dubbo的商业版本，在未来一段时间内，Dubbo的更新与维护应该不会再长时间中断。在我们进行服务治理以及微服务架构设计时，新版本Dubbo对我们广大开发者来说都将会是一个不错的选择。 参考链接 http://dubbo.io https://github.com/alibaba/dubbo http://shiyanjun.cn/archives/325.html http://mp.weixin.qq.com/s/eVYx-tUIMYtAk5wP-qkdkw 原文链接：浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生","tags":[{"name":"Dubbo,Service Mesh,微服务,ZooKeeper","slug":"Dubbo-Service-Mesh-微服务-ZooKeeper","permalink":"http://yoursite.com/tags/Dubbo-Service-Mesh-微服务-ZooKeeper/"}]},{"title":"如何选择 Linux 上的跟踪器","date":"2018-03-18T14:38:01.000Z","path":"2018/03/18/choosing-a-linux-tracer.html","text":"tracer是一个高级的性能分析和诊断工具，但是不要让这名词唬住你，如果你使用过 strace 和 tcpdump，其实你就已经使用过跟踪器了。系统跟踪器可以获取更多的系统调用和数据包。它们通常能跟踪任意的内核和应用程序。 有太多的 Linux 跟踪器可以选择。每一种都有其官方的（或非官方的）的卡通的独角兽吉祥物，足够撑起一台”儿童剧”了。 那么我们应该使用哪个跟踪器呢？ 可分为两类：大多数人和性能/内核工程师。 对于大多数人大多数人 (开发者，系统管理员，开发管理者，运维人员，评测人员，等等) 不关心系统追踪器的细节。下面是对于追踪器你应该知道和做的： 1. 使用 perf_events 分析 CPU 性能使用 perf_events 做 CPU 性能分析。性能指标可以使用 flame graph 等工具做可视化。 git clone --depth 1 https://github.com/brendangregg/FlameGraph perf record -F 99 -a -g -- sleep 30 perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; perf.svg Linux perf_events (又称 “perf”，同命令名) 是 Linux 用户的官方跟踪器和性能分析器。内置于内核代码，有很好维护（近来获得快速增强），通常通过 linux-tools-common 软件包安装。 perf 有很多功能，如果只能推荐一个，我选择 CPU 性能分析。尽管这只是采样，而不是从技术上追踪事件。最难的部分是获取完整的栈和信息，我为 java 和 node.js 做的一个演讲 Linux Profiling at Netflix 中已经说过这个问题。 2. 了解其他的跟踪器正如我一个朋友说的：“你不需要知道如何操作 X 射线机器，但是一旦你吞了一枚硬币，你得知道这得去做 X 射线”，你应该了解各种跟踪器都能做什么，这样就能在你工作中真正需要跟踪器的时候，你既可以选择稍后学习使用，也可以雇相应的人来完成。 简短来说：几乎所有的东西都可以使用跟踪器来进行分析和跟踪。如，文件系统内部、TCP/IP 过程、设备驱动、应用程序内部。可以看一下我的个人网站上关于 ftrace 的文章，还有我写的关于 perf_events 文档介绍，可以做为一个追踪（或者性能分析）的例子。 3. 寻求前端支持工具如果你正想买一个能支持跟踪 Linux 的性能分析工具（有许多卖这类工具的公司）。想像一下，只需要直接点击一下界面就能“洞察”整个系统内核，包括隐藏的不同堆栈位置的热图，我在 Monitorama talk 中介绍了一个这样带图形界面的工具。 我开源了一些我自己开发的前端工具，尽管只是命令行界面而不是图形界面。这些工具也会让人们更加快速容易的使用跟踪器。比如下面的例子，用我的 perf_tool，跟踪一个新进程: # ./execsnoop Tracing exec()s. Ctrl-C to end. PID PPID ARGS 22898 22004 man ls 22905 22898 preconv -e UTF-8 22908 22898 pager -s 22907 22898 nroff -mandoc -rLL=164n -rLT=164n -Tutf8 [...] 在 Netflix 上，我们创建了一个 Vector，一个分析工具的实例，同时也是 Linux 上的跟踪器的最终前端。 对于性能/内核工程师我们的工作变的越来越困难，很多的人会问我们怎么样去追踪，哪种跟踪器可以用！为了正确理解一个跟踪器，你经常需要花上至少100个小时才能做到。理解所有的 linux 跟踪器去做出理性的选择是一个浩大的工程。（我可能是唯一一个快做到这件事情的人） 这里是我的建议，可以二选其一： A) 选中一个全能的跟踪器，并且使它标准化，这将涉及花费大量的时间去弄清楚它在测试环境中的细微差别和安全性。我现在推荐 SystemTap 的最新版本（可以从源代码构建）。我知道有些公司已经选用 LTTng，而且他们用的很好，尽管它不是非常的强大（虽然它更安全）。如果 Sysdig 可以增加追踪点tracepoint或者 kprobes，可以做为另一个候选。 B) 遵循我上面提供的流程图，它将意味着尽可能更多的使用 ftrace 或者 perf_event， 并整合 eBPF，之后其他的跟踪器像 SystemTap/LTTng 会去填补剩下的空白。 这就是我目前在 Netflix 做的工作。 对跟踪器的评价1. ftrace我喜欢用 ftrace，它是内核 hacker 的首选，内置于系统内核，可以使用跟踪点（静态检查点），能调用内核 kprobes 和 uprobes 调试工具。并且提供几个这样的功能：带可选过滤器和参数的事件追踪功能；在内核中进行统计的事件计数和定时功能；还有函数流程遍历的功能。可以看一下内核代码中 ftrace.txt 例子了解一下。ftrace 由 /sys 控制，仅支持单一的 root 用户使用（但是你可以通过缓冲区实例改成支持多用户）。某些时候 ftrace 的操作界面非常繁琐，但是的确非常“hack”，而且它有前端界面。ftace 的主要作者 Steven Rostedt 创建了 trace-cmd 命令工具，而我创建了 perf 的工具集。我对这个工具最大的不满就是它不可编程。举例来说，你不能保存和获取时间戳，不能计算延迟，不能把这些计算结果保存成直方图的形式。你需要转储事件至用户层，并且花一些时间去处理结果。ftrace 可以通过 eBPF 变成可编程的。 2. perf_eventsperf_events 是 Linux 用户的主要跟踪工具，它内置在内核源码中，通常通过 linux-tools-commom 安装。也称为“perf”，即其前端工具名称，它通常用来跟踪和转储信息到一个叫做 perf.data 的文件中，perf.data 文件相当于一个动态的缓冲区，用来保存之后需要处理的结果。ftrace 能做到的，perf_events 大都也可以做到，perf-events 不能做函数流程遍历，少了一点儿“hack”劲儿（但是对于安全/错误检查有更好的支持）。它可以进行 CPU 分析和性能统计，用户级堆栈解析，也可以使用对于跟踪每行局部变量产生的调试信息。它也支持多用户并发操作。和 ftrace 一样也不支持可编程。如果要我只推荐一款跟踪器，那一定是 perf 了。它能解决众多问题，并且它相对较安全。 3. eBPFextended Berkeley Packet Filter（eBPF）是一个可以在事件上运行程序的高效内核虚拟机（JIT）。它可能最终会提供 ftrace 和 perf_events 的内核编程，并强化其他的跟踪器。这是 Alexei Starovoitov 目前正在开发的，还没有完全集成，但是从4.1开始已经对一些优秀的工具有足够的内核支持了，如块设备 I/O 的延迟热图。可参考其主要作者 Alexei Starovoitov 的 BPF slides 和 eBPF samples。 4. SystemTapSystemTap 是最强大的跟踪器。它能做所有事情，如概要分析，跟踪点，探针，uprobes（来自SystemTap），USDT 和内核编程等。它将程序编译为内核模块，然后加载，这是一种获取安全的巧妙做法。它也是从 tree 发展而来，过去有很多问题（崩溃或冻结）。很多不是 SystemTap 本身的错——它常常是第一个使用某个内核追踪功能，也是第一个碰到 bug 的。SystemTap 的最新版本好多了（必须由源代码编译），但是很多人仍然会被早期版本吓到。如果你想用它，可先在测试环境中使用，并与 irc.freenode.net 上 的 #systemtap 开发人员交流。（Netflix 有容错机制，我们已经使用了 SystemTap，但是可能我们考虑的安全方面的问题比你们少。）我最大的不满是，它似乎认为你应该有内核 debug 信息，但是经常没有。实际上没有它也能做很多事情，但是缺少文档和例子（我必须自己全靠自己开始学习）。 5. LTTngLTTng 优化了事件采集，这比其他跟踪器做得好，它也支持几种事件类型，包括 USTD。它从 tree 发展而来，它的核心很简单：通过一组小规模的固定指令集将事件写入追踪缓冲区，这种方式使它安全、快速，缺点是它没有内核编码的简单途径。我一直听说这不是一个大问题，因为尽管需要后期处理，它也已经优化到可以充分的度量。此外，它还首创了一个不同的分析技术，对所有关注事件的更多黑盒记录将能够稍后以 GUI 的方式进行研究。我关心的是前期没有考虑到要录制的事件缺失问题如何解决，但我真正要做的是花更多时间来看它在实践中用的怎么样。这是我花的时间最少的一个跟踪器（没有什么特殊原因）。 6. Ktapktap 是一款前景很好的跟踪器，它使用内核中的 lua 虚拟机处理，在没有调试信息的情况下在嵌入式设备上运行的很好。这让它得到了关注，并在有一段时间似乎超过了 Linux 上所有的追踪器。然后 eBPF 开始集成到内核了，而 ktap 的集成会在可以使用 eBPF 替代它自己的虚拟机后才开始。因为 eBPF 仍将持续集成几个月，ktap 开发者要继续等上一段时间。我希望今年晚些时候它能重新开发。 7. dtrace4linuxdtrace4linux 主要是 Paul Fox 一个人在业余时间完成的，它是 Sun DTrace 的 Linux 版本。它引人瞩目，已经有一些供应器provider可以工作，但是从某种程度上来说还不完整，更多的是一种实验性的工具（不安全）。我认为，顾忌到许可证问题，人们会小心翼翼的为 dtrace4linux 贡献代码：由于当年 Sun 开源DTrace 使用的是 CDDL 协议，而 dtrace4linux 也不大可能最终进入 Linux kernel。Paul 的方法很可能会使其成为一个 add-on。我很乐意看到 Linux 平台上的 DTrace 和这个项目的完成，我认为当我加入 Netflix 后将会花些时间来协助完成这个项目。然而，我还是要继续使用内置的跟踪器，如 ftrace 和 perf_events。 8. OL DTraceOracle Linux DTrace 为了将 DTrace 引入 Linux，特别是为 Oracle Linux，做出了很大的努力。这些年来发布的多个版本表明了它的稳定进展。开发者们以一种对这个项目的前景看好的态度谈论着改进 DTrace 测试套件。很多有用的 供应器provider 已经完成了，如：syscall, profile, sdt, proc, sched 以及 USDT。我很期待 fbt（function boundary tracing，用于内核动态跟踪）的完成，它是 Linux 内核上非常棒的 供应器provider。OL DTrace 最终的成功将取决于人们对运行 Oracle Linux（为技术支持付费）有多大兴趣，另一方面取决于它是否完全开源：它的内核元件是开源的，而我没有看到它的用户级别代码。 9. sysdigsysdig 是一个使用类 tcpdump 语法在系统事件上操作的新跟踪器，它使用 lua 进行后期处理。它很优秀，它见证了系统跟踪领域的变革。它的局限性在于它只在当前进行系统调用，将所有事件转储为用户级别用于后期处理。你可以使用系统调用做很多事情，然而我还是很希望它能支持跟踪点、kprobe 和 uprobe。我还期待它能支持 eBPF 做内核摘要。目前，sysdig 开发者正在增加容器支持。留意这些内容。","tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]}]