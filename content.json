[{"title":"应用容器化和与Kubernetes适配的7条军规【转载】","date":"2018-03-30T01:08:53.000Z","path":"2018/03/30/Container-7-regulations.html","text":"导读本文来自于Red Hat咨询顾问Bilgin Ibryam所编写的一篇白皮书，名为《PRINCIPLES OF CONTAINER-BASED APPLICATION DESIGN》。已被被Kubernetes官网转载。白皮书在Red Hat官网的下载地址：https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper 本微信文章是我是对这篇文章的学习和整理。 先回顾经典的软件设计原则： 保持简单，愚蠢（KISS） 不要重复自己（DRY） 适可而止 （YAGNI） 关注点分离（SoC） 单一责任, 开放封闭, 里氏替换, 迪米特法则，接口分离, 依赖倒置（SOLID） 然后是Red Hat的云原生容器设计原则： 单一关注性原则（SCP） 高度可观测性原则（HOP） 生命周期一致性原则（LCP） 镜像不可变性原则（IIP） 进程可处置性原则（PDP） 自包含性原则（S-CP） 运行时约束性原则（RCP） 很多组织都理解云原生的重要性和必要性，但是并不知道从哪里开始。那么请确保：云原生平台和容器化应用能无缝的运行在一起，并且具备抵御故障的能力，甚至在底层的基础架构出现宕机的时候，也能通过过弹性扩展的方式表现出可靠性。本文描述了容器化应用时需要遵循的基本准则，实施这些原则有助于使之与云原生平台Kubernetes更加适配。 1、单一关注性原则SINGLE CONCERN PRINCIPLE（SCP） 在许多方面，单一关注性原则与来自SOLID的SRP是类似的，它建议一个类应该只有一个责任。SRP背后的动机是每个责任是变更的一个轴心，一个类应该有，且也只有一个需要改变的理由。SCP原则中的“关注”一词强调关注是一种更高层次的抽象的责任，而且它更好地将范围描述为一个容器而不是一个类。虽然SRP的主要动机是变化原因的唯一性，而SCP的主要动机是容器镜像重用和可替换性。如果你创建一个解决单个问题的容器，并且以功能完整的方式来实现，不同应用程序中的容器镜像重用的可能性就会更高。 因此，SCP原则规定每个集容器都应该解决一个问题，并做得很好。 实现这一点，通常比在面向对象的世界中实现SRP更容易，容器通常管理的一个单一的进程，大多数情况下一个进程解决一个问题。 如果你的容器化微服务需要解决多个问题，它可以使用这样的模式，将多个容器用sidecar和init-containers的模式合并成一个部署单元（pod），这样每个容器仍然是处理单个问题。同样，您可以替换处理同样问题的容器。 例如，将Web服务器容器或队列实现容器，更新为更具可扩展性的容器。 2、高度可观测性原则HIGH OBSERVABILITY PRINCIPLE（HOP） 容器提供了一种统一的方式来打包和运行应用程序，将它们视为一个黑盒子对象。 但任何旨在成为云原生公民的容器都必须提供API支持，要为运行时环境编写接口（API），以观察容器的健康状况和行为。 这是自动化容器更新和生命周期回收的基本先决条件和统一的方式，从而提高系统的弹性和用户体验。 实际上，您的容器化应用程序必须至少为其提供不同类型的健康检查的API–活动和就绪等状态。更好的应用程序的行为则必须提供其他手段来观察容器化应用程序的状态。应用程序应该将重要事件记录到标准错误（STDERR）和标准输出（STDOUT）中，从而通过统一的日志聚合工具（诸如Fluentd和Logstash之类的工具）进行分析，并与跟踪和指标收集库相结合，例如OpenTracing，Prometheus等。 将您的应用程序视为黑盒子，但实施所有必要的API以帮助平台对其进行观测，并以最佳方式管理您的应用程序。 3、生命周期一致性原则LIFE-CYCLE CONFORMANCE PRINCIPLE（LCP） HOP规定了你的容器提供供平台观测的API。 LCP则规定：您的应用程序有办法读取来自平台的事件。 此外，除了获得事件以外，容器还应该对这些事件相应地作出反应。这就是此原则名字由来。这几乎就像在应用程序通过一个“写入API”与平台进行交互。 来自管理平台的各种事件都是为了帮助您管理您的容器的生命周期的。决定处理哪些事件取决于您的应用程序 以及是否对这些事件做出反应。 但有些事件比其他事件更重要。例如，任何需要一个干净的关闭进程，这就需要捕获信号：终止（SIGTERM）消息，并尽可能迅速关闭。 这是为了避免通过强制关闭信号：kill（SIGKILL），之后跟随一个SIGTERM。 还有其他事件，例如PostStart和PreStop，可能对您的应用程序生命周期管理也非常重要。 例如，某些应用程序需要在服务之前进行预热请求和一些需要在关闭干净之前释放资源。 4、镜像不可变性原则IMAGE IMMUTABILITY PRINCIPLE（IIP） 容器化的应用程序是不可变更的，镜像一旦完成了构建，预计在不同的环境中运行都不会改变。这意味着在因外部环境的不同，在需要的时候需要使用外部手法处理所依赖的外部配置数据，而不是每个环境修改或者构建不同的容器。而容器应用程序中的任何变更，都应该因此触发构建新的容器映像，并在所有环境中重用它。相同于这个原理的，不可变服务器和不可变基础架构的概念也很受欢迎，并且对于服务器/主机管理也是如此。 在遵循IIP原则的情况下，应该防止为不同的环境创建相似的容器镜像，要始终坚持为所有环境只配置一个容器映像。 这个原则允许在应用程序更新期间，采用自动回滚和前滚等做法，这是云原生自动化的重要方面。 5、进程可处置性原则PROCESS DISPOSABILITY PRINCIPLE（PDP） 迁移到容器应用程序的主要动机之一是：容器需要尽可能做到临时性，并做好在任何时候被另一个容器实例替换的准备。需要更换容器的原因有很多，比如：健康检查失败、缩容、应用程序将容器迁移到不同的主机，平台资源匮乏或其它的问题。 这意味着容器化的应用程序必须保持其状态为向外扩展的或分布式和冗余的。这也意味着应用程序应该快速启动和关闭，甚至为彻底的硬件故障做好准备。 实施这一原则的另一个有用的做法是创建小容器。 容器在云原生环境可以自动调度并在不同的主机上启动。较小的容器可以实现更快启动时间，因为在重新启动之前容器镜像需要被物理地复制到主机系统。 6、自包含性原则SELF-CONTAINMENT PRINCIPLE（S-CP） 这个原则规定一个容器应该在构建时包含所有需要的东西。容器的存在应该仅仅依赖于Linux®内核，在并添加相关额外的库，在容器构建时加入它们。除了库之外，它还应该包含语言运行时，应用程序平台（如果需要），以及运行所需的其他依赖关系，等运行容器化应用所需要的诸如此类的东西。 唯一的例外是：由于不同环境之间差异，并且只能在运行时提供的配置; 例如，通过Kubernetes提供的ConfigMap。 某些应用程序由多个容器组件组成。 例如，容器化的Web应用程序也可能需要数据库容器。 根据这个原则，并不建议合并两个容器。相反，它建议的是数据库容器只包含运行数据库所需的所有内容，Web应用程序容器只包含运行Web应用程序所需的所有内容，如Web服务器。 在运行时，Web应用程序容器将根据需要依赖于并访问数据库容器。 7、运行时约束性原则RUNTIME CONFINEMENT PRINCIPLE（RCP） S-CP从构建时的角度查看容器，并关注于生成的二进制文件及其内容。但是容器不仅仅是磁盘上一个只有尺寸大小的单一维度的黑盒子。 容器运行时有多个维度，例如内存使用维度，CPU使用维度等资源消耗维度。 这个RCP原则建议每个容器申报资源需求，并发送信息到平台。它应该分享容器的资源配置文件，从CPU，内存，网络，磁盘的角度声明。这影响到平台如何执行调度，自动扩展，容量 管理以及容器常规的服务级别协议（SLA）等。 除了向平台声明容器的资源需求之外，还有一点也很重要， 应用被约束在使用所声明的资源需求内。如果应用程序对资源的使用保持在约束的范围内，则当资源匮乏发生时，平台不太可能将其终止和迁移。 8、结论云原生不仅仅是一种最终状态 – 它也是一种工作方式。 本份白皮书描述了一系列容器应用的基本原则，必须遵守才能成为优秀的云原生公民。 除了这些原则之外，创建良好的容器应用程序还需要熟悉其他容器相关的最佳实践和技术。 尽管上述原则非常根本，适用于大多数用例，下面列出的最佳实践在应用和不应用的时候，则需要判断力。以下是一些与容器相关的更常见的最佳实践： 镜像要尽可能的小。 通过清理临时文件，并避免安装不必要的软件包来构建小尺寸镜像。 这减少了容器的尺寸，构建时间和复制容器镜像的网络传输时间。 支持任意用户ID。 避免使用sudo命令或要求特定用户名运行你的容器。 标记重要的端口。 虽然可以在运行时指定端口号，然而使用EXPOSE命令在运行的时候指定，则可以让镜像的使用者更轻松。 为持久数据使用卷。 在容器摧毁之后还需要保存的容器数据的，必须将数据写入一个数据卷。 设置镜像元数据。 以标签和注释形式存在的镜像元数据可以使您的容器镜像更加实用，从而为使用您的容器的开发人员提供了更好的体验。 使主机和镜像同步。 一些容器应用需要容器在某些属性（如时间和机器ID）上与主机同步。 这里是指向各种模式和最佳实践的资源的链接，以帮助您能有效地实现上述目标： https://www.slideshare.net/luebken/container-patternshttps://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practiceshttp://docs.projectatomic.io/container-best-practiceshttps://docs.openshift.com/enterprise/3.0/creating_images/guidelines.htmlhttps://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdfhttps://leanpub.com/k8spatterns/https://12factor.net","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://yoursite.com/tags/Kubernetes/"}]},{"title":"微服务入门系列(五)：服务部署【转载】","date":"2018-03-28T07:35:15.000Z","path":"2018/03/28/Service-deployment.html","text":"当我们完成业务代码的开发后，就需要进入部署阶段。在部署过程中，我们将会引入持续集成、持续交付、持续部署，并且阐述如何在微服务中使用他们。 1. 持续集成、持续部署、持续交付在介绍这三个概念之前，我们首先来了解下使用了这三个概念之后的软件开发流程，如下图所示： 首先是代码的开发阶段，当代码完成开发后需要提交至代码仓库，此时需要对代码进行编译、打包，打包后的产物被称为“构建物”，如：对Web项目打包之后生成的war包、jar包就是一种构建物。此时的构建物虽然没有语法错误，但其质量是无法保证的，必须经过一系列严格的测试之后才能具有部署到生产环境的资格。我们一般会给系统分配多套环境，如开发环境、测试环境、预发环境、生产环境。每套环境都有它测试标准，当构建物完成了一套环境的测试，并达到交付标准时，就会自动进入下一个环境。构建物依次会经过这四套环境，构建物每完成一套环境的验证，就具备交付给下一套环境的资格。当完成预发环境的验证后，就具备的上线的资格。 测试和交付过程是相互伴随的，每一套环境都有各自的测试标准。如在开发环境中，当代码提交后需要通过编译、打包生成构建物，在编译的过程中会对代码进行单元测试，如果有任何测试用例没通过，整个构建流程就会被中止。此时开发人员需要立即修复问题，并重新提交代码、重新编译打包。 当单元测试通过之后，构建物就具备了进入测试环境的资格，此时它会被自动部署到测试环境，进行新一轮的测试。在测试环境中，一般需要完成接口测试和人工测试。接口测试由自动化脚本完成，这个过程完成后还需要人工进行功能性测试。人工测试完成后，需要手动触发进入下一个阶段。 此时构建物将会被部署到预发环境。预发环境是一种“类生产环境”，它和生产环境的服务器配置需要保持高度一致。在预发环境中，一般需要对构建物进行性能测试，了解其性能指标是否能满足上线的要求。当通过预发验证后，构建物已经具备了上线的资格，此时它可以随时上线。 上述过程涵盖了持续集成、持续交付、持续部署，那么下面我们就从理论角度来介绍这三个概念。 1.1 持续集成“集成”指的是修改后/新增的代码向代码仓库合并的过程，而“持续集成”指的是代码高频率合并。这样有什么好处呢？大家不妨想一想，如果我们集成代码的频率变高了，那么每次集成的代码量就会变少，由于每次集成的时候都会进行单元测试，从而当出现问题的时候问题出现的范围就被缩小的，这样就能快速定位到出错的地方，寻找问题就更容易了。此外，频繁集成能够使问题尽早地暴露，这样解决问题的成本也就越低。因为在软件测试中有这样一条定律，时间和bug修复的成本成正比，也就是时间越长，bug修复的成本也就越大。所以持续集成能够尽早发现问题，并能够及时修复问题，这对于软件的质量是非常重要的。 1.2 持续部署“持续部署”指的是当存在多套环境时，当构建物完成上一套环境的测试后，自动部署到下一套环境并进行一系列的测试，直到构建物满足上线的要求为止。 1.3 持续交付当系统通过了所有的测试之后，就具备了部署到生产环境的资格，这个过程也就被称为“交付”。“持续交付”指的是每个版本的构建物都具有上线的资格，这就要求每当代码库中有新的版本后，都需要自动触发构建、测试、部署、交付等一系列流程，当构建物在某个阶段的测试未通过时，就需要开发人员立即解决这个问题，并重新构建，从而保证每个版本的构建物都具备上线的资格，可以随时部署到生产环境中。 2. 微服务与持续集成当我们了解了持续集成后，下面来介绍微服务如何与持续集成相整合。当我们对系统进行了微服务化后，原本单一的系统被拆分成多个可独立运行的微服务。单服务系统的持续集成较为简单，代码库、构建和构建物之间都是一对一的关系。然而，当我们将系统微服务化后，持续集成就变得复杂了。下面介绍两种在微服务中使用持续集成的方法，分别是单库多构建和多库多构建，并依次介绍这两种方式的优缺点及使用场景。 2.1 单库多构建“单库”指的是单个代码仓库，即整个系统的多个模块的代码均由一个代码仓库维护。“多构建”指的是持续集成平台中的构建项目会有多个，每个构建都会生成一个构建物，如下如所示： 在这种持续集成的模式中，整个项目的所有代码均在同一个代码仓库中维护。但在持续集成平台中，每一项服务都有各自独立的构建，从而持续集成平台能够为每一项服务产出各自的构建物。 这种持续集成的模式在微服务架构中显然是不合理的。首先，一个系统的可能会有很多服务构成，如果将这些服务的代码均在同一个代码仓库中维护，那么一个程序员在开发服务A代码的时候很有可能会因为疏忽，修改了服务B的代码，此时服务B构建之后就会存在安全隐患，如果这个问题在服务B上线前被发现，那么还好，但无疑增加了额外的工作量；但如果这个问题及其隐讳，导致之前的测试用例没有覆盖到，从而服务B会带着这个问题进入生产环境，这可能会给企业带来巨大的损失。所以，在微服务架构中，尽量选择多库多构建模式来实现持续集成，它将带来更大的安全性。 虽然这种模式不合理，但它也有存在的必要性，当我们在项目建设初期的时候，这种模式会给我们带来更多的便利性。因为项目在建设初期，服务之间的边界往往是比较模糊的，而且需要经过一段时间的演化才能够构建出稳定的边界。所以如果在项目建设初期直接使用微服务架构，那么服务边界频繁地调整会极大增加系统开发的复杂度，你要知道，在多个系统之间调整边界比在单个系统的多个模块之间调整边界的成本要高很多。所以在项目建设初期，我们可以使用单服务结构，服务内部采用模块作为未来各个微服务的边界，当系统演化出较为清晰、稳定的边界后再将系统拆分成多个微服务。此时代码在同一个代码仓库中维护是合理的，这也符合敏捷开发中快速迭代的理念。 2.2 多库多构建 当系我们的系统拥有了稳定、清晰的边界后，就可以将系统向微服务架构演进。与此同时，持续集成模式也可以从单库多构建向多库多构建演进。 在多库多构建模式中，每项服务都有各自独立的代码仓库，代码仓库之间互不干扰。开发团队只需关注属于自己的某几项服务的代码仓库即可。每一项服务都有各自独立的构建。这种方式逻辑清晰，维护成本较低，而且能避免单库多构建模式中出现的影响其他服务的问题。 3. 微服务构建物持续集成平台对源码编译、打包后生成的产物称为“构建物”。根据打包的粒度不同，可以将构建物分为如下三种：平台构建物、操作系统构建物和镜像构建物。 3.1 平台构建物平台构建物指的是由某一特定平台生成的构建物，比如JVM平台生成的Jar包、War包，Python生成的egg等都属于平台构建物。但平台构建物运行需要部署在特定的容器中，如war需要运行在Servlet容器中，而Servlet容器又依赖的JVM环境。所以若要部署平台构建物，则需要先给它们提供好运行所需的环境。 3.2 操作系统构建物操作系统构建物是将系统打包成一个操作系统可执行程序，，如CentOS的RPM包、Windows的MSI包等。这些安装包可以在操作系统上直接安装运行。但和平台构建物相同的是，操作系统构建物往往也需要依赖于其他环境，所以也需要在部署之前搭建好安装包所需的依赖。此外，配置操作系统构建物的复杂度较大，构建的成本较高，所以一般不使用这种方式，这里仅作介绍。 3.3 镜像构建物平台构建物和操作系统构建物都有一个共同的缺点就是需要安装构建物运行的额外依赖，增加部署复杂度，而镜像构建物能很好地解决这个问题。 我们可以把镜像理解成一个小型操作系统，这个操作系统中包含了系统运行所需的所有依赖，并将系统也部署在这个“操作系统”中。这样当持续集成平台构建完这个镜像后，就可以直接运行它，无需任何依赖的安装，从而极大简化了构建的复杂度。但是，镜像往往比较庞大，构建镜像的过程也较长，从而当我们将生成的镜像从持续集成服务器发布到部署服务器的时间将会很长，这无疑降低了部署的效率。不过好在Docker的出现解决了这一问题。持续集成平台在构建过程中并不需要生成一个镜像，而只需生成一个镜像的Dockerfile文件即可。Dockerfile文件用命令定义了镜像所包含的内容，以及镜像创建的过程。从而持续集成服务器只需将这个体积较小的镜像文件发布到部署服务器上即可。然后部署服务器会通过docker build命令基于这个Dockerfile文件创建镜像，并创建该镜像的容器，从而完成服务的部署。 相对于平台构建物和操作系统构建物而言，镜像构建物在部署时不需要安装额外的环境依赖，它把环境依赖的配置都在持续集成平台构建Dockerfile文件时完成，从而简化了部署的过程。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(四)：数据库的服务化切分【转载】","date":"2018-03-28T07:17:47.000Z","path":"2018/03/28/Database-Service-Segmentation.html","text":"1. 什么是“分库分表”？随着大数据时代的到来，业务系统的数据量日益增大，数据存储能力逐渐成为影响系统性能的瓶颈。目前主流的关系型数据库单表存储上限为1000万条记录，而这一存储能力显然已经无法满足大数据背景下的业务系统存储要求了。随着微服务架构、分布式存储等概念的出现，数据存储问题也渐渐迎来了转机。而数据分片是目前解决海量数据持久化存储与高效查询的一种重要手段。数据分库分表的过程在系统设计阶段完成，要求系统设计人员根据系统预期的业务量，将未来可能出现瓶颈的数据库、数据表按照一定规则拆分成多个库、多张表。这些数据库和数据表需要部署在不同的服务器上，从而将数据读写压力分摊至集群中的各个节点，提升数据库整体处理能力，避免出现读写瓶颈的现象。 目前数据分片的方式一共有两种：离散分片和连续分片。 离散分片是按照数据的某一字段哈希取模后进行分片存储。只要哈希算法选择得当，数据就会均匀地分布在不同的分片中，从而将读写压力平均分配给所有分片，整体上提升数据的读写能力。然而，离散存储要求数据之间有较强的独立性，但实际业务系统并非如此，不同分片之间的数据往往存在一定的关联性，因此在某些场景下需要跨分片连接查询。由于目前所有的关系型数据库出于安全性考虑，均不支持跨库连接。因此，跨库操作需要由数据分库分表中间件来完成，这极大影响数据的查询效率。此外，当数据存储能力出现瓶颈需要扩容时，离散分片规则需要将所有数据重新进行哈希取模运算，这无疑成为限制系统可扩展性的一个重要因素。虽然，一致性哈希能在一定程度上减少系统扩容时的数据迁移，但数据迁移问题仍然不可避免。对于一个已经上线运行的系统而言，系统停止对外服务进行数据迁移的代价太大。 第二种数据分片的方式即为连续分片，它能解决系统扩容时产生的数据迁移问题。这种方式要求数据按照时间或连续自增主键连续存储。从而一段时间内的数据或相邻主键的数据会被存储在同一个分片中。当需要增加分片时，不会影响现有的分片。因此，连续分片能解决扩容所带来的数据迁移问题。但是，数据的存储时间和读写频率往往呈正比，也就是大量的读写往往都集中在最新存储的那一部分数据，这就会导致热点问题，并不能起到分摊读写压力的初衷。 2. 数据库扩展的几种方式数据库扩展一共有四种分配方式，分别是：垂直分库、垂直分表、水平分表、水平数据分片。每一种策略都有各自的适用场景。 1、垂直分库垂直分库即是将一个完整的数据库根据业务功能拆分成多个独立的数据库，这些数据库可以运行在不同的服务器上，从而提升数据库整体的数据读写性能。这种方式在微服务架构中非常常用。微服务架构的核心思想是将一个完整的应用按照业务功能拆分成多个可独立运行的子系统，这些子系统称为“微服务”，各个服务之间通过RPC接口通信，这样的结构使得系统耦合度更低、更易于扩展。垂直分库的理念与微服务的理念不谋而合，可以将原本完整的数据按照微服务拆分系统的方式，拆分成多个独立的数据库，使得每个微服务系统都有各自独立的数据库，从而可以避免单个数据库节点压力过大，影响系统的整体性能，如下图所示。 2、垂直分表垂直分表如果一张表的字段非常多，那么很有可能会引起数据的跨页存储，这会造成数据库额外的性能开销，而垂直分表可以解决这个问题。垂直分表就是将一张表中不常用的字段拆分到另一张表中，从而保证第一章表中的字段较少，避免出现数据库跨页存储的问题，从而提升查询效率。而另一张表中的数据通过外键与第一张表进行关联，如下图所示。 3、水平分表如果一张表中的记录数过多（超过1000万条记录），那么会对数据库的读写性能产生较大的影响，虽然此时仍然能够正确地读写，但读写的速度已经到了业务无法忍受的地步，此时就需要使用水平分表来解决这个问题。水平分表是将一张含有很多记录数的表水平切分，拆分成几张结构相同的表。举个例子，假设一张订单表目前存储了2000万条订单的数据，导致数据读写效率极低。此时可以采用水平分表的方式，将订单表拆分成100张结构相同的订单表，分别叫做order_1、order_2……、order_100。然后可以根据订单所属用户的id进行哈希取模后均匀地存储在这100张表中，从而每张表中只存储了20万条订单记录，极大提升了订单的读写效率，如下图所示。 当然，如果拆分出来的表都存储在同一个数据库节点上，那么当请求量过大的时候，毕竟单台服务器的处理能力是有限的，数据库仍然会成为系统的瓶颈，所以为了解决这个问题，就出现了水平数据分片的解决方案。 4、水平分库分表水平数据分片与数据分片区别在于：水平数据分片首先将数据表进行水平拆分，然后按照某一分片规则存储在多台数据库服务器上。从而将单库的压力分摊到了多库上，从而避免因为数据库硬件资源有限导致的数据库性能瓶颈，如下图所示。 分库分表的几种方式目前常用的数据分片策略有两种，分别是连续分片和离散分片。 1、离散分片离散分片是指将数据打散之后均匀地存储在逻辑表的各个分片中，从而使的对同一张逻辑表的数据读取操作均匀地落在不同库的不同表上，从而提高读写速度。离散分片一般以哈希取模的方式实现。比如：一张逻辑表有4个分片，那么在读写数据的时候，中间件首先会取得分片字段的哈希值，然后再模以4，从而计算出该条记录所在的分片。在这种方法中，只要哈希算法选的好，那么数据分片将会比较均匀，从而数据读写就会比较均匀地落在各个分片上，从而就有较高的读写效率。但是，这种方式也存在一个最大的缺陷——数据库扩容成本较高。采用这种方式，如果需要再增加分片，原先的分片算法将失效，并且所有记录都需要重新计算所在分片的位置。对于一个已经上线的系统来说，行级别的数据迁移成本相当高，而且由于数据迁移期间系统仍在运行，仍有新数据产生，从而无法保证迁移过程数据的一致性。如果为了避免这个问题而停机迁移，那必然会对业务造成巨大影响。当然，如果为了避免数据迁移，在一开始的时候就分片较多的分片，那需要承担较高的费用，这对于中小公司来说是无法承受的。 2、连续分片连续分片指的是按照某一种分片规则，将某一个区间内的数据存储在同一个分片上。比如按照时间分片，每个月生成一张物理表。那么在读写数据时，直接根据当前时间就可以找到数据所在的分片。再比如可以按照记录ID分片，这种分片方式要求ID需要连续递增。由于Mysql数据库单表支持最大的记录数约为1000万，因此我们可以根据记录的ID，使得每个分片存储1000万条记录，当目前的记录数即将到达存储上限时，我们只需增加分片即可，原有的数据无需迁移。连续分片的一个最大好处就是方便扩容，因为它不需要任何的数据迁移。但是，连续分片有个最大的缺点就是热点问题。连续分片使得新插入的数据集中在同一个分片上，而往往新插入的数据读写频率较高，因此，读写操作都会集中在最新的分片上，从而无法体现数据分片的优势。 引入分库分表中间件后面临的问题1、跨库操作在关系型数据库中，多张表之间往往存在关联，我们在开发过程中需要使用JOIN操作进行多表连接。但是当我们使用了分库分表模式后，由于数据库厂商处于安全考虑，不允许跨库JOIN操作，从而如果需要连接的两张表被分到不同的库中后，就无法使用SQL提供的JOIN关键字来实现表连接，我们可能需要在业务系统层面，通过多次SQL查询，完成数据的组装和拼接。这一方面会增加业务系统的复杂度，另一方面会增加业务系统的负载。因此，当我们使用分库分表模式时，需要根据具体的业务场景，合理地设置分片策略、设置分片字段，这将会在本文的后续章节中介绍。 2、分布式事务我们知道，数据库提供了事务的功能，以保证数据一致性。然而，这种事务只是针对单数据库而言的，数据库厂商并未提供跨库事务。因此，当我们使用了分库分表之后，就需要我们在业务系统层面实现分布式事务。关于分布式事务的详细内容，可以参考笔者的另一篇文章《常用的分布式事务解决方案》。 现有分库分表中间件的横向对比 Cobar实现数据库的透明分库，让开发人员能够在无感知的情况下操纵数据库集群，从而简化数据库的编程模型。然而Cobar仅实现了分库功能，并未实现分表功能。分库可以解决单库IO、CPU、内存的瓶颈，但无法解决单表数据量过大的问题。此外，Cobar是一个独立运行的系统，它处在应用系统与数据库系统之间，因此增加了额外的部署复杂度，增加了运维成本。 为了解决上述问题，Cobar还推出了一个Cobar-Client项目，它只是一个安装在应用程序的Jar包，并不是一个独立运行的系统，一定程度上降低了系统的复杂度。但和Cobar一样，仍然只支持分库，并不支持分表，也不支持读写分离。 MyCat是基于Cobar二次开发的数据库中间件，和Cobar相比，它增加了读写分离的功能，并修复了Cobar的一些bug。但是，MyCat和Cobar一样，都是一套需要独立部署的系统，因此会增加部署的复杂度，提高了后期系统运维的成本。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(三)：微服务架构下的分布式事务解决方案【转载】","date":"2018-03-28T05:55:06.000Z","path":"2018/03/28/Distributed-transaction-solutions.html","text":"分布式事务的解决方案有如下几种： 全局消息 基于可靠消息服务的分布式事务 TCC 最大努力通知 方案1：全局事务（DTP模型）全局事务基于DTP模型实现。DTP是由X/Open组织提出的一种分布式事务模型——X/Open Distributed Transaction Processing Reference Model。它规定了要实现分布式事务，需要三种角色： AP：Application 应用系统它就是我们开发的业务系统，在我们开发的过程中，可以使用资源管理器提供的事务接口来实现分布式事务。 TM：Transaction Manager 事务管理器 分布式事务的实现由事务管理器来完成，它会提供分布式事务的操作接口供我们的业务系统调用。这些接口称为TX接口。 事务管理器还管理着所有的资源管理器，通过它们提供的XA接口来同一调度这些资源管理器，以实现分布式事务。 DTP只是一套实现分布式事务的规范，并没有定义具体如何实现分布式事务，TM可以采用2PC、3PC、Paxos等协议实现分布式事务。 RM：Resource Manager 资源管理器 能够提供数据服务的对象都可以是资源管理器，比如：数据库、消息中间件、缓存等。大部分场景下，数据库即为分布式事务中的资源管理器。 资源管理器能够提供单数据库的事务能力，它们通过XA接口，将本数据库的提交、回滚等能力提供给事务管理器调用，以帮助事务管理器实现分布式的事务管理。 XA是DTP模型定义的接口，用于向事务管理器提供该资源管理器(该数据库)的提交、回滚等能力。 DTP只是一套实现分布式事务的规范，RM具体的实现是由数据库厂商来完成的。 有没有基于DTP模型的分布式事务中间件？ DTP模型有啥优缺点？ 方案2：基于可靠消息服务的分布式事务这种实现分布式事务的方式需要通过消息中间件来实现。假设有A和B两个系统，分别可以处理任务A和任务B。此时系统A中存在一个业务流程，需要将任务A和任务B在同一个事务中处理。下面来介绍基于消息中间件来实现这种分布式事务。 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程可以得出如下几个结论： 消息中间件扮演者分布式事务协调者的角色。 系统A完成任务A后，到任务B执行完成之间，会存在一定的时间差。在这个时间差内，整个系统处于数据不一致的状态，但这短暂的不一致性是可以接受的，因为经过短暂的时间后，系统又可以保持数据一致性，满足BASE理论。 上述过程中，如果任务A处理失败，那么需要进入回滚流程，如下图所示： 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 此时系统又处于一致性状态，因为任务A和任务B都没有执行。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？ 答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件的超时询问机制能够防止上游系统因在传输过程中丢失Commit/Rollback指令而导致的系统不一致情况，而且能降低上游系统的阻塞时间，上游系统只要发出Commit/Rollback指令后便可以处理其他任务，无需等待确认应答。而Commit/Rollback指令丢失的情况通过超时询问机制来弥补，这样大大降低上游系统的阻塞时间，提升系统的并发度。 下面来说一说消息投递过程的可靠性保证。当上游系统执行完任务并向消息中间件提交了Commit指令后，便可以处理其他任务了，此时它可以认为事务已经完成，接下来消息中间件一定会保证消息被下游系统成功消费掉！那么这是怎么做到的呢？这由消息中间件的投递流程来保证。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 有的同学可能要问：消息投递失败后为什么不回滚消息，而是不断尝试重新投递？ 这就涉及到整套分布式事务系统的实现成本问题。 我们知道，当系统A将向消息中间件发送Commit指令后，它便去做别的事情了。如果此时消息投递失败，需要回滚的话，就需要让系统A事先提供回滚接口，这无疑增加了额外的开发成本，业务系统的复杂度也将提高。对于一个业务系统的设计目标是，在保证性能的前提下，最大限度地降低系统复杂度，从而能够降低系统的运维成本。 不知大家是否发现，上游系统A向消息中间件提交Commit/Rollback消息采用的是异步方式，也就是当上游系统提交完消息后便可以去做别的事情，接下来提交、回滚就完全交给消息中间件来完成，并且完全信任消息中间件，认为它一定能正确地完成事务的提交或回滚。然而，消息中间件向下游系统投递消息的过程是同步的。也就是消息中间件将消息投递给下游系统后，它会阻塞等待，等下游系统成功处理完任务返回确认应答后才取消阻塞等待。为什么这两者在设计上是不一致的呢？ 首先，上游系统和消息中间件之间采用异步通信是为了提高系统并发度。业务系统直接和用户打交道，用户体验尤为重要，因此这种异步通信方式能够极大程度地降低用户等待时间。此外，异步通信相对于同步通信而言，没有了长时间的阻塞等待，因此系统的并发性也大大增加。但异步通信可能会引起Commit/Rollback指令丢失的问题，这就由消息中间件的超时询问机制来弥补。 那么，消息中间件和下游系统之间为什么要采用同步通信呢？ 异步能提升系统性能，但随之会增加系统复杂度；而同步虽然降低系统并发度，但实现成本较低。因此，在对并发度要求不是很高的情况下，或者服务器资源较为充裕的情况下，我们可以选择同步来降低系统的复杂度。 我们知道，消息中间件是一个独立于业务系统的第三方中间件，它不和任何业务系统产生直接的耦合，它也不和用户产生直接的关联，它一般部署在独立的服务器集群上，具有良好的可扩展性，所以不必太过于担心它的性能，如果处理速度无法满足我们的要求，可以增加机器来解决。而且，即使消息中间件处理速度有一定的延迟那也是可以接受的，因为前面所介绍的BASE理论就告诉我们了，我们追求的是最终一致性，而非实时一致性，因此消息中间件产生的时延导致事务短暂的不一致是可以接受的。 方案3：最大努力通知（定期校对）最大努力通知也被称为定期校对，其实在方案二中已经包含，这里再单独介绍，主要是为了知识体系的完整性。这种方案也需要消息中间件的参与，其过程如下： 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。如果这量步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第二种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。 方案4：TCC（两阶段型、补偿型）TCC即为Try Confirm Cancel，它属于补偿型分布式事务。顾名思义，TCC实现分布式事务一共有三个步骤： Try：尝试待执行的业务 这个过程并未执行业务，只是完成所有业务的一致性检查，并预留好执行所需的全部资源 Confirm：执行业务 这个过程真正开始执行业务，由于Try阶段已经完成了一致性检查，因此本过程直接执行，而不做任何检查。并且在执行的过程中，会使用到Try阶段预留的业务资源。 Cancel：取消执行的业务 若业务执行失败，则进入Cancel阶段，它会释放所有占用的业务资源，并回滚Confirm阶段执行的操作。 下面以一个转账的例子来解释下TCC实现分布式事务的过程。 假设用户A用他的账户余额给用户B发一个100元的红包，并且余额系统和红包系统是两个独立的系统。 Try 创建一条转账流水，并将流水的状态设为交易中 将用户A的账户中扣除100元（预留业务资源） Try成功之后，便进入Confirm阶段 Try过程发生任何异常，均进入Cancel阶段 Confirm 向B用户的红包账户中增加100元 将流水的状态设为交易已完成 Confirm过程发生任何异常，均进入Cancel阶段 Confirm过程执行成功，则该事务结束 Cancel 将用户A的账户增加100元 将流水的状态设为交易失败 在传统事务机制中，业务逻辑的执行和事务的处理，是在不同的阶段由不同的部件来完成的：业务逻辑部分访问资源实现数据存储，其处理是由业务系统负责；事务处理部分通过协调资源管理器以实现事务管理，其处理由事务管理器来负责。二者没有太多交互的地方，所以，传统事务管理器的事务处理逻辑，仅需要着眼于事务完成（commit/rollback）阶段，而不必关注业务执行阶段。 TCC全局事务必须基于RM本地事务来实现全局事务TCC服务是由Try/Confirm/Cancel业务构成的，其Try/Confirm/Cancel业务在执行时，会访问资源管理器（Resource Manager，下文简称RM）来存取数据。这些存取操作，必须要参与RM本地事务，以使其更改的数据要么都commit，要么都rollback。 这一点不难理解，考虑一下如下场景： 假设图中的服务B没有基于RM本地事务（以RDBS为例，可通过设置auto-commit为true来模拟），那么一旦[B:Try]操作中途执行失败，TCC事务框架后续决定回滚全局事务时，该[B:Cancel]则需要判断[B:Try]中哪些操作已经写到DB、哪些操作还没有写到DB：假设[B:Try]业务有5个写库操作，[B:Cancel]业务则需要逐个判断这5个操作是否生效，并将生效的操作执行反向操作。 不幸的是，由于[B:Cancel]业务也有n（0&lt;=n&lt;=5）个反向的写库操作，此时一旦[B:Cancel]也中途出错，则后续的[B:Cancel]执行任务更加繁重。因为，相比第一次[B:Cancel]操作，后续的[B:Cancel]操作还需要判断先前的[B:Cancel]操作的n（0&lt;=n&lt;=5）个写库中哪几个已经执行、哪几个还没有执行，这就涉及到了幂等性问题。而对幂等性的保障，又很可能还需要涉及额外的写库操作，该写库操作又会因为没有RM本地事务的支持而存在类似问题。。。可想而知，如果不基于RM本地事务，TCC事务框架是无法有效的管理TCC全局事务的。 反之，基于RM本地事务的TCC事务，这种情况则会很容易处理：[B:Try]操作中途执行失败，TCC事务框架将其参与RM本地事务直接rollback即可。后续TCC事务框架决定回滚全局事务时，在知道“[B:Try]操作涉及的RM本地事务已经rollback”的情况下，根本无需执行[B:Cancel]操作。 换句话说，基于RM本地事务实现TCC事务框架时，一个TCC型服务的cancel业务要么执行，要么不执行，不需要考虑部分执行的情况。 TCC事务框架应该提供Confirm/Cancel服务的幂等性保障一般认为，服务的幂等性，是指针对同一个服务的多次(n&gt;1)请求和对它的单次(n=1)请求，二者具有相同的副作用。 在TCC事务模型中，Confirm/Cancel业务可能会被重复调用，其原因很多。比如，全局事务在提交/回滚时会调用各TCC服务的Confirm/Cancel业务逻辑。执行这些Confirm/Cancel业务时，可能会出现如网络中断的故障而使得全局事务不能完成。因此，故障恢复机制后续仍然会重新提交/回滚这些未完成的全局事务，这样就会再次调用参与该全局事务的各TCC服务的Confirm/Cancel业务逻辑。 既然Confirm/Cancel业务可能会被多次调用，就需要保障其幂等性。那么，应该由TCC事务框架来提供幂等性保障？还是应该由业务系统自行来保障幂等性呢？个人认为，应该是由TCC事务框架来提供幂等性保障。如果仅仅只是极个别服务存在这个问题的话，那么由业务系统来负责也是可以的；然而，这是一类公共问题，毫无疑问，所有TCC服务的Confirm/Cancel业务存在幂等性问题。TCC服务的公共问题应该由TCC事务框架来解决；而且，考虑一下由业务系统来负责幂等性需要考虑的问题，就会发现，这无疑增大了业务系统的复杂度。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(二)：微服务架构下的分布式事务基础入门【转载】","date":"2018-03-26T10:04:42.000Z","path":"2018/03/26/Distributed-transaction-basics.html","text":"众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？本文首先会为大家梳理分布式事务的基本概念和理论基础，然后介绍几种目前常用的分布式事务解决方案。废话不多说，那就开始吧～ 1. 什么是事务？事务由一组操作构成，我们希望这组操作能够全部正确执行，如果这一组操作中的任意一个步骤发生错误，那么就需要回滚之前已经完成的操作。也就是同一个事务中的所有操作，要么全都正确执行，要么全都不要执行。 2. 事务的四大特性 ACID说到事务，就不得不提一下事务著名的四大特性。 1、原子性 原子性要求，事务是一个不可分割的执行单元，事务中的所有操作要么全都执行，要么全都不执行。 2、一致性 一致性要求，事务在开始前和结束后，数据库的完整性约束没有被破坏。 3、隔离性 事务的执行是相互独立的，它们不会相互干扰，一个事务不会看到另一个正在运行过程中的事务的数据。 4、持久性 持久性要求，一个事务完成之后，事务的执行结果必须是持久化保存的。即使数据库发生崩溃，在数据库恢复后事务提交的结果仍然不会丢失。 注意：事务只能保证数据库的高可靠性，即数据库本身发生问题后，事务提交后的数据仍然能恢复；而如果不是数据库本身的故障，如硬盘损坏了，那么事务提交的数据可能就丢失了。这属于『高可用性』的范畴。因此，事务只能保证数据库的『高可靠性』，而『高可用性』需要整个系统共同配合实现。 3. 事务的隔离级别这里扩展一下，对事务的隔离性做一个详细的解释。 在事务的四大特性ACID中，要求的隔离性是一种严格意义上的隔离，也就是多个事务是串行执行的，彼此之间不会受到任何干扰。这确实能够完全保证数据的安全性，但在实际业务系统中，这种方式性能不高。因此，数据库定义了四种隔离级别，隔离级别和数据库的性能是呈反比的，隔离级别越低，数据库性能越高，而隔离级别越高，数据库性能越差。 3.1 事务并发执行会出现的问题我们先来看一下在不同的隔离级别下，数据库可能会出现的问题： 1、更新丢失 当有两个并发执行的事务，更新同一行数据，那么有可能一个事务会把另一个事务的更新覆盖掉。 当数据库没有加任何锁操作的情况下会发生。 2、脏读 一个事务读到另一个尚未提交的事务中的数据。 该数据可能会被回滚从而失效。 如果第一个事务拿着失效的数据去处理那就发生错误了。 3、不可重复读 不可重复度的含义：一个事务对同一行数据读了两次，却得到了不同的结果。它具体分为如下两种情况： 虚读：在事务1两次读取同一记录的过程中，事务2对该记录进行了修改，从而事务1第二次读到了不一样的记录。 幻读：事务1在两次查询的过程中，事务2对该表进行了插入、删除操作，从而事务1第二次查询的结果发生了变化。 不可重复读 与 脏读 的区别？脏读读到的是尚未提交的数据，而不可重复读读到的是已经提交的数据，只不过在两次读的过程中数据被另一个事务改过了。 3.2 数据库的四种隔离级别数据库一共有如下四种隔离级别： 1、Read uncommitted 读未提交 在该级别下，一个事务对一行数据修改的过程中，不允许另一个事务对该行数据进行修改，但允许另一个事务对该行数据读。 因此本级别下，不会出现更新丢失，但会出现脏读、不可重复读。 2、Read committed 读提交 在该级别下，未提交的写事务不允许其他事务访问该行，因此不会出现脏读；但是读取数据的事务允许其他事务的访问该行数据，因此会出现不可重复读的情况。 3、Repeatable read 重复读 在该级别下，读事务禁止写事务，但允许读事务，因此不会出现同一事务两次读到不同的数据的情况（不可重复读），且写事务禁止其他一切事务。 4、Serializable 序列化 该级别要求所有事务都必须串行执行，因此能避免一切因并发引起的问题，但效率很低。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 4. 什么是分布式事务？到此为止，所介绍的事务都是基于单数据库的本地事务，目前的数据库仅支持单库事务，并不支持跨库事务。而随着微服务架构的普及，一个大型业务系统往往由若干个子系统构成，这些子系统又拥有各自独立的数据库。往往一个业务流程需要由多个子系统共同完成，而且这些操作可能需要在一个事务中完成。在微服务系统中，这些业务场景是普遍存在的。此时，我们就需要在数据库之上通过某种手段，实现支持跨数据库的事务支持，这也就是大家常说的“分布式事务”。 这里举一个分布式事务的典型例子——用户下单过程。 当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 5. CAP理论CAP理论说的是：在一个分布式系统中，最多只能满足C、A、P中的两个需求。 CAP的含义： C：Consistency 一致性 同一数据的多个副本是否实时相同。 A：Availability 可用性 可用性：一定时间内 &amp; 系统返回一个明确的结果 则称为该系统可用。 P：Partition tolerance 分区容错性 将同一服务分布在多个系统中，从而保证某一个系统宕机，仍然有其他系统提供相同的服务。 CAP理论告诉我们，在分布式系统中，C、A、P三个条件中我们最多只能选择两个。那么问题来了，究竟选择哪两个条件较为合适呢？ 对于一个业务系统来说，可用性和分区容错性是必须要满足的两个条件，并且这两者是相辅相成的。业务系统之所以使用分布式系统，主要原因有两个： 提升整体性能 当业务量猛增，单个服务器已经无法满足我们的业务需求的时候，就需要使用分布式系统，使用多个节点提供相同的功能，从而整体上提升系统的性能，这就是使用分布式系统的第一个原因。 实现分区容错性 单一节点 或 多个节点处于相同的网络环境下，那么会存在一定的风险，万一该机房断电、该地区发生自然灾害，那么业务系统就全面瘫痪了。为了防止这一问题，采用分布式系统，将多个子系统分布在不同的地域、不同的机房中，从而保证系统高可用性。 这说明分区容错性是分布式系统的根本，如果分区容错性不能满足，那使用分布式系统将失去意义。 此外，可用性对业务系统也尤为重要。在大谈用户体验的今天，如果业务系统时常出现“系统异常”、响应时间过长等情况，这使得用户对系统的好感度大打折扣，在互联网行业竞争激烈的今天，相同领域的竞争者不甚枚举，系统的间歇性不可用会立马导致用户流向竞争对手。因此，我们只能通过牺牲一致性来换取系统的可用性和分区容错性。这也就是下面要介绍的BASE理论。 6. BASE理论CAP理论告诉我们一个悲惨但不得不接受的事实——我们只能在C、A、P中选择两个条件。而对于业务系统而言，我们往往选择牺牲一致性来换取系统的可用性和分区容错性。不过这里要指出的是，所谓的“牺牲一致性”并不是完全放弃数据一致性，而是牺牲强一致性换取弱一致性。下面来介绍下BASE理论。 BA：Basic Available 基本可用 “一定时间”可以适当延长 当举行大促时，响应时间可以适当延长 给部分用户返回一个降级页面 给部分用户直接返回一个降级页面，从而缓解服务器压力。但要注意，返回降级页面仍然是返回明确结果。 整个系统在某些不可抗力的情况下，仍然能够保证“可用性”，即一定时间内仍然能够返回一个明确的结果。只不过“基本可用”和“高可用”的区别是： S：Soft State：柔性状态 同一数据的不同副本的状态，可以不需要实时一致。 E：Eventual Consisstency：最终一致性 同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后仍然是一致的。 7. 酸碱平衡ACID能够保证事务的强一致性，即数据是实时一致的。这在本地事务中是没有问题的，在分布式事务中，强一致性会极大影响分布式系统的性能，因此分布式系统中遵循BASE理论即可。但分布式系统的不同业务场景对一致性的要求也不同。如交易场景下，就要求强一致性，此时就需要遵循ACID理论，而在注册成功后发送短信验证码等场景下，并不需要实时一致，因此遵循BASE理论即可。因此要根据具体业务场景，在ACID和BASE之间寻求平衡。 8. 分布式事务协议下面介绍几种实现分布式事务的协议。 8.1 两阶段提交协议 2PC分布式系统的一个难点是如何保证架构下多个节点在进行事务性操作的时候保持一致性。为实现这个目的，二阶段提交算法的成立基于以下假设： 该分布式系统中，存在一个节点作为协调者(Coordinator)，其他节点作为参与者(Cohorts)。且节点之间可以进行网络通信。 所有节点都采用预写式日志，且日志被写入后即被保持在可靠的存储设备上，即使节点损坏不会导致日志数据的消失。 所有节点不会永久性损坏，即使损坏后仍然可以恢复。 1. 第一阶段（投票阶段）： 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 2. 第二阶段（提交执行阶段）：当协调者节点从所有参与者节点获得的相应消息都为”同意”时： 协调者节点向所有参与者节点发出”正式提交(commit)”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的： 执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。（没有多少容错机制） 协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。（这个可以依赖后面要讲的Paxos协议实现HA） 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 为此，Dale Skeen和Michael Stonebraker在“A Formal Model of Crash Recovery in a Distributed System”中提出了三阶段提交协议（3PC）。 8.2 三阶段提交协议 3PC与两阶段提交不同的是，三阶段提交有两个改动点。 引入超时机制。同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。 1. CanCommit阶段3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 1、事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 2、响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No 2. PreCommit阶段协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。 1、发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。 2、事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。 3、响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 1、发送中断请求 协调者向所有参与者发送abort请求。 2、中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 3. doCommit阶段该阶段进行真正的事务提交，也可以分为以下两种情况。 3.1 执行提交 1、发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。 2、事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 3、响应反馈 事务提交完之后，向协调者发送Ack响应。 4、完成事务 协调者接收到所有参与者的ack响应之后，完成事务。 3.2 中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 1、发送中断请求 协调者向所有参与者发送abort请求 2、事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 3、反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息 4、中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(一)：走进微服务的世界【转载】","date":"2018-03-25T03:55:31.000Z","path":"2018/03/25/Into-Microservices.html","text":"1. 什么是微服务？我们首先给出微服务的定义，然后再对该定义给出详细的解释。 微服务就是一些可独立运行、可协同工作的小的服务。 从概念中我们可以提取三个关键词：可独立运行、可协同工作、小。这三个词高度概括了微服务的核心特性。下面我们就对这三个词作详细解释。 1、可独立运行 微服务是一个个可以独立开发、独立部署、独立运行的系统或者进程。 2、可协同工作 采用了微服务架构后，整个系统被拆分成多个微服务，这些服务之间往往不是完全独立的，在业务上存在一定的耦合，即一个服务可能需要使用另一个服务所提供的功能。这就是所谓的“可协同工作”。与单服务应用不同的是，多个微服务之间的调用时通过RPC通信来实现，而非单服务的本地调用，所以通信的成本相对要高一些，但带来的好处也是可观的。 3、小而美 微服务的思想是，将一个拥有复杂功能的庞大系统，按照业务功能，拆分成多个相互独立的子系统，这些子系统则被称为“微服务”。每个微服务只承担某一项职责，从而相对于单服务应用来说，微服务的体积是“小”的。小也就意味着每个服务承担的职责变少，根据单一职责原则，我们在系统设计时，要尽量使得每一项服务只承担一项职责，从而实现系统的“高内聚”。 2. 微服务的优点1. 易于扩展在单服务应用中，如果目前性能到达瓶颈，无法支撑目前的业务量，此时一般采用集群模式，即增加服务器集群的节点，并将这个单服务应用“复制”到所有的节点上，从而提升整体性能。然而这种扩展的粒度是比较粗糙的。如果只是系统中某一小部分存在性能问题，在单服务应用中，也要将整个应用进行扩展，这种方式简单粗暴，无法对症下药。而当我们使用了微服务架构后，如果某一项服务的性能到达瓶颈，那么我们只需要增加该服务的节点数即可，其他服务无需变化。这种扩展更加具有针对性，能够充分利用计算机硬件/软件资源。而且只扩展单个服务影响的范围较小，从而系统出错的概率也就越低。 2. 部署简单对于单服务应用而言，所有代码均在一个项目中，从而导致任何微小的改变都需要将整个项目打包、发布、部署，而这一系列操作的代价是高昂的。长此以往，团队为了降低发布的频率，会使得每次发布都伴随着大量的修改，修改越多也就意味着出错的概率也越大。当我们采用微服务架构以后，每个服务只承担少数职责，从而每次只需要发布发生修改的系统，其他系统依然能够正常运行，波及范围较小。此外，相对于单服务应用而言，每个微服务系统修改的代码相对较少，从而部署后出现错误的概率也相对较低。 3. 技术异构性对于单服务应用而言，一个系统的所有模块均整合在一个项目中，所以这些模块只能选择相同的技术。但有些时候，单一技术没办法满足不同的业务需求。如对于项目的算法团队而言，函数试编程语言可能更适合算法的开发，而对于业务开发团队而言，类似于Java的强类型语言具有更高的稳定性。然而在单服务应用中只能互相权衡，选择同一种语言，而当我们使用微服务结构后，这个问题就能够引刃而解。我们将一个完整的系统拆分成了多个独立的服务，从而每个服务都可以根据各自不同的特点，选择最为合适的技术体系。 当然，并不是所有的微服务系统都具备技术异构性，要实现技术异构性，必须保证所有服务都提供通用接口。我们知道，在微服务系统中，服务之间采用RPC接口通信，而实现RPC通信的方式有很多。有一些RPC通信方式与语言强耦合，如Java的RMI技术，它就要求通信的双方都必须采用Java语言开发。当然，也有一些RPC通信方式与语言无关，如基于HTTP协议的REST。这种通信方式对通信双方所采用的语言没有做任何限制，只要通信过程中传输的数据遵循REST规范即可。当然，与语言无关也就意味着通信双方没有类型检查，从而会提高出错的概率。所以，究竟选择与语言无关的RPC通信方式，还是选择与语言强耦合的RPC通信方式，需要我们根据实际的业务场景合理地分析。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"Kubernetes中的亲和性【转载】","date":"2018-03-24T02:12:08.000Z","path":"2018/03/24/Affinity-in-Kubernetes.html","text":"现实中应用的运行对于kubernetes在亲和性上提出了一些要求，可以归类到以下几个方面： Pod固定调度到某些节点之上 Pod不会调度到某些节点之上 Pod的多副本调度到相同的节点之上 Pod的多副本调度到不同的节点之上 实践下面我们将通过例子的方式来说明在kubernetes需要去设置亲和性实现上面要求． Pod调动到某些节点上Pod的定义中通过nodeSelector指定label标签，pod将会只调度到具有该标签的node之上 apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd 这个例子中pod只会调度到具有disktype=ssd的node上面． 节点亲和性/反亲和性Affinity/anti-affinity node 相对于nodeSelector机制更加的灵活和丰富 表达的语法：支持In,NotIn,Exists,DoesNotExist,Gt,Lt． 支持soft(preference)和hard(requirement),hard表示pod sheduler到某个node上，则必须满足亲和性设置．soft表示scheduler的时候，无法满足节点的时候，会选择非nodeSelector匹配的节点． nodeAffinity的基础上添加多个nodeSelectorTerms字段，调度的时候Node只需要nodeSelectorTerms中的某一个符合条件就符合nodeAffinity的规则．在nodeSelectorTerms中添加matchExpressions，需要可以调度的Node是满足matchExpressions中表示的所有规则． apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: k8s.gcr.io/pause:2.0 Pod间的亲和性和反亲和性基于已经运行在Node 上pod的labels来决定需要新创建的Pods是否可以调度到node节点上，配置的时候可以指定那个namespace中的pod需要满足pod的亲和性．可以通过topologyKey来指定topology domain, 可以指定为node／cloud provider zone／cloud provider region的范围． 表达的语法：支持In, NotIn, Exists, DoesNotExist Pod的亲和性和反亲和性可以分成 requiredDuringSchedulingIgnoredDuringExecution #硬要求 preferredDuringSchedulingIgnoredDuringExecution ＃软要求 类似上面node的亲和策略类似，requiredDuringSchedulingIgnoredDuringExecution亲和性可以用于约束不同服务的pod在同一个topology domain的Nod上．preferredDuringSchedulingIgnoredDuringExecution反亲和性可以将服务的pod分散到不同的topology domain的Node上． topologyKey可以设置成如下几种类型 kubernetes.io/hostname ＃Node failure-domain.beta.kubernetes.io/zone ＃Zone failure-domain.beta.kubernetes.io/region #Region 可以设置node上的label的值来表示node的name,zone,region等信息，pod的规则中指定topologykey的值表示指定topology范围内的node上运行的pod满足指定规则 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname containers: - name: with-pod-affinity image: k8s.gcr.io/pause:2.0 利用社区官方的例子来进一步的说明，例子中指定了pod的亲和性和反亲和性，preferredDuringSchedulingIgnoredDuringExecution指定的规则是pod将会调度到的node尽量会满足如下条件： node上具有failure-domain.beta.kubernetes.io/zone，并且具有相同failure-domain.beta.kubernetes.io/zone的值的node上运行有一个pod,它符合label为securtity=S1. preferredDuringSchedulingIgnoredDuringExecution规则表示将不会调度到node上运行有security=S2的pod．如果这里我们将topologyKey＝failure-domain.beta.kubernetes.io/zone，那么pod将不会调度到node满足的条件是：node上具有failure-domain.beta.kubernetes.io/zone相同的Value,并且这些相同zone下的node上运行有security=S2的pod. Notice:对于topologyKey字段具有如下约束 对于亲和性以及RequiredDuringScheduling的反亲和性，topologyKey需要指定 对于RequiredDuringScheduling的反亲和性，LimitPodHardAntiAffinityTopology的准入控制限制topologyKey为kubernetes.io/hostname,可以通过修改或者disable解除该约束 对于PreferredDuringScheduling的反亲和性，空的topologyKey表示kubernetes.io/hostname, failure-domain.beta.kubernetes.io/zone and failure-domain.beta.kubernetes.io/region的组合． topologyKey在遵循其他约束的基础上可以设置成其他的key. 规则中可以指定匹配pod所在namespace,如果定义了但是为空，它表示所有namespace范围内的pod. 常用的场景一些更加常用的场景见例子所示 例子一 apiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: redis-server image: redis:3.2-alpine 创建了一个Deployment,副本数为３，指定了反亲和规则如上所示，pod的label为app:store,那么pod调度的时候将不会调度到node上已经运行了label为app:store的pod了，这样就会使得Deployment的三副本分别部署在不同的host的node上． 例子二 apiVersion: apps/v1 kind: Deployment metadata: name: web-server spec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: &quot;kubernetes.io/hostname&quot; podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: web-app image: nginx:1.12-alpine 在一个例子中基础之上，要求pod的亲和性满足requiredDuringSchedulingIgnoredDuringExecution中topologyKey=”kubernetes.io/hostname”,并且node上需要运行有app=store的label. 运行完例子一，例子二，那么pod的分布如下所示 $kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE redis-cache-1450370735-6dzlj 1/1 Running 0 8m 10.192.4.2 kube-node-3 redis-cache-1450370735-j2j96 1/1 Running 0 8m 10.192.2.2 kube-node-1 redis-cache-1450370735-z73mh 1/1 Running 0 8m 10.192.3.1 kube-node-2 web-server-1287567482-5d4dz 1/1 Running 0 7m 10.192.2.3 kube-node-1 web-server-1287567482-6f7v5 1/1 Running 0 7m 10.192.4.3 kube-node-3 web-server-1287567482-s330j 1/1 Running 0 7m 10.192.3.2 kube-node-2 例子三 apiVersion: apps/v1beta1 # for versions before 1.6.0 use extensions/v1beta1 kind: Deployment metadata: name: web-server spec: replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: web-app image: hub.easystack.io/library/nginx:1.9.0 在一些应用中，pod副本之间需要共享cache,需要将pod运行在一个节点之上 web-server-77bfb4575f-bhxvg 1/1 Running 0 11s 10.233.66.79 hzc-slave2 app=web-store,pod-template-hash=3369601319 web-server-77bfb4575f-mkfd9 1/1 Running 0 11s 10.233.66.80 hzc-slave2 app=web-store,pod-template-hash=3369601319 web-server-77bfb4575f-wgjq6 1/1 Running 0 11s 10.233.66.78 hzc-slave2 app=web-store,pod-template-hash=3369601319 参考https://github.com/davidkbainbridge/demo-affinityhttps://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-featurehttps://medium.com/kokster/scheduling-in-kubernetes-part-2-pod-affinity-c2b217312ae1","tags":[{"name":"亲和性","slug":"亲和性","permalink":"http://yoursite.com/tags/亲和性/"}]},{"title":"Pipeline语法","date":"2018-03-23T09:23:48.000Z","path":"2018/03/23/Pipeline-syntax.html","text":"介绍本文基于入门介绍，仅仅是一个语法参考。至于如何在特定的例子中运用Pipeline语法，请参考Jenkins Pipeline。从插件Pipeline plugin的2.5版本开始，Pipeline支持两种格式的语法。对于它们之间的区别请参考语法对比。 正如在入门介绍里说的，流水线最主要的就是”步骤“。基本上，就是步骤来告诉Jenkins该干什么，它是申明式和脚本式流水线语法的基础。 你可以在流水线步骤参考中，找到一份可用的步骤列表。 申明Pipeline申明式流水线是最近添加到Jenkins流水线功能中的，这种语法更加简单。 所有合法的申明式流水线必须在 pipeline 代码块中，例如： pipeline { /* insert Declarative Pipeline here */ } 在申明式流水线中，基本的语句和表达式是遵循 Groovy语法 ，但是有以下几个例外： 流水线的顶层必须是一个代码块： pipeline { } 不需分号作为语句的分隔符。每个语句单独占一行 只能包括段落、步骤、或者赋值语句 属性引用语句被当作无参数的方法调用。例如：input会当作方法input() 段落在申明式流水线中，通常包括一个或者多个指令或者步骤。 代理agent代理指定了整个流水线或者特定的阶段的运行环境。它必须在pipeline块的顶层定义，而在阶段中是可选的。 参数为了支持多种情况的流水线使用场景，代理（agent）支持几种不同类型的参数。这些参数既可以在顶层的pipeline块也可以在每个阶段中使用。 any在任意可用的代理上执行流水线。例如： agent any none当在顶层的pipeline块中使用时，不会有全局的代理分配给整个流水线，每个阶段都需要包含个人的代理。例如：agent none label根据Jenkins环境中提供的标签，确定一个可用的代理来chiding流水线或者阶段。例如：agent { label ‘my-defined-label’ } nodeagent { node { label ‘labelName’ } } 和 agent { label ‘labelName’ }一样，但是 node 允许增加选项（例如 customWorkspace） docker在指定的容器里执行流水线或者阶段，容器会被动态分配到预先配置好的基于Docker的流水线节点，或者通过label参数来匹配。docker也有一个可选参数args，该参数会直接传递给docker run来执行；还有一个alwaysPull 选项，及时镜像名已经存在了依然会强制执行docker pull。 例如： agent { docker &apos;maven:3-alpine&apos; } 或者： agent { docker { image &apos;maven:3-alpine&apos; label &apos;my-defined-label&apos; args &apos;-v /tmp:/tmp&apos; } } dockerfile由码线中的Dockerfile构建出来的容器，执行流水线或者阶段。为了使用该特性，Jenkinsfile 必须是在多分支流水线或者从SCM中加载。约定Dockerfile 在码线的根目录中，可以是agent { dockerfile true } 。如果Dockerfile 在另外一个目录中，可以使用参数dir ：agent { dockerfile { dir ‘someSubDir’ } } 。如果Dockerfile 有其他的名称，你可以通过参数filename 指定文件名称。你可以通过参数additionalBuildArgs 给命令docker build … 传递额外的选项，例如agent { dockerfile { additionalBuildArgs ‘–build-arg foo=bar’ } } 。例如：一个码线有文件build/Dockerfile.build，并需要一个构建参数version： agent { // Equivalent to &quot;docker build -f Dockerfile.build --build-arg version=1.0.2 ./build/ dockerfile { filename &apos;Dockerfile.build&apos; dir &apos;build&apos; label &apos;my-defined-label&apos; additionalBuildArgs &apos;--build-arg version=1.0.2&apos; } 通用选项Common Options有一些选项可以在多种代理实现中使用。没有指定的话，就不是必须的。 label字符串。可以在流水线或者 stage上。 该选项可以在 node，docker 和 dockerfile中使用，但对于 node是必须的。 customWorkspace字符串。指定工作空间，而不使用默认的。可以是相对于节点上的根工作空间，也可以是绝对路径。例如： agent { node { label &apos;my-defined-label&apos; customWorkspace &apos;/some/other/path&apos; } } 该选项可以用在 node， docker 和 dockerfile。 reuseNode布尔值，默认为false。如果为true，则在相同的工作空间中运行，而不是每次创建新的。该选项可以在 docker 和 dockerfile中使用，而且只有在 agent 配置到单独的 stage中才能使用。 Jenkinsfile (Declarative Pipeline) pipeline { agent { docker &apos;maven:3-alpine&apos; } stages { stage(&apos;Example Build&apos;) { steps { sh &apos;mvn -B clean verify&apos; } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent none stages { stage(&apos;Example Build&apos;) { agent { docker &apos;maven:3-alpine&apos; } steps { echo &apos;Hello, Maven&apos; sh &apos;mvn --version&apos; } } stage(&apos;Example Test&apos;) { agent { docker &apos;openjdk:8-jre&apos; } steps { echo &apos;Hello, JDK&apos; sh &apos;java -version&apos; } } } } post 阶段stages 步骤steps 指令Directive 环境environment 选项options 参数parameters 触发器triggers 阶段stage 工具tools 输入input 条件when 指令when 允许流水线根据条件来决定是否要执行特定的阶段。指令when 必须至少包含一个条件。如果指令when 包含多个条件，所有的条件都必须为true才可以会执行该阶段。这和allOf 条件是类似的（请参考下面的例子）。 更复杂的结构可以使用嵌套：not， allOf，或 anyOf。可以嵌套任意深度。 内置条件：分支branch当匹配分支名称时执行，例如： when { branch ‘master’ }。这只有在多分支流水线中才可以使用。 环境environment当指定的环境变量值和给定的一样时执行，例如： when { environment name: ‘DEPLOY_TO’, value: ‘production’ } 表达式expression当Groovy表达式为true时，例如： when { expression { return params.DEBUG_BUILD } } not当嵌套条件值为false时执行。必须包含一个条件。例如： when { not { branch ‘master’ } } allOf当嵌套条件为true时执行。必须至少包含一个。例如： when { allOf { branch ‘master’; environment name: ‘DEPLOY_TO’, value: ‘production’ } } anyOf当任意一个表达式为true时。必须至少包含一个。例如： when { anyOf { branch ‘master’; branch ‘staging’ } } 在进入阶段的代理节点之前计算when表达式 默认情况下，when 条件是在进入阶段的代理之后计算。然而，通过增加选项beforeAgent 可以改变。如果把选项beforeAgent 设置为true，就会首先计算when 条件，只有在值为true时才会进入。 示例1： Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 示例2： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 示例3： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { allOf { branch &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例4： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; anyOf { environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例5： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { expression { BRANCH_NAME ==~ /(production|staging)/ } anyOf { environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例6： pipeline { agent none stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { agent { label &quot;some-label&quot; } when { beforeAgent true branch &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 并发parallel阶段是可以并行执行的。注意，在阶段内必须要只能有一个steps 或 parallel。任何包含parallel 的阶段不能包括agent 或 tools，也不包括steps。 另外，当有一个任务失败后，你可以强制整个并行失败。只要设置参数failFast 为true就可以。 示例： pipeline { agent any stages { stage(&apos;Non-Parallel Stage&apos;) { steps { echo &apos;This stage will be executed first.&apos; } } stage(&apos;Parallel Stage&apos;) { when { branch &apos;master&apos; } failFast true parallel { stage(&apos;Branch A&apos;) { agent { label &quot;for-branch-a&quot; } steps { echo &quot;On Branch A&quot; } } stage(&apos;Branch B&apos;) { agent { label &quot;for-branch-b&quot; } steps { echo &quot;On Branch B&quot; } } } } } } 步骤脚本script 可以在申明时的流水线中执行脚本时步骤。大多数情况下 script 是用不到的。 示例： pipeline { agent any stages { stage(&apos;Example&apos;) { steps { echo &apos;Hello World&apos; script { def browsers = [&apos;chrome&apos;, &apos;firefox&apos;] for (int i = 0; i &amp;lt; browsers.size(); ++i) { echo &quot;Testing the ${browsers[i]} browser&quot; } } } } } } 脚本化流水线脚本化流水线可以使用普通的Groovy语法，因此，它可以实现很强大的功能。 在Jenkins流水线刚被开发出来时，采用Groovy作为基础。Jenkins已经很长时间内采用嵌入式的Groovy引擎提供了高级脚本功能给管理员和普通用户。也就是说，基于Groovy脚本的流水线指的就是脚本化流水线。 流程控制步骤语法对比参考https://jenkins.io/doc/book/pipeline/syntax/","tags":[{"name":"Pipeline","slug":"Pipeline","permalink":"http://yoursite.com/tags/Pipeline/"}]},{"title":"Jenkins Pipeline","date":"2018-03-22T05:02:36.000Z","path":"2018/03/22/Jenkins-Pipeline.html","text":"介绍本文介绍如何在Jenkins中使用pipeline插件。 Jenkins安装启动后，还需要安装一些插件才可以使用pipeline（流水线）的特性。你可以在系统管理–插件管理–可选插件中搜索Pipeline进行按钮；要提醒一下的是，Jenkins会自己查找依赖的插件，所以你可能看到安装的插件不只一个。Pipeline插件的wiki地址是https://wiki.jenkins.io/display/JENKINS/Pipeline+Plugin。另外，你可以通过这里https://plugins.jenkins.io/workflow-aggregator，查看该插件的依赖关系，并找到Pipeline插件在Github上的托管地址。 什么是PipelineJenkins Pipeline是一套插件，支持实现和持续集成作为流水线应用到Jenkins。Pipeline提供了一套可扩展的工具。 Pipeline大致可以分为：节点、阶段、步骤。步骤是具体的功能表达式，例如：执行shell命令等。阶段，你可以理解为步骤的集合。而节点则是包含阶段，它规定了这些阶段（步骤）都会在哪些slave上运行。 节点，可以是一个普通的slave，也可以运行在Docker容器中。 为什么要用Pipeline根本上来说，Jenkins是一个支持很多自动化模式的引擎。Pipeline增加了一套强大的工具到Jenkins中，支持用户从简单持续集成到全面的持续集成。通过模块化一些列相关的任务，用户可以利用很多Pipeline的特性。 代码：Pipelines通过代码来实现，并通常可以由版本控制系统（svn、git等）来管理。 可暂停：Pipelines可以暂停（停止），并且可以在运行之前接收人工输入或者等待同意。 Pipeline表达式Step是一个单一任务，告诉Jenkins该做什么。例如，在step中执行shell命令make。当一个插件扩展了Pipeline DSL，就意味着可以使用新的step。 Node大多数工作是在一个或者多个节点（node）中完成的。 语法Jenkins的流水线（pipeline）采用groovy语法来编写。逻辑判断、循环、异常等功能都是具备的，另外，熟悉groovy的人就明白这和Java的写法有一定的相似。 下面我介绍一些流水线的步骤（或者函数），首先介绍的是在插件workflow-basic-steps-plugin中的。我们从插件的名称上也能看到，这些流水线步骤大多是基础、简单的。首先，给出我研究时的版本信息： &lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt;` &lt;artifactId&gt;workflow-basic-steps&lt;/artifactId&gt; &lt;version&gt;2.7-SNAPSHOT&lt;/version&gt; 以便各位依据本文可以进一步学习Jenkins流水线插件的源码。 环境变量node { echo env.JENKINS_HOME sh &apos;echo $JENKINS_HOME&apos; echo env.JOB_NAME echo env.NODE_NAME echo env.NODE_LABELS echo env.WORKSPACE echo env.JENKINS_URL echo env.BUILD_URL env.SUREN_VER = &apos;12&apos; echo env.SUREN_VER } 上面的示例中，给出了如何使用内置的环境变量和自定义环境变量的做法 node() { env.JDK_HOME = &quot;${tool &apos;8u131&apos;}&quot; env.PATH=&quot;${env.JDK_HOME}/bin:${env.PATH}&quot; echo env.JDK_HOME echo env.PATH sh &apos;java -version&apos; } node(&apos;bimpm_deploytodev&apos;) { def pass_bin = &apos;/opt/pass/bin&apos; env.PASS_BIN = pass_bin stage(&apos;Clean&apos;) { sh &apos;rm -rfv $PASS_BIN&apos; } } 工具node() { tool name: &apos;JDK8_Linux&apos;, type: &apos;jdk&apos; tool name: &apos;maven339_linux_dir&apos;, type: &apos;maven&apos; echo &apos;hello&apos; } 上面的pipeline指定需要工具jdk和maven的名称（在Global Tool Configuration中配置）。 对应的实现类为ToolStep，该类被final关键字所修饰，因此是不能做扩展的了。 属性node { echo &apos;hello&apos; } properties([ buildDiscarder( logRotator( artifactDaysToKeepStr: &apos;&apos;, artifactNumToKeepStr: &apos;&apos;, daysToKeepStr: &apos;5&apos;, numToKeepStr: &apos;10&apos; ) ), pipelineTriggers([ cron(&apos;H 3,12,17 * * *&apos;) ]) ]) 拷贝成品node { stage(&apos;Copy&apos;) { step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true, projectName: &apos;BIM_PMJF/BIM-PMJF-BUILD/BIM_PMJF_DISCOVERY&apos;, selector: [$class: &apos;StatusBuildSelector&apos;, stable: false], target: &apos;/opt/pass/bin&apos;]) } } 我们通常会在一个Job里实现工程构建，在另外的Job里做程序的部署，这时候就可以用到Jenkins的成品特性。它可以实现在多个slave之间拷贝成品。实现类为ArtifactArchiverStep。 循环node(&apos;suren&apos;) { def dev_path = &apos;/opt/suren/bin&apos; def services = [ [ &apos;name&apos;: &apos;admin&apos;, &apos;project&apos;: &apos;admin&apos;, &apos;port&apos;: &apos;7002&apos;, &apos;jarName&apos;: &apos;admin&apos; ] ]; stage(&apos;Copy Artifact&apos;) { for(service in services){ step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true, projectName: service.project, selector: [$class: &apos;StatusBuildSelector&apos;, stable: false], target: dev_path + &apos;/&apos; + service.name ]) } } stage(&apos;Stop Service&apos;) { for(service in services){ sh &apos;fuser -n tcp -k &apos; + service.port + &apos; &gt; redirection &amp;&apos; } } stage(&apos;Start Service&apos;) { for(service in services){ sh &apos;cd &apos; + pass_bin + &apos;/&apos; + service.name + &apos; &amp;&amp; nohup nice java -server -Xms128m -Xmx384m \\ -jar &apos; + service.jarName + &apos;.jar \\ --server.port=&apos; + service.port + &apos; $&gt; initServer.log 2&gt;&amp;1 &amp;&apos; } } } 上面的例子，展示了如何在jenkins pipeline中调用循环语句，实现批量操作。 参数化构建properties([[$class: &apos;JobRestrictionProperty&apos;], parameters([run(description: &apos;&apos;, filter: &apos;ALL&apos;, name: &apos;Name&apos;, projectName: &apos;Project&apos;)]), pipelineTriggers([])] ) 为了能让我们的流水线定义更加具有通用性，除了可以在流水线中使用系统预定义的变量外，可以使用由用户动态输入的变量值。当流水线Job加入参数化后，在执行任务时候就必须有用户输入一系列值才可以执行。 并行node { stage(&apos;Start Service&apos;) { parallel &apos;test&apos;: { echo &apos;test&apos; }, &apos;deply&apos;: { echo &apos;deply&apos; } } parallel &apos;one&apos; : { stage(&apos;one&apos;) { echo &apos;one&apos; } }, &apos;two&apos; : { stage(&apos;two&apos;) { echo &apos;two&apos; } } } parallel &apos;one&apos;: { node{ stage(&apos;one&apos;) { echo &apos;one&apos; } } }, &apos;two&apos;: { node { stage(&apos;two&apos;) { echo &apos;two&apos; } } } Jenkins的流水线同时支持节点（node）、阶段（stage）和步骤（step）之间的并行执行。如果多个节点并发执行的话，并发数量会少于当前可用的执行器（exector）数量。 超时node { stage(&apos;stage2&apos;) { timeout(time: 600, unit: &apos;SECONDS&apos;) { sleep 20 echo &apos;2&apos; } } } 遇到可能执行时间会比较长的情况，可以通过超时来约定最长的执行时间。 对应的实现类为TimeoutStep。 下面介绍的函数在插件workflow-durable-task-step-plugin中，版本信息如下： &lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt; &lt;artifactId&gt;workflow-durable-task-step&lt;/artifactId&gt; &lt;version&gt;2.18-SNAPSHOT&lt;/version&gt; 工作空间当你希望在一个流水线中，对多个工程（例如git工程）做构建以及部署等操作，如果不切换工作空间的话就会发生代码错乱的问题。你可以参考下面的示例代码来解决这个问题： node{ stage(&apos;suren&apos;){ ws(&apos;suren-a-work&apos;) { pwd } ws(&apos;suren-b-work&apos;) { pwd } } } 实现类为WorkspaceStep，使用final修饰，无法扩展。 执行节点Jenkins里可能会配置很多节点（node），而不一定所有的节点都满足你的构建环境要求，这时候就需要来指定节点了： node(&apos;local&apos;) { echo &apos;hello&apos; } properties([ buildDiscarder( logRotator( artifactDaysToKeepStr: &apos;&apos;, artifactNumToKeepStr: &apos;&apos;, daysToKeepStr: &apos;5&apos;, numToKeepStr: &apos;10&apos; ) ), pipelineTriggers([ cron(&apos;H 3,12,17 * * *&apos;) ]) ]) 上面的pipeline指定了运行节点的label为local。 实现类为ExecutorStep，使用final修饰，无法扩展。 异常捕获node{ stage(&apos;suren&apos;){ try{ trigger }catch(error){ echo error.getMessage() } } } 这里调用了一个不存在的流水线函数，然后使用catch来捕获并打印错误信息。 stashnode { stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true) } node(&apos;jenkins-slave&apos;) { unstash(name: &apos;test&apos;) } pipeline的文件存储（stash）这个功能，可以在流水线需要运行在多个节点（node）的情况下使用。stash和unstash会把存储的文件从一个节点转移到另一个节点上。上面给出的例子中，把所有的xml文件从master转移到了当前执行任务的slaver节点上。 node(&apos;jenkins-slave&apos;){ checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]]) } node { def path = JENKINS_HOME + &apos;/jobs/&apos; + JOB_NAME + &apos;/builds/&apos; + BUILD_ID echo path dir(path){ stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true) } } node(&apos;jenkins-slave&apos;) { unstash(name: &apos;test&apos;) } 敏感信息我们可以利用Jenkins的Credentials机制，在Pipeline中传递密码等敏感信息，例如： pipeline { agent any stages{ stage(&apos;test&apos;) { steps{ withCredentials([usernamePassword(credentialsId: &apos;aaa&apos;, passwordVariable: &apos;passwd&apos;, usernameVariable: &apos;user&apos;)]) { sh &apos;&apos;&apos;echo $user $passwd&apos;&apos;&apos; } } } } } 文件读取很多情况下，我们需要读取文件内容。 获取pom.xml版本号，获取groupId（需要的插件Pipeline Utility Steps）： node { stage(&apos;test&apos;){ checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]]) pom = readMavenPom file: &apos;pom.xml&apos; echo pom.version echo pom.groupId echo pom.artifactId } } 使用Docker在docker容器中执行任务： pipeline { agent { docker { image &apos;eclipse/mysql&apos; args &apos;-e MYSQL_ROOT_PASSWORD=root&apos; } } stages { stage(&apos;test&apos;) { steps { sh &apos;mysql&apos; } } } } withDockerContainer withDockerServer dockerFingerprintRun withDockerRegistry dockerFingerprintFrom 其他插件另外有一些比较好的Jenkins流水线插件，给出推荐： 支持从SCM加载库文件 https://github.com/suren-jenkins/workflow-remote-loader-plugin 远程调试安装环境： sudo apt-get install -y npm jenkins-pipeline --file test.groovy --url http://localhost:8080/jenkins/job/MyJob --credentials admin:123456 参考https://github.com/spring-cloud/spring-cloud-pipelines","tags":[{"name":"Pipeline","slug":"Pipeline","permalink":"http://yoursite.com/tags/Pipeline/"}]},{"title":"初探Jenkins X","date":"2018-03-21T13:42:25.000Z","path":"2018/03/21/Preliminary-Jenkins-X.html","text":"Jenkins 于 3月21日 发布了名为Jenkins X的项目，这一项目对开发人员和云端的 CI/CD 环境之间的交互过程进行了审视和反思，结合自动化、工具链以及 DevOps 最佳实践。为开发团队提供了新的生产效率增长点。 Jenkins X是什么？“X”注定是一个不平凡的名字，Jenkins X 对于整个Jenkins生态而言也是不平凡的存在。 简而言之，Jenkins X 是一个高度集成化的CI/CD平台，基于Jenkins和Kubernetes实现，旨在解决微服务体系架构下的云原生应用的持续交付的问题，简化整个云原生应用的开发、运行和部署过程。 你猜的没错，Jenkins X 只能在Kubernetes集群上运行，这有并不意外。Kubernetes已然成为了容器编排的一枝独秀，各大厂商纷纷转向Kubernetes，发布了自己的公有云、操作系统或PaaS平台。 另外，微服务和云原生应用解决方案也日臻成熟，以Spring Boot为代表的一系列体系框架也开始走到舞台中央。 与此同时，随着应用架构的细分和服务间的解耦，服务具备了独立发布的能力，这也使得微服务架构下的持续交付成为业界所关注的热门领域，我们需要更加灵活的CI/CD自动化解决方案，以应对越发快速的交付需求。 注：Jenkins的企业版CloudBees，已经加入CNCF（云原生计算）基金会 看到这里，你是不是觉得Jenkins X 就是个基于Kubernetes的持续交付平台呢？ 那你就大错特错了，因为Jenkins X想要实现的远非如此而已！ 试想如下场景： 越来越多的工具和实践，工程师们需要会写Kubernetes YAML，Dockerfile，Jenkinsfile，对微服务、云原生、Kubernetes和Jenkins非常熟悉。 臣妾做不到呀！ 而在Jenkins X的世界中，这一切都是通过命令完成。 可以说Jenkins X重新思考了未来云原生应用下研发工程师和CI/CD的交付方式，通过整合工具，自动化和DevOps最佳实践，改善了研发过程中的复杂环节，让研发可以专注于价值创造，其他的事情通通交给Jenkins X来帮你解决。 神奇吗？ 的确，在第一次看到项目演示的时候，我也惊叹世界的变化如此之快，在Jenkins X的设计中，整合了Helm，Draft，GitOps，以及Nexus，chartmuseum，monocular等诸多新系统和工具，从而实现自动构建编译环境，生成容器镜像，流水线，自动化部署，并通过简单的Review实现不同环境间的自动发布。 这一切都被完美的封装在简单的jx命令之后。同时你也无需担心对内部实现细节的失控，因为一切都被妥善的版本控制，可以自定义和修改，可以说Jenkins X为你实现了自动化的CI/CD和DevOps最佳实践，持续交付不再是难事，进而提升生产力，实现促进企业的业务成功！ Jenkins X 部分新特性1. 自动化一切：自动化CI/CD流水线 选择项目类型自动生成Jenkinsfile定义流水线 自动生成Dockerfile并打包容器镜像 自动创建Helm Chart并运行在Kubernetes集群 自动关联代码库和流水线，作为代码变更自动触发（基于Webhook实现） 自动版本号自动归档 2. Review代码一键部署应用：基于GitOps的环境部署 所有的环境，应用列表，版本，配置信息统一放在代码库中进行版本控制 通过Pull Request实现研发和运维的协同，完成应用部署升级（Promotion） 可自动部署和手动部署，在必要的时候增加手工Review 当然这些都封装在jx命令中实现 3. 自动生成预览环境和信息同步反馈 预览环境用于代码Review环节中临时创建 同Pull Request工作流程集成并实现信息同步和有效通知 验证完毕后自动清理 提交和应用状态自动同步到Github注释 自动生成release notes信息供验证 Jenkins X 核心组件 JenkinsJenkins X不是一个全新的Jenkins。 他依然使用Jenkins作为持续交付的核心引擎，实际上Jenkins X作为Jenkins的一个子项目存在，专注于云原生应用的CI/CD实现，同时也帮助Jenkins自身完成云原生应用的转型，毕竟现在越来越多的人在诟病单体应用的设计和文件存储系统。 在之前同Jenkins创始人和核心骨干的交流中，我们也了解到Jenkins已经开始着手改变。 HELMHelm是用于管理Kubernetes资源对象的工具，类似APT，YUM和HOMEBREW，他通过将Kubernetes的资源对象打包成Chart的形式，完成复杂应用的部署和版本控制，是目前业界流行的解决方案 DRAFTDraft是自动化应用构建和运行在Kubernetes上面的工具，具有语言识别能力，能够自动生成构建脚本，依赖，环境并打包成docker镜像并部署在Kubernetes集群上，加快代码开发节奏，而无需关心基础设施层面的技术实现 GitOpsGitOps是weaveworks推出的天才的应用部署解决方案，他将Git作为整个应用部署的单一可信数据源（SSOT），通过类似代码开发的Pull Request流程完成应用部署的Review和自动化实现，并且将部署配置信息纳入版本控制。 Jenkins X 安装试用先决条件工具 helm kubectl git Kubernetes 集群 互联网连接 Tiller 公网 IP github 账号 安装 jxhttp://jenkins-x.io/getting-started/install/ 提供了几种系统下的安装说明： OS X：brew tap jenkins-x/jx &amp;&amp; brew tap jenkins-x/jx Linux：curl -L https://github.com/jenkins-x/jx/releases/download/v1.1.10/jx-darwin-amd64.tar.gz | tar xzv &amp;&amp; mv jx /usr/local/bin jx installjx create cluster 支持多种公有云的创建。 配置好集群和对应的 kubeconfig 访问之后，就可以使用jx install进行安装了。 过程中几个需要注意的点： 如果 Tiller 的 SA 权限不足，会导致安装失败，可设置相应的 ClusterRole 进行解决。 安装过程会修改 kubeconfig 文件，因此建议做好备份。 为完整体验功能，建议听从安装器建议，安装 Ingress Controller。 Jenkins X 的环境管理以及代码拉取等功能需要和 Github 进行交互，因此会提问 GitHub 的 Token。 安装过程相对较长，可以使用watch kubectl get pods -n jx查看进程状况。 最后步骤会显示管理密码，注意复制保存。 安装完成JX 会为用户建立三个环境分别是，Dev、Staging 以及 Production。 运行命令jx console，会打开浏览器进入 Jenkins 登录页面。 登录之后我们会看到正在进行构建，如果是一个排队状态，可能是因为正在创建 Worker Pod，可以使用kubectl查询具体情况。 构建完成，会看到这一示例中包含了拉取、构建、Helm、环境等几个步骤，可以作为工作的基础环节来进行使用。 应用接下来就可以做几个善后工作 jx 支持插件，可以通过jx get addons查看支持的插件列表，进行安装。 根据实际工作需要，对缺省环境进行调整，安装所需软件。 对 Jenkins X 中的软件、集群进行安全加固。 使用import或者create spring/create quickstart，进行项目工作。 最后要注意的一点是，Jenkins X 目前的升级频率非常高。不建议生产使用。","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://yoursite.com/tags/Jenkins/"}]},{"title":"浅谈服务治理、微服务与Service Mesh（二） Spring Cloud从入门到精通到放弃【转载】","date":"2018-03-19T14:36:59.000Z","path":"2018/03/19/spring-cloud-start-to-give-up.html","text":"作为本系列文章的第二篇(第一篇链接请戳：浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生)，本文主要为大家介绍下微服务概念中非常火热的Spring Cloud开发框架。由于网上关于Spring Cloud的文章多如牛毛，为了让大家阅读后能有不一样的收获，因此本文将用一个相对轻松的叙述方式来为大家讲解一下Spring Cloud框架和微服务。虽然不可能通过一篇文章让大家对Spring Cloud做到从“入门到精通到放弃”，但是希望大家通过阅读本文能对Spring Cloud和微服务有一个更加清晰的认识和了解，为后面学习Service Mesh做好一个铺垫。 Spring Cloud 之“出身名门望族”作为当下最火热的微服务框架，Spring Cloud的名字可以说是无人不知、无人不晓，凭借之前Spring Framework的良好群众基础和Cloud这个具有时代感的名字，Spring Cloud一出现便被大家认知。 提到Spring Cloud，便会让人想起刚刚发布了2.0版本的Spring Boot。Spring Boot和Spring Cloud都是出自Pivotal公司，Spring Boot和Spring Cloud虽然火热，但是了解Pivotal公司的人在国内却是不多。实际上Pivotal公司在云计算、大数据、虚拟化等领域都有所建树，这里先给大家简单八卦下Pivotal的情况。 Pivotal公司是由EMC和VMware联合成立的一家公司，GE（通用电气）也对Pivotal进行了股权收购，同时GE也是Pivotal的一个重要大客户。除了Spring Framework、Spring Boot和Spring Cloud之外，我们日常开发中经常使用的Reids、RabbitMQ、Greenplum、Gemfire、Cloud Foundry等，目前都是归属于Pivotal公司的产品。其中Gemfire也是被中国铁路总公司12306使用的分布式内存数据库，也就是说你过年回家买不到火车票，这个锅Pivotal的Gemfire也会跟着一起背（开个小玩笑，哈哈）。 Spring Cloud 之“入门”Spring Cloud作为一个微服务的开发框架，其包括了很多的组件，包括：Spring Cloud Netflix（Eureka、Hystrix、Zuul、Archaius）、Spring Cloud Config、Spring Cloud Bus、Spring Cloud Cluster、Spring Cloud Consul、Spring Cloud Security、Spring Cloud Sleuth、Spring Cloud Data Flow、Spring Cloud Stream、Spring Cloud Task、Spring Cloud ZooKeeper、Spring Cloud Connectors、Spring Cloud Starters、Spring Cloud CLI等。 在上述组件中，Spring Cloud Netflix是一套微服务的核心框架，由互联网流媒体播放商Netflix开源后并入Spring Cloud大家庭，它提供了的微服务最基础的功能：服务发现（Service Discovery）、动态路由（Dynamic Routing）、负载均衡（Load Balancing）和边缘服务器（Edge Server）等。 Spring Boot是Spring的一套快速配置脚手架，可以基于Spring Boot快速开发单个微服务。Spring Boot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、生产级别的Spring应用。由于Spring Cloud是基于Spring Boot进行的开发，因此使用Spring Cloud就必须使用到Spring Boot。 下图是一个常见的关于Spring Cloud的架构图。下面此图为例，对Spring Cloud最常用的几个组件做一个简单的介绍： Eureka：服务注册中心，一个基于REST的服务，用于定位服务，以实现微服务架构中服务发现和故障转移。 Hystrix：熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。 Turbine：Turbine是聚合服务器发送事件流数据的一个工具，用来监控集群下Hystrix的Metrics情况。 Zuul：API网关，Zuul是在微服务中提供动态路由、监控、弹性、安全等边缘服务的框架。 Ribbon：提供微服务中的负载均衡功能，有多种负载均衡策略可供选择，可配合服务发现和断路器使用。 Feign：Feign是一种声明式、模板化的HTTP客户端。 Spring Cloud Config：配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。 Spring Cloud Security：基于Spring Security的安全工具包，为微服务的应用程序添加安全控制。 Spring Cloud Sleuth：日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。 除了上面介绍的基础组件外，常见的Spring Cloud组件还有非常多种，涉及到了微服务以及应用开发的方方面面： Spring Cloud Starters：Spring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。 Archaius：配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。 Consul：封装了Consul操作，Consul是一个服务发现与配置工具，与Docker容器可以无缝集成。 Spring Cloud Stream：数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 Spring Cloud CLI：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 Spring Cloud Task：提供云端计划任务管理、任务调度。 Spring Cloud Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与 Spring Cloud Config 联合实现热部署。 Spring Cloud Data Flow：大数据操作工具，作为Spring XD的替代产品，它是一个混合计算模型，结合了流数据与批量数据的处理方式。 Spring Cloud ZooKeeper：操作ZooKeeper的工具包，用于使用ZooKeeper方式的服务发现和配置管理。 Spring Cloud Connectors：便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。 Spring Cloud之“精通”Spring Cloud虽然集成了众多组件，可以构建一个完整的微服务应用，但是其中的各个组件却并非完美无缺，很多组件在实际应用中都存在诸多不足和缺陷。因此，需要我们对其中的一些组件进行替换和修改，方能构建一个强大、灵活、健壮的微服务架构应用。 配置中心Spring Cloud Config可以说是Spring Cloud家族中实现最Low的一个组件，直接采用了本地存储/SVN/Git的方式进行存储。同时，Spring Cloud Config也缺乏一个完整的可视化管理查询后台，当存在比较复杂的权限管理和版本管理需求时，Spring Cloud Config会显得非常力不从心。如果需要在配置修改后，能自动进行配置信息推送的话，使用Spring Cloud Config也无法满足要求，需要自行编写代码进行实现。 目前开源社区中，已经有了很多的开源配置中心实现方案，同时很多公司也自研了自己的配置中心方案。包括淘宝的统一配置中心Diamond（已经多年未更新）、百度的分布式配置管理平台Disconf、携程的开源分布式配置中心Apollo、360的分布式配置管理工具QConf等等。目前，笔者公司采用的是自己公司自研的配置中心，没有采用开源实现的主要原因是因为需要同时适配Spring Cloud和Dubbo等多种场景的应用。 注册中心作为Spring Cloud的服务注册中心，从分布式CAP理论来看，Eureka采用是AP型设计，强调的是注册中心的高可用性。和Dubbo常用的服务注册中心ZooKeeper相比，ZooKeeper则是采用的CP型设计，强调的是注册中心数据的一致性。 Eureka的设计确实简单易用，但是默认没有实现对注册中心数据的持久化。同时，在极端场景下，也会出现多个Eureka注册中心节点数据不一致，甚至服务注册数据丢失的情况。当然，从分布式CAP理论来看，理论上是没办法做到同时兼顾CAP三点的。目前也有一些互联网公司对Eureka进行了改造，支持了数据的持久化，但是尚不能完整的支持CAP的全部要求。 API网关API网关可以说是微服务需求最多，也是最有难点的一个组件。Spring Cloud中集成的Zuul应该说更多的是实现了服务的路由功能，对于负载均衡等其他功能，需要结合Ribbon等组件来实现。对于很多个性化的需求，需要开发者自己来进行编码实现。 和大部分基于Java的Web应用类似，Zuul也采用了Servlet架构，因此Zuul处理每个请求的方式是针对每个请求是用一个线程来处理。同时，由于Zuul是基于JVM的实现，因此性能也会在高并发访问场景下成为瓶颈。虽然网上一些文章评测Zuul和Nginx性能接近，但是在性能要求较高的场景下，JVM的内存管理和垃圾回收问题，仍然是一个很大的问题。所以在实际的应用场景中，通常会采用在多个Zuul几点前面再添加一层Nginx或者OpenResty来进行代理。 为了解决Zuul的性能问题，Netflix将自己的网关服务Zuul进行了升级，新的Zuul 2将HTTP请求的处理方式从同步变成了异步，并且新增诸如HTTP/2、websocket等功能。但是遗憾的是，开源版本的Zuul 2一直处于难产状态中，始终没有和大家正式见面。 熔断器微服务中对于服务的限流、降级、熔断的需求是多种多样的，需要在API网关和各个具体服务接口中分别进行控制，才能满足复杂场景下微服务架构的应用需求。 单独使用Spring Cloud中的Hystrix无法完整的满足上述的复杂需求，需要结合API网关，并通过Kubernetes对资源、进程和命名空间来提供隔离，并通过部分自定义编码方能实现对全部服务的限流、降级、熔断等需求。 监控系统无论是Spring Cloud中集成的Spring Cloud Sleuth，还是集成经典的ELK，都只是对日志级别的追踪和监控。在大中型微服务应用架构中，尤其是基于JVM的项目，还需要添加APM的监控机制，才能保证及时发现各种潜在的性能问题。 APM整体上主要完成3点功能：1.日志追踪、2.监控报警、3.性能统计。目前，国内外商业版本的APM方案已经有很多，开源版本的APM方案也开始丰富起来。国内开源的APM方案主要有：大众点评的CAT和Apache孵化中的SkyWalking。这里给大家重点推荐下SkyWalking，SkyWalking是针对分布式系统的应用性能监控系统，特别针对微服务、Cloud Native和容器化（Docker、Kubernetes、Mesos）架构，项目的关注度和发展速度都很快，中文文档资料也比较齐全。 Spring Cloud之“放弃”Spring Cloud可以说是一个完美的微服务入门框架，如果你是在一个中小型项目中应用Spring Cloud，那么你不需要太多的改造和适配，就可以实现微服务的基本功能。但是如果是在大型项目中实践微服务，可能会发现需要处理的问题还是比较多，尤其是项目中老代码比较多，没办法全部直接升级到Spring Boot框架下开发的话，你会非常希望能有一个侵入性更低的方案来实施微服务架构。在这种场景下，Service Mesh将会成为你的最佳选择，经过一段时间的发展，目前Service Mesh这个概念已经开始逐步被大家了解和认知。同时，一些Service Mesh的实现方案也逐步成熟和落地，例如Istio、Linkerd、Envoy等。在本系列文章的下一篇中，将为大家对Service Mesh概念做一个系统的介绍。但是在了解Service Mesh概念之前，还是建议大家先对微服务和Spring Cloud这些概念和框架有一个深入的了解，这样才能体会到应用Service Mesh的价值和意义。 Spring Cloud与Dubbo网上关于Spring Cloud和Dubbo对比的文章很多，大多数对比结果都是Spring Cloud压倒性优势战胜Dubbo，下表是对Dubbo和Spring Cloud做的一个基础功能的对比： 实际上，Dubbo的关注点在于服务治理，并不能算是一个真正的微服务框架。包括目前在开发中的Dubbo 3.0，也不能完整覆盖微服务的各项功能需求。而Spring Cloud一方面是针对微服务而设计，另外一方面Spring Cloud是通过集成各种组件的方式来实现微服务，因此理论上可以集成目前业内的绝大多数的微服务相关组件，从而实现微服务的全部功能。 而对Dubbo而言，如果一定要应用到微服务的使用场景中的话，上表中欠缺的大多数功能都可以通过集成第三方应用和组件的方式来实现，跟Spring Cloud相比主要的缺陷在于集成过程中的便利性和兼容性等问题。 Spring Cloud与Docker虽然网上也有很多文章写到如何使用Docker来实现微服务，但是事实上单独使用Docker是没办法完整的实现微服务的所有功能的。在实际上微服务架构中，Spring Cloud和Docker更多的是一种协作的关系，而不是一种竞争的关系。通过Docker容器化技术，可以更好的解决引入Spring Cloud微服务后带来的部署和运维的复杂性。 Spring Cloud生态圈中的Pivotal Cloud Foundry（PCF）作为PaaS实现，也提供一些类似于Docker的功能支持，但是无论上功能上还是易用性上和Docker还是存在比较大的差异。Pivotal Cloud Foundry和Docker之间的关系更多的是一种兼容关系，而不是竞争关系，Pivotal Cloud Foundry的主要竞争对手是Red Hat的OpenShift。目前，Pivotal Cloud Foundry支持的IaaS包括：AWS、AZURE、GCP、vSphere、OpenStack等。 Spring Cloud与Kubernetes网上也有一些“Spring Cloud与Kubernetes哪个更好”，“当已经有了Kubernetes之后，还需要使用Spring Cloud么”之类的文章。首先说笔者并不认为Spring Cloud与Kubernetes是竞争关系，但是也不否认二者确实在诸多功能上存在一些重合。下图是对Spring Cloud与Kubernetes在微服务架构中的一些基础功能上的对比： 通过对比可以看出，Spring Cloud和Kubernetes确实存在一些功能上的重合，但是二者的定位其实差别很大。Spring Cloud是一个基于Java语言的微服务开发框架，而Kubernetes是一个针对容器应用的自动化部署、伸缩和管理的开源系统，它兼容多种语言且提供了创建、运行、伸缩以及管理分布式系统的原语。Spring Cloud更多的是面向有Spring开发经验的Java语言开发者，而Kubernetes不是一个针对开发者的平台，它的目的是供有DevOps思想的IT人员使用。 为了区分Spring Cloud和Kubernetes两个项目的范围，下面这张图列出了几乎是端到端的微服务架构需求，从最底层的硬件，到最上层的DevOps和自服务经验，并且列出了如何关联到Spring Cloud和Kubernetes平台。 总结通过Spring Cloud、Docker和Kubernetes的组合，可以构建更加完整和强大的微服务架构程序。通过三者的整合，使用Spring Boot提供应用的打包，Docker和Kubernetes提供应用的部署和调度。Spring Cloud通过Hystrix线程池提供应用内的隔离，而Kubernetes通过资源、进程和命名空间来提供隔离。Spring Cloud为每个微服务提供健康终端，而Kubernetes执行健康检查，且把流量导到健康服务。Spring Cloud外部化配置并更新它们，而Kubernetes分发配置到每个微服务。 对于一名开发人员或者架构师来说，想要精通微服务设计与开发，能够在大中型项目中应用微服务架构，单纯掌握Spring Cloud是远远不够的，Docker和Kubernetes等都是需要学习和掌握的内容。同时，由于采用微服务架构后带来了分布式的相关问题，对于分布式系统理论也必须有一定的了解。当然，最重要的还是对系统业务的深入理解，对整体业务进行合理的规划和拆分，才能真正行之有效的应用微服务架构，构建高效、健壮、灵活、可扩展的微服务应用。 参考链接： https://springcloud.cc https://www.kubernetes.org.cn https://my.oschina.net/u/3677020/blog/1570248 http://blog.csdn.net/rickiyeat/article/details/60792925 http://www.uml.org.cn/wfw/201711271.asp https://projects.spring.io/spring-cloud/","tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生【转载】","date":"2018-03-19T09:44:57.000Z","path":"2018/03/19/dubbo-past-and-present.html","text":"本系列文章将为大家介绍当下最流行的服务治理、微服务等相关内容，从服务治理、SOA、微服务到最新的服务网格（Service Mesh）进行综合介绍和分析。易商阜极自2017年便积极引进微服务的先进理念，运用在项目实践中，为项目集成带来了显著效果。本文将以Dubbo为例，向为大家介绍SOA、服务治理等概念，以及Dubbo的基础知识和最新发展情况。 SOA与服务治理SOA（面向服务的体系结构）概念由来已久，在10多年前便开始进入到我们广大软件开发者的视线中。SOA是一种粗粒度、松耦合服务架构，服务之间通过简单、精确定义接口进行通讯，不涉及底层编程接口和通讯模型。SOA可以看作是B/S模型、Web Service技术之后的自然延伸。 服务治理，也称为SOA治理，是指用来管理SOA的采用和实现的过程。以下是在2006年时IBM对于服务治理要点的总结： 服务定义（服务的范围、接口和边界） 服务部署生命周期（各个生命周期阶段） 服务版本治理（包括兼容性） 服务迁移（启用和退役） 服务注册中心（依赖关系） 服务消息模型（规范数据模型） 服务监视（进行问题确定） 服务所有权（企业组织） 服务测试（重复测试） 服务安全（包括可接受的保护范围） 限于当时的技术发展水平，广大软件设计与开发人员对于SOA和服务治理的技术认知还主要停留在Web Service和ESB总线等技术和规范上，并没有真正在软件开发中得以充分落地。 Dubbo开源直到2011年10月27日，阿里巴巴开源了自己的SOA服务化治理方案的核心框架Dubbo，服务治理和SOA的设计理念开始逐渐在国内软件行业中落地，并被广泛应用。Dubbo作为阿里巴巴内部的SOA服务化治理方案的核心框架，在2012年时已经每天为2000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用，其中既有当当网、网易考拉等互联网公司，也有中国人寿、青岛海尔等传统企业。 Dubbo简介Dubbo是一个高性能服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，使得应用可通过高性能RPC实现服务的输出和输入功能，和Spring框架可以无缝集成。 作为一个分布式服务框架，以及SOA治理方案，Dubbo其功能主要包括：高性能NIO通讯及多协议集成，服务动态寻址与路由，软负载均衡与容错，依赖分析与服务降级等。Dubbo最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。 Dubbo包含远程通讯、集群容错和自动发现三个核心部分。提供透明化的远程方法调用，实现像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入。同时具备软负载均衡及容错机制，可在内网替代F5等硬件负载均衡器，降低成本，减少单点。可以实现服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者。 下图来自从Dubbo官网，描述了服务注册中心、服务提供方、服务消费方、服务监控中心之间的调用关系，具体如下图所示： 节点角色说明： 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 Dubbo总体架构Dubbo框架设计共划分了10层，最上面的Service层是留给实际使用Dubbo开发分布式服务的开发者实现业务逻辑的接口层。图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。 各层说明： Config配置层：对外配置接口，以ServiceConfig、ReferenceConfig为中心，可以直接初始化配置类，也可以通过Spring解析配置生成配置类。 Proxy服务代理层：服务接口透明代理，生成服务的客户端Stub和服务器端Skeleton，以ServiceProxy为中心，扩展接口为ProxyFactory。 Registry注册中心层：封装服务地址的注册与发现，以服务URL为中心，扩展接口为RegistryFactory、Registry、RegistryService。 Cluster路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以Invoker为中心，扩展接口为Cluster、Directory、Router、LoadBalance。 Monitor监控层：RPC调用次数和调用时间监控，以Statistics为中心，扩展接口为MonitorFactory、Monitor、MonitorService。 Protocol远程调用层：封将RPC调用，以Invocation、Result为中心，扩展接口为Protocol、Invoker、Exporter。 Exchange信息交换层：封装请求响应模式，同步转异步，以Request、Response为中心，扩展接口为Exchanger、ExchangeChannel、ExchangeClient、ExchangeServer。 Transport网络传输层：抽象MINA和Netty为统一接口，以Message为中心，扩展接口为Channel、Transporter、Client、Server、Codec。 Serialize数据序列化层：可复用的一些工具，扩展接口为Serialization、ObjectInput、ObjectOutput、ThreadPool。 模块分包 各模块说明： dubbo-common公共逻辑模块：包括Util类和通用模型。 dubbo-remoting远程通讯模块：相当于Dubbo协议的实现，如果RPC用 RMI协议则不需要使用此包。 dubbo-rpc远程调用模块：抽象各种协议，以及动态代理，只包含一对一的调用，不关心集群的管理。 dubbo-cluster集群模块：将多个服务提供方伪装为一个提供方，包括：负载均衡、容错、路由等，集群的地址列表可以是静态配置的，也可以是由注册中心下发。 dubbo-registry注册中心模块：基于注册中心下发地址的集群方式，以及对各种注册中心的抽象。 dubbo-monitor监控模块：统计服务调用次数、调用时间的、调用链跟踪的服务。 dubbo-config配置模块：是Dubbo对外的API，用户通过Config使用Dubbo，隐藏Dubbo所有细节。 dubbo-container容器模块：是一个Standlone的容器，以简单的Main加载Spring启动，因为服务通常不需要Tomcat/JBoss等Web容器的特性，没必要用Web容器去加载服务。 协议支持 Dubbo协议（默认协议） Hessian协议 HTTP协议 RMI协议 WebService协议 Thrift协议 Memcached协议 Redis协议 注册中心（1）Multicast注册中心：Multicast注册中心不需要启动任何中心节点，只要广播地址一样，就可以互相发现。组播受网络结构限制，只适合小规模应用或开发阶段使用。组播地址段：224.0.0.0 - 239.255.255.255。 （2）ZooKeeper注册中心（推荐）：ZooKeeper是Apacahe子项目，是一个树型的目录服务，支持变更推送，适合作为Dubbo服务的注册中心，可用于生产环境。 对上图流程说明如下： 服务提供者（Provider）启动时，向/dubbo/com.foo.BarService/providers目录下写入URL。 服务消费者（Consumer）启动时，订阅/dubbo/com.foo.BarService/providers目录下的URL，向/dubbo/com.foo.BarService/consumers目录下写入自己的URL。 监控中心（Monitor）启动时，订阅/dubbo/com.foo.BarService目录下的所有提供者和消费者URL。 （3）Redis注册中心：阿里内部并没有采用Redis做为注册中心，而是使用自己实现的基于数据库的注册中心，即：Redis注册中心并没有在阿里内部长时间运行的可靠性保障，此Redis桥接实现只为开源版本提供，其可靠性依赖于Redis本身的可靠性。 （4）Simple注册中心：Simple注册中心本身就是一个普通的Dubbo服务，可以减少第三方依赖，使整体通讯方式一致。只是简单实现，不支持集群，可作为自定义注册中心的参考，但不适合直接用于生产环境。 远程通信与信息交换远程通信需要指定通信双方所约定的协议，在保证通信双方理解协议语义的基础上，还要保证高效、稳定的消息传输。Dubbo继承了当前主流的网络通信框架，主要包括如下几个： Mina Netty（默认） Grizzly 停止维护从2012年10月23日Dubbo 2.5.3发布后，在Dubbo开源将满一周年之际，阿里基本停止了对Dubbo的主要升级。只在之后的2013年和2014年更新过2次对Dubbo 2.4的维护版本，然后停止了所有维护工作。Dubbo对Srping的支持也停留在了Spring 2.5.6版本上。 分支出现在阿里停止维护和升级Dubbo期间，当当网开始维护自己的Dubbo分支版本Dubbox，支持了新版本的Spring，并对外开源了Dubbox。同时，网易考拉也维护了自己的独立分支Dubbok，可惜并未对外开源。 重获新生经过多年漫长的等待，随着微服务的火热兴起，在国内外开发者对阿里不再升级维护Dubbo的吐槽声中，阿里终于开始重新对Dubbo的升级和维护工作。在2017年9月7日 ，阿里发布了Dubbo的2.5.4版本，距离上一个版本2.5.3发布已经接近快5年时间了。在随后的几个月中，阿里Dubbo开发团队以差不多每月一版本的速度开始快速升级迭代，修补了Dubbo老版本多年来存在的诸多bug，并对Spring等组件的支持进行了全面升级。 分支合并在2018年1月8日，Dubbo 2.6.0版本发布，新版本将之前当当网开源的Dubbo分支Dubbox进行了合并，实现了Dubbo版本的统一整合。 Dubbo与Spring Cloud阿里巴巴负责主导了 Dubbo 重启维护的研发工程师刘军在接受采访时表示：当前由于 RPC 协议、注册中心元数据不匹配等问题，在面临微服务基础框架选型时Dubbo与Spring Cloud是只能二选一，这也是为什么大家总是拿Dubbo和Spring Cloud做对比的原因之一。Dubbo之后会积极寻求适配到Spring Cloud生态，比如作为Spring Cloud的二进制通信方案来发挥Dubbo的性能优势，或者Dubbo通过模块化以及对http的支持适配到Spring Cloud。 未来展望2018年1月8日，Dubbo创始人之一梁飞在Dubbo交流群里透露了Dubbo 3.0正在动工的消息。Dubbo 3.0内核与Dubbo 2.0完全不同，但兼容Dubbo 2.0。Dubbo 3.0将以Streaming为内核，不再是Dubbo时代的RPC，但是RPC会在Dubbo 3.0中变成远程Streaming对接的一种可选形态。Dubbo 3.0将支持可选Service Mesh，多加一层IPC，这主要是为了兼容老系统，而内部则会优先尝试内嵌模式。代理模式Ops可独立升级框架，减少业务侵入，而内嵌模式可以带业务测试、部署节点少、稳定性检测方便。同时，可以将Dubbo 3.0启动为独立进程，由dubbo-mesh进行IPC，路由、负载均衡和熔断机制将由独立进程控制。 总结从 Dubbo 新版本的路线规划上可以看出，新版本的Dubbo在原有服务治理的功能基础上，将全面拥抱微服务和Service Mesh。同时，考虑到在阿里云已经有了Dubbo的商业版本，在未来一段时间内，Dubbo的更新与维护应该不会再长时间中断。在我们进行服务治理以及微服务架构设计时，新版本Dubbo对我们广大开发者来说都将会是一个不错的选择。 参考链接 http://dubbo.io https://github.com/alibaba/dubbo http://shiyanjun.cn/archives/325.html http://mp.weixin.qq.com/s/eVYx-tUIMYtAk5wP-qkdkw 原文链接：浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生","tags":[{"name":"Dubbo,Service Mesh,微服务,ZooKeeper","slug":"Dubbo-Service-Mesh-微服务-ZooKeeper","permalink":"http://yoursite.com/tags/Dubbo-Service-Mesh-微服务-ZooKeeper/"}]},{"title":"如何选择 Linux 上的跟踪器","date":"2018-03-18T14:38:01.000Z","path":"2018/03/18/choosing-a-linux-tracer.html","text":"tracer是一个高级的性能分析和诊断工具，但是不要让这名词唬住你，如果你使用过 strace 和 tcpdump，其实你就已经使用过跟踪器了。系统跟踪器可以获取更多的系统调用和数据包。它们通常能跟踪任意的内核和应用程序。 有太多的 Linux 跟踪器可以选择。每一种都有其官方的（或非官方的）的卡通的独角兽吉祥物，足够撑起一台”儿童剧”了。 那么我们应该使用哪个跟踪器呢？ 可分为两类：大多数人和性能/内核工程师。 对于大多数人大多数人 (开发者，系统管理员，开发管理者，运维人员，评测人员，等等) 不关心系统追踪器的细节。下面是对于追踪器你应该知道和做的： 1. 使用 perf_events 分析 CPU 性能使用 perf_events 做 CPU 性能分析。性能指标可以使用 flame graph 等工具做可视化。 git clone --depth 1 https://github.com/brendangregg/FlameGraph perf record -F 99 -a -g -- sleep 30 perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; perf.svg Linux perf_events (又称 “perf”，同命令名) 是 Linux 用户的官方跟踪器和性能分析器。内置于内核代码，有很好维护（近来获得快速增强），通常通过 linux-tools-common 软件包安装。 perf 有很多功能，如果只能推荐一个，我选择 CPU 性能分析。尽管这只是采样，而不是从技术上追踪事件。最难的部分是获取完整的栈和信息，我为 java 和 node.js 做的一个演讲 Linux Profiling at Netflix 中已经说过这个问题。 2. 了解其他的跟踪器正如我一个朋友说的：“你不需要知道如何操作 X 射线机器，但是一旦你吞了一枚硬币，你得知道这得去做 X 射线”，你应该了解各种跟踪器都能做什么，这样就能在你工作中真正需要跟踪器的时候，你既可以选择稍后学习使用，也可以雇相应的人来完成。 简短来说：几乎所有的东西都可以使用跟踪器来进行分析和跟踪。如，文件系统内部、TCP/IP 过程、设备驱动、应用程序内部。可以看一下我的个人网站上关于 ftrace 的文章，还有我写的关于 perf_events 文档介绍，可以做为一个追踪（或者性能分析）的例子。 3. 寻求前端支持工具如果你正想买一个能支持跟踪 Linux 的性能分析工具（有许多卖这类工具的公司）。想像一下，只需要直接点击一下界面就能“洞察”整个系统内核，包括隐藏的不同堆栈位置的热图，我在 Monitorama talk 中介绍了一个这样带图形界面的工具。 我开源了一些我自己开发的前端工具，尽管只是命令行界面而不是图形界面。这些工具也会让人们更加快速容易的使用跟踪器。比如下面的例子，用我的 perf_tool，跟踪一个新进程: # ./execsnoop Tracing exec()s. Ctrl-C to end. PID PPID ARGS 22898 22004 man ls 22905 22898 preconv -e UTF-8 22908 22898 pager -s 22907 22898 nroff -mandoc -rLL=164n -rLT=164n -Tutf8 [...] 在 Netflix 上，我们创建了一个 Vector，一个分析工具的实例，同时也是 Linux 上的跟踪器的最终前端。 对于性能/内核工程师我们的工作变的越来越困难，很多的人会问我们怎么样去追踪，哪种跟踪器可以用！为了正确理解一个跟踪器，你经常需要花上至少100个小时才能做到。理解所有的 linux 跟踪器去做出理性的选择是一个浩大的工程。（我可能是唯一一个快做到这件事情的人） 这里是我的建议，可以二选其一： A) 选中一个全能的跟踪器，并且使它标准化，这将涉及花费大量的时间去弄清楚它在测试环境中的细微差别和安全性。我现在推荐 SystemTap 的最新版本（可以从源代码构建）。我知道有些公司已经选用 LTTng，而且他们用的很好，尽管它不是非常的强大（虽然它更安全）。如果 Sysdig 可以增加追踪点tracepoint或者 kprobes，可以做为另一个候选。 B) 遵循我上面提供的流程图，它将意味着尽可能更多的使用 ftrace 或者 perf_event， 并整合 eBPF，之后其他的跟踪器像 SystemTap/LTTng 会去填补剩下的空白。 这就是我目前在 Netflix 做的工作。 对跟踪器的评价1. ftrace我喜欢用 ftrace，它是内核 hacker 的首选，内置于系统内核，可以使用跟踪点（静态检查点），能调用内核 kprobes 和 uprobes 调试工具。并且提供几个这样的功能：带可选过滤器和参数的事件追踪功能；在内核中进行统计的事件计数和定时功能；还有函数流程遍历的功能。可以看一下内核代码中 ftrace.txt 例子了解一下。ftrace 由 /sys 控制，仅支持单一的 root 用户使用（但是你可以通过缓冲区实例改成支持多用户）。某些时候 ftrace 的操作界面非常繁琐，但是的确非常“hack”，而且它有前端界面。ftace 的主要作者 Steven Rostedt 创建了 trace-cmd 命令工具，而我创建了 perf 的工具集。我对这个工具最大的不满就是它不可编程。举例来说，你不能保存和获取时间戳，不能计算延迟，不能把这些计算结果保存成直方图的形式。你需要转储事件至用户层，并且花一些时间去处理结果。ftrace 可以通过 eBPF 变成可编程的。 2. perf_eventsperf_events 是 Linux 用户的主要跟踪工具，它内置在内核源码中，通常通过 linux-tools-commom 安装。也称为“perf”，即其前端工具名称，它通常用来跟踪和转储信息到一个叫做 perf.data 的文件中，perf.data 文件相当于一个动态的缓冲区，用来保存之后需要处理的结果。ftrace 能做到的，perf_events 大都也可以做到，perf-events 不能做函数流程遍历，少了一点儿“hack”劲儿（但是对于安全/错误检查有更好的支持）。它可以进行 CPU 分析和性能统计，用户级堆栈解析，也可以使用对于跟踪每行局部变量产生的调试信息。它也支持多用户并发操作。和 ftrace 一样也不支持可编程。如果要我只推荐一款跟踪器，那一定是 perf 了。它能解决众多问题，并且它相对较安全。 3. eBPFextended Berkeley Packet Filter（eBPF）是一个可以在事件上运行程序的高效内核虚拟机（JIT）。它可能最终会提供 ftrace 和 perf_events 的内核编程，并强化其他的跟踪器。这是 Alexei Starovoitov 目前正在开发的，还没有完全集成，但是从4.1开始已经对一些优秀的工具有足够的内核支持了，如块设备 I/O 的延迟热图。可参考其主要作者 Alexei Starovoitov 的 BPF slides 和 eBPF samples。 4. SystemTapSystemTap 是最强大的跟踪器。它能做所有事情，如概要分析，跟踪点，探针，uprobes（来自SystemTap），USDT 和内核编程等。它将程序编译为内核模块，然后加载，这是一种获取安全的巧妙做法。它也是从 tree 发展而来，过去有很多问题（崩溃或冻结）。很多不是 SystemTap 本身的错——它常常是第一个使用某个内核追踪功能，也是第一个碰到 bug 的。SystemTap 的最新版本好多了（必须由源代码编译），但是很多人仍然会被早期版本吓到。如果你想用它，可先在测试环境中使用，并与 irc.freenode.net 上 的 #systemtap 开发人员交流。（Netflix 有容错机制，我们已经使用了 SystemTap，但是可能我们考虑的安全方面的问题比你们少。）我最大的不满是，它似乎认为你应该有内核 debug 信息，但是经常没有。实际上没有它也能做很多事情，但是缺少文档和例子（我必须自己全靠自己开始学习）。 5. LTTngLTTng 优化了事件采集，这比其他跟踪器做得好，它也支持几种事件类型，包括 USTD。它从 tree 发展而来，它的核心很简单：通过一组小规模的固定指令集将事件写入追踪缓冲区，这种方式使它安全、快速，缺点是它没有内核编码的简单途径。我一直听说这不是一个大问题，因为尽管需要后期处理，它也已经优化到可以充分的度量。此外，它还首创了一个不同的分析技术，对所有关注事件的更多黑盒记录将能够稍后以 GUI 的方式进行研究。我关心的是前期没有考虑到要录制的事件缺失问题如何解决，但我真正要做的是花更多时间来看它在实践中用的怎么样。这是我花的时间最少的一个跟踪器（没有什么特殊原因）。 6. Ktapktap 是一款前景很好的跟踪器，它使用内核中的 lua 虚拟机处理，在没有调试信息的情况下在嵌入式设备上运行的很好。这让它得到了关注，并在有一段时间似乎超过了 Linux 上所有的追踪器。然后 eBPF 开始集成到内核了，而 ktap 的集成会在可以使用 eBPF 替代它自己的虚拟机后才开始。因为 eBPF 仍将持续集成几个月，ktap 开发者要继续等上一段时间。我希望今年晚些时候它能重新开发。 7. dtrace4linuxdtrace4linux 主要是 Paul Fox 一个人在业余时间完成的，它是 Sun DTrace 的 Linux 版本。它引人瞩目，已经有一些供应器provider可以工作，但是从某种程度上来说还不完整，更多的是一种实验性的工具（不安全）。我认为，顾忌到许可证问题，人们会小心翼翼的为 dtrace4linux 贡献代码：由于当年 Sun 开源DTrace 使用的是 CDDL 协议，而 dtrace4linux 也不大可能最终进入 Linux kernel。Paul 的方法很可能会使其成为一个 add-on。我很乐意看到 Linux 平台上的 DTrace 和这个项目的完成，我认为当我加入 Netflix 后将会花些时间来协助完成这个项目。然而，我还是要继续使用内置的跟踪器，如 ftrace 和 perf_events。 8. OL DTraceOracle Linux DTrace 为了将 DTrace 引入 Linux，特别是为 Oracle Linux，做出了很大的努力。这些年来发布的多个版本表明了它的稳定进展。开发者们以一种对这个项目的前景看好的态度谈论着改进 DTrace 测试套件。很多有用的 供应器provider 已经完成了，如：syscall, profile, sdt, proc, sched 以及 USDT。我很期待 fbt（function boundary tracing，用于内核动态跟踪）的完成，它是 Linux 内核上非常棒的 供应器provider。OL DTrace 最终的成功将取决于人们对运行 Oracle Linux（为技术支持付费）有多大兴趣，另一方面取决于它是否完全开源：它的内核元件是开源的，而我没有看到它的用户级别代码。 9. sysdigsysdig 是一个使用类 tcpdump 语法在系统事件上操作的新跟踪器，它使用 lua 进行后期处理。它很优秀，它见证了系统跟踪领域的变革。它的局限性在于它只在当前进行系统调用，将所有事件转储为用户级别用于后期处理。你可以使用系统调用做很多事情，然而我还是很希望它能支持跟踪点、kprobe 和 uprobe。我还期待它能支持 eBPF 做内核摘要。目前，sysdig 开发者正在增加容器支持。留意这些内容。","tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]}]