[{"title":"Kubernetes 排错指南：POD【转载】","date":"2018-05-08T15:38:43.000Z","path":"2018/05/08/troubleshooting-pod.html","text":"本章介绍 Pod 运行异常的排错方法。 一般来说，无论 Pod 处于什么异常状态，都可以执行以下命令来查看 Pod 的状态 kubectl get pod &lt;pod-name&gt; -o yaml 查看 Pod 的配置是否正确 kubectl describe pod &lt;pod-name&gt; 查看 Pod 的事件 kubectl logs &lt;pod-name&gt; [-c &lt;container-name&gt;] 查看容器日志 这些事件和日志通常都会有助于排查 Pod 发生的问题。 Pod 一直处于 Pending 状态Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 kubectl describe pod &lt;pod-name&gt; 命令查看到当前 Pod 的事件，进而判断为什么没有调度。如 123456$ kubectl describe pod mypod...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 12s (x6 over 27s) default-scheduler 0/4 nodes are available: 2 Insufficient cpu. 可能的原因包括 资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 或者临时存储空间等资源。解决方法是删除集群内不用的 Pod 或者增加新的 Node。 HostPort 端口已被占用，通常推荐使用 Service 对外开放服务端口 Pod 一直处于 Waiting 或 ContainerCreating 状态首先还是通过 kubectl describe pod &lt;pod-name&gt; 命令查看到当前 Pod 的事件 123456789$ kubectl -n kube-system describe pod nginx-podEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned nginx-pod to node1 Normal SuccessfulMountVolume 1m kubelet, gpu13 MountVolume.SetUp succeeded for volume &quot;config-volume&quot; Normal SuccessfulMountVolume 1m kubelet, gpu13 MountVolume.SetUp succeeded for volume &quot;coredns-token-sxdmc&quot; Warning FailedSync 2s (x4 over 46s) kubelet, gpu13 Error syncing pod Normal SandboxChanged 1s (x4 over 46s) kubelet, gpu13 Pod sandbox changed, it will be killed and re-created. 可以发现，该 Pod 的 Sandbox 容器无法正常启动，具体原因需要查看 Kubelet 日志： 123456$ journalctl -u kubelet...Mar 14 04:22:04 node1 kubelet[29801]: E0314 04:22:04.649912 29801 cni.go:294] Error adding network: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.4.1/24Mar 14 04:22:04 node1 kubelet[29801]: E0314 04:22:04.649941 29801 cni.go:243] Error while adding to cni network: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.4.1/24Mar 14 04:22:04 node1 kubelet[29801]: W0314 04:22:04.891337 29801 cni.go:258] CNI failed to retrieve network namespace path: Cannot find network namespace for the terminated container &quot;c4fd616cde0e7052c240173541b8543f746e75c17744872aa04fe06f52b5141c&quot;Mar 14 04:22:05 node1 kubelet[29801]: E0314 04:22:05.965801 29801 remote_runtime.go:91] RunPodSandbox from runtime service failed: rpc error: code = 2 desc = NetworkPlugin cni failed to set up pod &quot;nginx-pod&quot; network: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.4.1/24 发现是 cni0 网桥配置了一个不同网段的 IP 地址导致，删除该网桥（网络插件会自动重新创建）即可修复 12$ ip link set cni0 down$ brctl delbr cni0 除了以上错误，其他可能的原因还有 镜像拉取失败，比如 配置了错误的镜像 Kubelet 无法访问镜像（国内环境访问 gcr.io 需要特殊处理） 私有镜像的密钥配置错误 镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项） CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如 无法配置 Pod 网络 无法分配 IP 地址 容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数 Pod 处于 ImagePullBackOff 状态这通常是镜像名称配置错误或者私有镜像的密钥配置错误导致。这种情况可以使用 docker pull &lt;image&gt; 来验证镜像是否可以正常拉取。 12345678910111213$ kubectl describe pod mypod...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 36s default-scheduler Successfully assigned sh to k8s-agentpool1-38622806-0 Normal SuccessfulMountVolume 35s kubelet, k8s-agentpool1-38622806-0 MountVolume.SetUp succeeded for volume &quot;default-token-n4pn6&quot; Normal Pulling 17s (x2 over 33s) kubelet, k8s-agentpool1-38622806-0 pulling image &quot;a1pine&quot; Warning Failed 14s (x2 over 29s) kubelet, k8s-agentpool1-38622806-0 Failed to pull image &quot;a1pine&quot;: rpc error: code = Unknown desc = Error response from daemon: repository a1pine not found: does not exist or no pull access Warning Failed 14s (x2 over 29s) kubelet, k8s-agentpool1-38622806-0 Error: ErrImagePull Normal SandboxChanged 4s (x7 over 28s) kubelet, k8s-agentpool1-38622806-0 Pod sandbox changed, it will be killed and re-created. Normal BackOff 4s (x5 over 25s) kubelet, k8s-agentpool1-38622806-0 Back-off pulling image &quot;a1pine&quot; Warning Failed 1s (x6 over 25s) kubelet, k8s-agentpool1-38622806-0 Error: ImagePullBackOff 如果是私有镜像，需要首先创建一个 docker-registry 类型的 Secret 1kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL 然后在容器中引用这个 Secret 123456spec: containers: - name: private-reg-container image: &lt;your-private-image&gt; imagePullSecrets: - name: my-secret Pod 一直处于 CrashLoopBackOff 状态CrashLoopBackOff 状态说明容器曾经启动了，但又异常退出了。此时 Pod 的 RestartCounts 通常是大于 0 的，可以先查看一下容器的日志 123kubectl describe pod &lt;pod-name&gt;kubectl logs &lt;pod-name&gt;kubectl logs --previous &lt;pod-name&gt; 这里可以发现一些容器退出的原因，比如 容器进程退出 健康检查失败退出 OOMKilled 123456789101112131415161718192021222324$ kubectl describe pod mypod...Containers: sh: Container ID: docker://3f7a2ee0e7e0e16c22090a25f9b6e42b5c06ec049405bc34d3aa183060eb4906 Image: alpine Image ID: docker-pullable://alpine@sha256:7b848083f93822dd21b0a2f14a110bd99f6efb4b838d499df6d04a49d0debf8b Port: &lt;none&gt; Host Port: &lt;none&gt; State: Terminated Reason: OOMKilled Exit Code: 2 Last State: Terminated Reason: OOMKilled Exit Code: 2 Ready: False Restart Count: 3 Limits: cpu: 1 memory: 1G Requests: cpu: 100m memory: 500M... 如果此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因 1kubectl exec cassandra -- cat /var/log/cassandra/system.log 如果还是没有线索，那就需要 SSH 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查了 12345# Query Nodekubectl get pod &lt;pod-name&gt; -o wide# SSH to Nodessh &lt;username&gt;@&lt;node-name&gt; Pod 处于 Error 状态通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括 依赖的 ConfigMap、Secret 或者 PV 等不存在 请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等 违反集群的安全策略，比如违反了 PodSecurityPolicy 等 容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定 Pod 处于 Terminating 或 Unknown 状态从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。想要删除这些状态的 Pod 有三种方法： 从集群中删除该 Node。使用公有云时，kube-controller-manager 会在 VM 删除后自动删除对应的 Node。而在物理机部署的集群中，需要管理员手动删除 Node（如 kubectl delete node &lt;node-name&gt;。 Node 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再决定删除或者继续运行这些 Pod。 用户强制删除。用户可以执行 kubectl delete pods &lt;pod&gt; --grace-period=0 --force 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。 如果 Kubelet 是以 Docker 容器的形式运行的，此时 kubelet 日志中可能会发现如下的错误： 123&#123;\"log\":\"I0926 19:59:07.162477 54420 kubelet.go:1894] SyncLoop (DELETE, \\\"api\\\"): \\\"billcenter-737844550-26z3w_meipu(30f3ffec-a29f-11e7-b693-246e9607517c)\\\"\\n\",\"stream\":\"stderr\",\"time\":\"2017-09-26T11:59:07.162748656Z\"&#125;&#123;\"log\":\"I0926 19:59:39.977126 54420 reconciler.go:186] operationExecutor.UnmountVolume started for volume \\\"default-token-6tpnm\\\" (UniqueName: \\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\") pod \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\" (UID: \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\") \\n\",\"stream\":\"stderr\",\"time\":\"2017-09-26T11:59:39.977438174Z\"&#125;&#123;\"log\":\"E0926 19:59:39.977461 54420 nestedpendingoperations.go:262] Operation for \\\"\\\\\\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\\\\\" (\\\\\\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\\\\\")\\\" failed. No retries permitted until 2017-09-26 19:59:41.977419403 +0800 CST (durationBeforeRetry 2s). Error: UnmountVolume.TearDown failed for volume \\\"default-token-6tpnm\\\" (UniqueName: \\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\") pod \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\" (UID: \\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\") : remove /var/lib/kubelet/pods/30f3ffec-a29f-11e7-b693-246e9607517c/volumes/kubernetes.io~secret/default-token-6tpnm: device or resource busy\\n\",\"stream\":\"stderr\",\"time\":\"2017-09-26T11:59:39.977728079Z\"&#125; 如果是这种情况，则需要给 kubelet 容器设置 --containerized 参数并传入以下的存储卷 12345678910111213141516# 以使用 calico 网络插件为例 -v /:/rootfs:ro,shared \\ -v /sys:/sys:ro \\ -v /dev:/dev:rw \\ -v /var/log:/var/log:rw \\ -v /run/calico/:/run/calico/:rw \\ -v /run/docker/:/run/docker/:rw \\ -v /run/docker.sock:/run/docker.sock:rw \\ -v /usr/lib/os-release:/etc/os-release \\ -v /usr/share/ca-certificates/:/etc/ssl/certs \\ -v /var/lib/docker/:/var/lib/docker:rw,shared \\ -v /var/lib/kubelet/:/var/lib/kubelet:rw,shared \\ -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \\ -v /etc/kubernetes/config/:/etc/kubernetes/config/ \\ -v /etc/cni/net.d/:/etc/cni/net.d/ \\ -v /opt/cni/bin/:/opt/cni/bin/ \\ Pod 行为异常这里所说的行为异常是指 Pod 没有按预期的行为执行，比如没有运行 podSpec 里面设置的命令行参数。这一般是 podSpec yaml 文件内容有误，可以尝试使用 --validate 参数重建容器，比如 12kubectl delete pod mypodkubectl create --validate -f mypod.yaml 也可以查看创建后的 podSpec 是否是对的，比如 1kubectl get pod mypod -o yaml 修改静态 Pod 的 Manifest 后未自动重建Kubelet 使用 inotify 机制检测 /etc/kubernetes/manifests 目录（可通过 Kubelet 的 --pod-manifest-path 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。 参考文档 Troubleshoot Applications","tags":[{"name":"Kubernetes 排错指南","slug":"Kubernetes-排错指南","permalink":"http://yoursite.com/tags/Kubernetes-排错指南/"}]},{"title":"Kubernetes 排错指南：集群【转载】","date":"2018-05-07T10:04:53.000Z","path":"2018/05/07/troubleshooting-cluster.html","text":"本章介绍集群状态异常的排错方法，包括 Kubernetes 主要组件以及必备扩展（如 kube-dns）等，而有关网络的异常排错请参考网络异常排错方法。 概述排查集群状态异常问题通常从 Node 和 Kubernetes 服务 的状态出发，定位出具体的异常服务，再进而寻找解决方法。集群状态异常可能的原因比较多，常见的有 虚拟机或物理机宕机 网络分区 Kubernetes 服务未正常启动 数据丢失或持久化存储不可用（一般在公有云或私有云平台中） 操作失误（如配置错误） 按照不同的组件来说，具体的原因可能包括 kube-apiserver 无法启动会导致 集群不可访问 已有的 Pod 和服务正常运行（依赖于 Kubernetes API 的除外） etcd 集群异常会导致 kube-apiserver 无法正常读写集群状态，进而导致 Kubernetes API 访问出错 kubelet 无法周期性更新状态 kube-controller-manager/kube-scheduler 异常会导致 复制控制器、节点控制器、云服务控制器等无法工作，从而导致 Deployment、Service 等无法工作，也无法注册新的 Node 到集群中来 新创建的 Pod 无法调度（总是 Pending 状态） Node 本身宕机或者 Kubelet 无法启动会导致 Node 上面的 Pod 无法正常运行 已在运行的 Pod 无法正常终止 网络分区会导致 Kubelet 等与控制平面通信异常以及 Pod 之间通信异常 为了维持集群的健康状态，推荐在部署集群时就考虑以下 在云平台上开启 VM 的自动重启功能 为 Etcd 配置多节点高可用集群，使用持久化存储（如 AWS EBS 等），定期备份数据 为控制平面配置高可用，比如多 kube-apiserver 负载均衡以及多节点运行 kube-controller-manager、kube-scheduler 以及 kube-dns 等 尽量使用复制控制器和 Service，而不是直接管理 Pod 跨地域的多 Kubernetes 集群 查看 Node 状态一般来说，可以首先查看 Node 的状态，确认 Node 本身是不是 Ready 状态 12kubectl get nodeskubectl describe node &lt;node-name&gt; 如果是 NotReady 状态，则可以执行 kubectl describe node &lt;node-name&gt; 命令来查看当前 Node 的事件。这些事件通常都会有助于排查 Node 发生的问题。 SSH 登录 Node在排查 Kubernetes 问题时，通常需要 SSH 登录到具体的 Node 上面查看 kubelet、docker、iptables 等的状态和日志。在使用云平台时，可以给相应的 VM 绑定一个公网 IP；而在物理机部署时，可以通过路由器上的端口映射来访问。但更简单的方法是使用 SSH Pod （不要忘记替换成你自己的 nodeName）： 123456789101112131415161718192021222324252627282930313233343536373839# cat ssh.yamlapiVersion: v1kind: Servicemetadata: name: sshspec: selector: app: ssh type: LoadBalancer ports: - protocol: TCP port: 22 targetPort: 22---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: ssh labels: app: sshspec: replicas: 1 selector: matchLabels: app: ssh template: metadata: labels: app: ssh spec: containers: - name: alpine image: alpine ports: - containerPort: 22 stdin: true tty: true hostNetwork: true nodeName: &lt;node-name&gt; 1234$ kubectl create -f ssh.yaml$ kubectl get svc sshNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEssh LoadBalancer 10.0.99.149 52.52.52.52 22:32008/TCP 5m 接着，就可以通过 ssh 服务的外网 IP 来登录 Node，如 ssh user@52.52.52.52。 在使用完后， 不要忘记删除 SSH 服务 kubectl delete -f ssh.yaml。 查看日志一般来说，Kubernetes 的主要组件有两种部署方法 直接使用 systemd 等启动控制节点的各个服务 使用 Static Pod 来管理和启动控制节点的各个服务 使用 systemd 等管理控制节点服务时，查看日志必须要首先 SSH 登录到机器上，然后查看具体的日志文件。如 12345journalctl -l -u kube-apiserverjournalctl -l -u kube-controller-managerjournalctl -l -u kube-schedulerjournalctl -l -u kubeletjournalctl -l -u kube-proxy 或者直接查看日志文件 /var/log/kube-apiserver.log /var/log/kube-scheduler.log /var/log/kube-controller-manager.log /var/log/kubelet.log /var/log/kube-proxy.log 而对于使用 Static Pod 部署集群控制平面服务的场景，可以参考下面这些查看日志的方法。 kube-apiserver 日志12PODNAME=$(kubectl -n kube-system get pod -l component=kube-apiserver -o jsonpath='&#123;.items[0].metadata.name&#125;')kubectl -n kube-system logs $PODNAME --tail 100 kube-controller-manager 日志12PODNAME=$(kubectl -n kube-system get pod -l component=kube-controller-manager -o jsonpath='&#123;.items[0].metadata.name&#125;')kubectl -n kube-system logs $PODNAME --tail 100 kube-scheduler 日志12PODNAME=$(kubectl -n kube-system get pod -l component=kube-scheduler -o jsonpath='&#123;.items[0].metadata.name&#125;')kubectl -n kube-system logs $PODNAME --tail 100 kube-dns 日志12PODNAME=$(kubectl -n kube-system get pod -l k8s-app=kube-dns -o jsonpath='&#123;.items[0].metadata.name&#125;')kubectl -n kube-system logs $PODNAME -c kubedns Kubelet 日志查看 Kubelet 日志需要首先 SSH 登录到 Node 上。 1journalctl -l -u kubelet Kube-proxy 日志Kube-proxy 通常以 DaemonSet 的方式部署 123456$ kubectl -n kube-system get pod -l component=kube-proxyNAME READY STATUS RESTARTS AGEkube-proxy-42zpn 1/1 Running 0 1dkube-proxy-7gd4p 1/1 Running 0 3dkube-proxy-87dbs 1/1 Running 0 4d$ kubectl -n kube-system logs kube-proxy-42zpn Kube-dns/Dashboard CrashLoopBackOff由于 Dashboard 依赖于 kube-dns，所以这个问题一般是由于 kube-dns 无法正常启动导致的。查看 kube-dns 的日志 123$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c dnsmasq$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c sidecar 可以发现如下的错误日志 123Waiting for services and endpoints to be initialized from apiserver...skydns: failure to forward request \"read udp 10.240.0.18:47848-&gt;168.63.129.16:53: i/o timeout\"Timeout waiting for initialization 这说明 kube-dns pod 无法转发 DNS 请求到上游 DNS 服务器。解决方法为 如果使用的 Docker 版本大于 1.12，则在每个 Node 上面运行 iptables -P FORWARD ACCEPT 等待一段时间，如果还未恢复，则检查 Node 网络是否正确配置，比如是否可以正常请求上游DNS服务器、是否有安全组禁止了 DNS 请求等 如果错误日志中不是转发 DNS 请求超时，而是访问 kube-apiserver 超时，比如 123456E0122 06:56:04.774977 1 reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeoutI0122 06:56:04.775358 1 dns.go:174] Waiting for services and endpoints to be initialized from apiserver...E0122 06:56:04.775574 1 reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Service: Get https://10.0.0.1:443/api/v1/services?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeoutI0122 06:56:05.275295 1 dns.go:174] Waiting for services and endpoints to be initialized from apiserver...I0122 06:56:05.775182 1 dns.go:174] Waiting for services and endpoints to be initialized from apiserver...I0122 06:56:06.275288 1 dns.go:174] Waiting for services and endpoints to be initialized from apiserver... 这说明 Pod 网络（一般是多主机之间）访问异常，包括 Pod-&gt;Node、Node-&gt;Pod 以及 Node-Node 等之间的往来通信异常。可能的原因比较多，具体的排错方法可以参考网络异常排错指南。 Kubelet: failed to initialize top level QOS containers重启 kubelet 时报错 Failed to start ContainerManager failed to initialise top level QOS containers（参考 #43856），临时解决方法是： 在 docker.service 配置中增加 --exec-opt native.cgroupdriver=systemd 选项。 重启主机 该问题已于2017年4月27日修复（v1.7.0+， #44940）。更新集群到新版本即可解决这个问题。 Kubelet 一直报 FailedNodeAllocatableEnforcement 事件当 NodeAllocatable 特性未开启时（即 kubelet 设置了 --cgroups-per-qos=false ），查看 node 的事件会发现每分钟都会有 Failed to update Node Allocatable Limits 的警告信息： 12345$ kubectl describe node node1Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedNodeAllocatableEnforcement 2m (x1001 over 16h) kubelet, aks-agentpool-22604214-0 Failed to update Node Allocatable Limits \"\": failed to set supported cgroup subsystems for cgroup : Failed to set config for supported subsystems : failed to write 7285047296 to memory.limit_in_bytes: write /var/lib/docker/overlay2/5650a1aadf9c758946073fefa1558446ab582148ddd3ee7e7cb9d269fab20f72/merged/sys/fs/cgroup/memory/memory.limit_in_bytes: invalid argument 如果 NodeAllocatable 特性确实不需要，那么该警告事件可以忽略。但根据 Kubernetes 文档 Reserve Compute Resources for System Daemons，最好开启该特性： Kubernetes nodes can be scheduled to Capacity. Pods can consume all the available capacity on a node by default. This is an issue because nodes typically run quite a few system daemons that power the OS and Kubernetes itself. Unless resources are set aside for these system daemons, pods and system daemons compete for resources and lead to resource starvation issues on the node. The kubelet exposes a feature named Node Allocatable that helps to reserve compute resources for system daemons. Kubernetes recommends cluster administrators to configure Node Allocatable based on their workload density on each node. 1234567891011121314 Node Capacity---------------------------| kube-reserved ||-------------------------|| system-reserved ||-------------------------|| eviction-threshold ||-------------------------|| || allocatable || (available for pods) || || |--------------------------- 开启方法为： 1kubelet --cgroups-per-qos=true --enforce-node-allocatable=pods ... Kube-proxy: error looking for path of conntrackkube-proxy 报错，并且 service 的 DNS 解析异常 1kube-proxy[2241]: E0502 15:55:13.889842 2241 conntrack.go:42] conntrack returned error: error looking for path of conntrack: exec: \"conntrack\": executable file not found in $PATH 解决方式是安装 conntrack-tools 包后重启 kube-proxy 即可。 Dashboard 中无资源使用图表正常情况下，Dashboard 首页应该会显示资源使用情况的图表，如 如果没有这些图表，则需要首先检查 Heapster 是否正在运行（因为Dashboard 需要访问 Heapster 来查询资源使用情况）： 123kubectl -n kube-system get pods -l k8s-app=heapsterNAME READY STATUS RESTARTS AGEheapster-86b59f68f6-h4vt6 2/2 Running 0 5d 如果查询结果为空，说明 Heapster 还未部署，可以参考 https://github.com/kubernetes/heapster 来部署。 但如果 Heapster 处于正常状态，那么需要查看 dashboard 的日志，确认是否还有其他问题 12345$ kubectl -n kube-system get pods -l k8s-app=kubernetes-dashboardNAME READY STATUS RESTARTS AGEkubernetes-dashboard-665b4f7df-dsjpn 1/1 Running 0 5d$ kubectl -n kube-system logs kubernetes-dashboard-665b4f7df-dsjpn HPA 不自动扩展 Pod查看 HPA 的事件，发现 1234567891011121314151617181920$ kubectl describe hpa php-apacheName: php-apacheNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;CreationTimestamp: Wed, 27 Dec 2017 14:36:38 +0800Reference: Deployment/php-apacheMetrics: ( current / target ) resource cpu on pods (as a percentage of request): &lt;unknown&gt; / 50%Min replicas: 1Max replicas: 10Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True SucceededGetScale the HPA controller was able to get the target's current scale ScalingActive False FailedGetResourceMetric the HPA was unable to compute the replica count: unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedGetResourceMetric 3m (x2231 over 18h) horizontal-pod-autoscaler unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io) 这说明 metrics-server 未部署，可以参考 这里 部署。 Node 存储空间不足Node 存储空间不足一般是容器镜像未及时清理导致的，比如短时间内运行了很多使用较大镜像的容器等。Kubelet 会自动清理未使用的镜像，但如果想要立即清理，可以使用 spotify/docker-gc： 1sudo docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v /etc:/etc:ro spotify/docker-gc 参考文档 Troubleshoot Clusters SSH into Azure Container Service (AKS) cluster nodes Kubernetes dashboard FAQ","tags":[{"name":"Kubernetes 排错指南","slug":"Kubernetes-排错指南","permalink":"http://yoursite.com/tags/Kubernetes-排错指南/"}]},{"title":"Kubernetes 排错指南：开篇【转载】","date":"2018-05-07T08:28:08.000Z","path":"2018/05/07/troubleshooting-index.html","text":"Kubernetes 正在越来越多的部署到生产环境中，随之而来的就是集群的运维和日常排错。接下来会分几篇文章介绍 Kubernetes 集群以及应用排错的一般方法，主要内容包括 集群状态异常排错 Pod运行异常排错 网络异常排错 持久化存储异常排错 AzureDisk 排错 AzureFile 排错 Windows容器排错 云平台异常排错 Azure 排错 常用排错工具 本文为该系列的开篇，介绍一下常见组件的日志查看方法。 在排错过程中，kubectl 是最重要的工具，通常也是定位错误的起点。这里也列出一些常用的命令，在后续的各种排错过程中都会经常用到。 查看 Pod 状态以及运行节点12kubectl get pods -o widekubectl -n kube-system get pods -o wide 查看 Pod 事件1kubectl describe pod &lt;pod-name&gt; 查看 Node 状态12kubectl get nodeskubectl describe node &lt;node-name&gt; kube-apiserver 日志12PODNAME=$(kubectl -n kube-system get pod -l component=kube-apiserver -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;&apos;)kubectl -n kube-system logs $PODNAME --tail 100 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-apiserver 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-apiserver 查看其日志。 kube-controller-manager 日志12PODNAME=$(kubectl -n kube-system get pod -l component=kube-controller-manager -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;&apos;)kubectl -n kube-system logs $PODNAME --tail 100 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-controller-manager 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-controller-manager 查看其日志。 kube-scheduler 日志12PODNAME=$(kubectl -n kube-system get pod -l component=kube-scheduler -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;&apos;)kubectl -n kube-system logs $PODNAME --tail 100 以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-scheduler 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-scheduler 查看其日志。 kube-dns 日志kube-dns 通常以 Addon 的方式部署，每个 Pod 包含三个容器，最关键的是 kubedns 容器的日志： 12PODNAME=$(kubectl -n kube-system get pod -l k8s-app=kube-dns -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;&apos;)kubectl -n kube-system logs $PODNAME -c kubedns Kubelet 日志Kubelet 通常以 systemd 管理。查看 Kubelet 日志需要首先 SSH 登录到 Node 上。 1journalctl -l -u kubelet Kube-proxy 日志Kube-proxy 通常以 DaemonSet 的方式部署，可以直接用 kubectl 查询其日志 123456$ kubectl -n kube-system get pod -l component=kube-proxyNAME READY STATUS RESTARTS AGEkube-proxy-42zpn 1/1 Running 0 1dkube-proxy-7gd4p 1/1 Running 0 3dkube-proxy-87dbs 1/1 Running 0 4d$ kubectl -n kube-system logs kube-proxy-42zpn","tags":[{"name":"Kubernetes 排错指南","slug":"Kubernetes-排错指南","permalink":"http://yoursite.com/tags/Kubernetes-排错指南/"}]},{"title":"五大 Kubernetes 最佳实践【转载】","date":"2018-04-18T07:58:15.000Z","path":"2018/04/18/top-5-kubernetes-best-practices.html","text":"这篇演讲中的最佳实践来源于Sandeep和团队进行的关于在Kubernetes上以多种不同方式运行同一任务的讨论。他们把讨论的结果总结为一个最佳实践的清单。 这些最佳实践分为如下大类： 构建容器 容器内部 部署 服务 应用架构 1、构建容器不要信任任意的基础镜像不幸的是我们看到这个错误一直在发生， Pradeep说到。人们从DockerHub上随便拉一个某人做的基础镜像——这么做的理由仅仅是第一眼看过去这个镜像里面打包有他们需要的包——接着他们就把这个随便选的镜像推到生产环境中。 这么做是非常错误的：你使用的代码可能有很多漏洞，bug，错误版本，或者本身就被人有意把恶意软件打包进去——只是你不知道罢了。 要减轻这种风险，你可以使用静态分析工具，比如 CoreOS’ Clair 或者 Banyon Collector来对容器进行漏洞扫描。 保持基础镜像尽量小基于最简洁的可用基础镜像，然后基于它构建软件包，这样你就知道镜像里面到底有哪些东西。 越小的基础镜像开销也越小。你的应用可能只要5M， 但是如果你盲目的随便找一个镜像，比如Node.js， 它里面就包括了额外500M你根本要不到的库文件。 使用小镜像的其它优势有： 快速构建 节约存储 拉取镜像更快 更小的潜在攻击面 使用构建器模式这种模式对静态语言特别有用，编译类似Go，C++或者Typescript for Node.js这些语言时。 在这种模式里你有一个构建容器，里面打包有编译器，依赖包，以及单元测试。 代码通过第一步之后产出构建的artifacts，这包括所有的文件，bundles等。然后再通过一个运行时容器，包括有监控和调试工具等。 到最后， 你的Dockerfile里面将会只包含你的基础镜像以及运行时环境容器。 2、容器内部在容器的内部使用非root用户如果你在容器内使用root来更新包，那么你要把用户改成非root用户。 原因很简单，如果你的容器有后门被人利用了而且你还没把它的用户改成root之外的，那么一个简单的容器逃离将会导致你整个主机的root权限都被利用。但是如果你改成了非root用户，黑客就没那么容易得到root用户的权限了。 做为最佳实践，你要对你的基础设施加多层外壳保护。 在Kubernetes里面你可以通过设置安全上下文 runAsNonRoot: true来实现，这样会对整个集群cluster来生效。 文件系统只读这一个最佳实践通过设置readOnlyFileSystem: true来实现。 每个容器里面跑一个进程你当然可以在一个容器里面跑多个进程，但是推荐跑一个。这是由编排器的工作方式决定的。Kubernetes基于一个进程是否健康来管理容器。如果你在一个容器里面有20个进程，它如何知道容器是否健康呢？ 不要使用 Restart on Failure， 而应当 Crash CleanlyKubernetes会重新启动失败的容器，因此你应该干净的做崩溃退出（给出一个错误码），这样Kubernetes就可以不用你的人工干预来成功重起了。 日志打到标准输出和标准错误输出（stdout &amp; stderr)Kubernetes缺省会监听这些管道，然后将输出传到日志服务上面去。在谷歌云上可以直接用StackDriver日志系统。 3、部署使用record选项来使回滚更方便在引用一个yaml文件时，请使用–record选项： kubectl apply -f deployment.yaml --record 带了这个选项之后，每次升级的时都会保存到部署的日志里面，这样就提供了回滚一个变更的能力。 多使用描述性的标签label因为标签可以是任意的键值对，其表达力非常强。参考下图，以有名字为’Nifty‘的应用部署到四个容器里面。 通过选择BE标签你可以挑选出后端容器。 使用sidecar来做代理、监视器等有时候你需要一组进程跟其它某个进程通讯。但是你又不希望把它们所有的都放进一个容器里面（前面提到的一个容器跑一个进程）， 你希望的是把相关的进程都放到一个Pod里面。 常见情况是你需要运行进程依赖的一个代理或者监视器，比如你的进程依赖一个数据库， 而你不希望把数据库的密码硬编码进每个容器里面，这个时会你可以把密码放到一个代理程序里面当作sidecar，由它来管理数据库连接： 不要使用sidecar来做启动引导尽管sidecar在处理集群内外的请求时非常有用，Sandeep不推荐使用它做启动。再过去，引导启动（bootstraping）是唯一选项，但是现在Kubernetes有了“init 容器”。 当容器里面的一个进程依赖于其它的一个微服务时， 你可以使用init容器来等到进程启动以后再启动你的容器。这可以避免当进程和微服务不同步时产生的很多错误。 基本原则就是： 使用sidecar来处理总是发生的事件，而用init容器来处理一次性的事件。 不要使用：latest或者无标签这个原则是很明显的而且大家基本都这么在用。如果你不给你的容器加标签，那么它会总是拉最新的，这个“最新的”并不能保证包括你认为它应该有的那些更新。 善用readiness、liveness探针使用探针可以让Kubernetes知道节点是否正常，以此决定是否把流量发给它。缺省情况下Kubernetes检查进程是否在运行。但是通过使用探针， 你可以在缺省行为下加上你自己的逻辑。 4、服务不要使用type: Loadbalancer每次你在部署文件里面加一个公有云提供商的loadbalancer（负载均衡器）的时候，它都会创建一个。 它确实是高可用，高速度，但是它也有经济成本。 使用Ingress来代替，同样可以实现通过一个end point来负载均衡多个服务。这种方式不但更简单，而且更经济。当然这个策略只有你提供http和web服务时有用，对于普通的TCP/UDP应用就没用了。 Type: Nodeport可能已经够用了这个更多是个人喜好，并不是所有人都推荐。NodePort把你的应用通过一个VM的特定端口暴露到外网上。 问题就是它没有像负载均衡器那样有高可用。比如极端情况，VM挂了你的服务也挂了。 使用静态IP， 它们免费！在谷歌云上很简单，只需要为你的ingress来创建全局IP。类似的对你的负载均衡器可以使用Regional IP。这样当你的服务down了之后你不必担心IP会变。 将外部服务映射到内部Kubernetes提供的这个功能不是所有人都知道。如果您需要群集外部的服务，您可以做的是使用ExternalName类型的服务。这样你就可以通过名字来调用这个服务，Kubernetes manager会把请求传递给它，就好像它在集群之中一样。Kubernetes对待这个服务就好像它在同一个内网里面，即使实际上它不在。 5、应用架构使用Helm ChartsHelm基本上就是打包Kubernetes应用配置的仓库。如果你要部署一个MongoDB， 存在一个预先配置好的Helm chart，包括了它所有的依赖，你可以十分容易的把它部署到集群中。 很多流行的软件/组件都有写好了的Helm charts， 你可以直接用，省掉大量的时间和精力。 所有下游的依赖是不可靠的你的应用应该有逻辑和错误信息负责审计你不能控制的所有依赖。Sandeep建议说你可以使用Istio或者Linkerd这样的服务网格来做下游管理。 使用Weave Cloud集群是很难可视化管理的。 使用Weave Cloud可以帮你监视集群内的情况和跟踪依赖。 确保你的微服务不要太“微小”你需要的是逻辑组件，而不是每个单独的功能/函数都变成一个微服务。 使用Namespace来分离集群例如， 你可以在同一个集群里面创建prod、dev、test这样不同的名字空间，同时可以对不同的名字空间分配资源， 这样万一某个进程有问题也不会用尽所有的集群资源。 基于角色的访问控制RBAC实施时当的访问控制来限制访问量， 这也是最佳的安全实践。 从运行Weave Cloud生产环境学到的教训接下来Jordan Pellizzari做了一个演讲，题目是在过去两年我们在Kubernetes上开发运行Weave Cloud学到的经验。 我们当前运行在AWS EC2上， 总共有72个Kubernetes部署运行在13个主机和150个容器里面。我们所有的持续性存储保存在S3，DynamoDB或者RDS里面， 我们并不在容器里面保存状态信息。 关于我们如何搭建基础设施的细节可以参看这篇文档。 挑战1：对基础设施做版本控制在Weaveworks我们把所有的基础架构保存在Git中， 如果我们要对基础设施做变更，要像代码一样提Pull request。我们把这称为GitOps，也写了多篇博文。你可以从这篇读起： GitOps - Pull Request支撑的运维。 在Weave， 所有的Terraform脚本，Ansible以及Kubernetes YAML文件都被保存在Git里面做版本控制。 把基础架构放在Git里面是一个最佳实践，这有多个原因： 发布可以很方便的回滚 对谁做了什么修改有追踪审计 灾难恢复相当简单 问题： 当生产与版本控制不一致时该怎么办？ 除了把所有内容保存在Git中之外，我们也有一个流程会检查生产集群中运行的状态与版本控制中的内容差异。如果检查到有不同，就会给我们的Slack频道发一个报警。 我们使用一个叫Kube-Diff的开源工具来检查不同。 挑战2：自动化的持续交付自动化你的CI/CD流水线，避免手工的Kubernetes部署。因为我们一天内做多次部署，这种方式节约了团队的时间也避免了手工容易发生错误的步骤。在Weaveworks，开发人员只需要做一个Git push，然后Weave Cloud会做以下的事情： 打过标签的代码通过CircleCI的测试然后构建一个新的容器镜像，推送这个新的镜像到仓库中。 Weave Cloud的“Deploy Automator‘检测到新镜像，从库中拉取新镜像然后在配置库里面更新对应的YAML文件。 Deploy Synchronizer会检测到集群需要更改in了，然后它会从配置库里面拉更新的配置清单，最后将新的镜像部署到集群中。 GitOps流水线 这里有一篇稍长的文章，我们认为的构建自动化CI/CD流水线的最佳实践都在里面描述了。 总结Sandeep Dinesh做了一个关于创建、部署、运行应用到Kubernetes里面的五个最佳实践的深度分享。随后Jordan Pellizzari做了Weave如何在kubernetes中管理SaaS产品Weave Cloud和经验教训的分享。 相关链接： https://www.meetup.com/pro/weave/ https://github.com/coreos/clair https://github.com/banyanops/collector https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ https://www.weave.works/features/troubleshooting-dashboard/ https://www.weave.works/technologies/weaveworks-on-aws/ https://www.weave.works/blog/gitops-operations-by-pull-request https://github.com/weaveworks/kubediff https://www.weave.works/blog/the-gitops-pipeline 原文链接：Top 5 Kubernetes Best Practices From Sandeep Dinesh (Google) 完整视频","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://yoursite.com/tags/Kubernetes/"}]},{"title":"Kubernetes 1.9 与 CentOS 7 内核兼容问题【转载】","date":"2018-04-02T07:14:18.000Z","path":"2018/04/02/kubernetes-19-conflict-with-centos7.html","text":"生产环境发现不定时 Java 应用出现 coredump 故障，测试环境不定时出现写入 /cgroup/memory 报 no space left on device 的故障，导致整个 kubernetes node 节点无法使用。甚至会随着堆积的 cgroup 越来越多，docker ps 执行异常，直到把内存吃光，机器挂死。 典型报错： kubelet.ns-k8s-node001.root.log.ERROR.20180214-113740.15702:1593018:E0320 04:59:09.572336 15702 remote_runtime.go:92] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod &quot;osp-xxx-com-ljqm19-54bf7678b8-bvz9s&quot;: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused &quot;process_linux.go:258: applying cgroup configuration for process caused \\&quot;mkdir /sys/fs/cgroup/memory/kubepods/burstable/podf1bd9e87-1ef2-11e8-afd3-fa163ecf2dce/8710c146b3c8b52f5da62e222273703b1e3d54a6a6270a0ea7ce1b194f1b5053: no space left on device\\&quot;&quot; 或者 Mar 26 18:36:59 ns-k8s-node-s0054 kernel: SLUB: Unable to allocate memory on node -1 (gfp=0x8020) Mar 26 18:36:59 ns-k8s-noah-node001 kernel: cache: ip6_dst_cache(1995:6b6bc0c9f30123084a409d89a300b017d26ee5e2c3ac8a02c295c378f3dbfa5f), object size: 448, buffer size: 448, default order: 2, min order: 0 该问题发生前后，进行过 kubernetes 1.6 到 1.9 的升级工作。排查了好久才定位到问题与 kubernetes 、操作系统内核有关。 1. 对比测试结果使用同样的测试方法，结果为： 使用初次部署 k8s 1.6 版本测试，没有出现 cgroup memory 遗漏问题； 从 k8s 1.6 升级到 1.9 后，测试没有出现 cgroup memory 遗漏问题； 重启 kubelet 1.9 node 节点重启，再次测试，出现 cgroup memory 遗漏问题。 对比 k8s 1.6 和 1.9 创建的 POD 基础容器和业务容器，runc、libcontainerd、docker inspect 的容器 json 参数都一致，没有差异。 为什么同样是 k8s 1.9（其他 docker、kernel 版本一致）的情况下，结果不一样呢？重启的影响是？ 2.问题重现对于 cgroup memory 报 no space left on device ，是由于 cgroup memory 存在 64k（65535 个）大小的限制。 采用下面的测试方式，可以发现在删除 pod 后，会出现 cgroup memory 遗漏的问题。该测试方法通过留空 99 个 系统 cgroup memory 位置，来判断引起问题的原因是由于 pod container 导致的。 1）填满系统 cgroup memory # uname -r 3.10.0-514.10.2.el7.x86_64 # kubelet --version Kubernetes 1.9.0 # mkdir /sys/fs/cgroup/memory/test # for i in `seq 1 65535`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done # cat /proc/cgroups |grep memory memory 11 65535 1 把系统 cgroup memory 填到 65535 个。 2）腾空 99 个 cgroup memory # for i in `seq 1 100`;do rmdir /sys/fs/cgroup/memory/test/test-${i} 2&gt;/dev/null 1&gt;&amp;2; done # mkdir /sys/fs/cgroup/memory/stress/ # for i in `seq 1 100`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-100’: No space left on device # for i in `seq 1 100`;do rmdir /sys/fs/cgroup/memory/test/test-${i}; done # cat /proc/cgroups |grep memory memory 11 65436 1 在写入第 100 个的时候提示无法写入，证明写入了 99 个。 3）创建一个 pod 到这个 node 上，查看占用的 cgroup memory 情况 每创建一个 pod ，会占用 3 个 cgroup memory 目录： # ll /sys/fs/cgroup/memory/kubepods/pod0f6c3c27-3186-11e8-afd3-fa163ecf2dce/ total 0 drwxr-xr-x 2 root root 0 Mar 27 14:14 6d1af9898c7f8d58066d0edb52e4d548d5a27e3c0d138775e9a3ddfa2b16ac2b drwxr-xr-x 2 root root 0 Mar 27 14:14 8a65cb234767a02e130c162e8d5f4a0a92e345bfef6b4b664b39e7d035c63d1 这时再次创建 100 个 cgroup memory ，因为 pod 占用了 3 个，会出现 4 个无法成功： # for i in `seq 1 100`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-97’: No space left on device &amp;lt;-- 3 directory used by pod mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-98’: No space left on device mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-99’: No space left on device mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-100’: No space left on device # cat /proc/cgroups memory 11 65439 1 写入到的 cgroup memory 增加到 65439 个。 4）删掉测试 pod ，看看 3 个占用的 cgroup memory 是否有释放 看到的结果： # cat /proc/cgroups memory 11 65436 1 # for i in `seq 1 100`;do mkdir /sys/fs/cgroup/memory/test/test-${i}; done mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-97’: No space left on device mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-98’: No space left on device mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-99’: No space left on device mkdir: cannot create directory ‘/sys/fs/cgroup/memory/test/test-100’: No space left on device 可以看到，虽然 cgroup memory 减少到 65536 ，似乎 3 个位置释放了。但实际上测试结果发现，并不能写入，结果还是 pod 占用时的无法写入 97-100 。 这就说明，cgroup memory 数量减少，但被 pod container 占用的空间没有释放。 反复验证后，发现随着 pod 发布和变更的增加，该问题会越来越严重，直到把整台机器的 cgroup memory 用完。 $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 10 229418 1 -- 但 cpuset 数量很恐怖 cpu 6 118 1 cpuacct 6 118 1 memory 7 109 1 -- 看上去不多，实际没有释放空间的 devices 3 229504 1 freezer 5 32 1 net_cls 4 32 1 blkio 9 118 1 perf_event 11 32 1 hugetlb 2 32 1 pids 8 32 1 net_prio 4 32 1 3.问题根源经过大量的测试和对比分析，在两个 k8s 1.9 环境中 kubelet 创建的 /sys/fs/cgroup/memory/kubepods 差异，发现： 没问题的： [root@k8s-node01 kubepods]# cat memory.kmem.slabinfo cat: memory.kmem.slabinfo: Input/output error 有问题的： [root@k8s-node01 kubepods]# cat memory.kmem.slabinfo slabinfo - version: 2.1 # name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt; 也就是说，在有问题的环境下，cgroup kernel memory 特性被激活了。 关于 cgroup kernel memory，在 kernel-doc 中有如下描述： # vim /usr/share/doc/kernel-doc-3.10.0/Documentation/cgroups/memory.txt 2.7 Kernel Memory Extension (CONFIG_MEMCG_KMEM) With the Kernel memory extension, the Memory Controller is able to limit the amount of kernel memory used by the system. Kernel memory is fundamentally different than user memory, since it can&apos;t be swapped out, which makes it possible to DoS the system by consuming too much of this precious resource. Kernel memory won&apos;t be accounted at all until limit on a group is set. This allows for existing setups to continue working without disruption. The limit cannot be set if the cgroup have children, or if there are already tasks in the cgroup. Attempting to set the limit under those conditions will return -EBUSY. When use_hierarchy == 1 and a group is accounted, its children will automatically be accounted regardless of their limit value. After a group is first limited, it will be kept being accounted until it is removed. The memory limitation itself, can of course be removed by writing -1 to memory.kmem.limit_in_bytes. In this case, kmem will be accounted, but not limited. 这是一个 cgroup memory 的扩展，用于限制对 kernel memory 的使用。但该特性在大于 4.0 版本中是个实验特性，若使用 docker run 运行，就会提示： # docker run -d --name test001 --kernel-memory 100M registry.vclound.com:5000/hyphenwang/sshdserver:v1 WARNING: You specified a kernel memory limit on a kernel older than 4.0. Kernel memory limits are experimental on older kernels, it won&apos;t work as expected and can cause your system to be unstable. # cat /sys/fs/cgroup/memory/docker/eceb6dfba2c64a783f33bd5e54cecb32d5e64647439b4932468650257ea06206/memory.kmem.limit_in_bytes 104857600 # cat /sys/fs/cgroup/memory/docker/eceb6dfba2c64a783f33bd5e54cecb32d5e64647439b4932468650257ea06206/memory.kmem.slabinfo slabinfo - version: 2.1 # name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt; ip6_dst_cache 0 0 448 18 2 : tunables 0 0 0 : slabdata 0 0 0 UDPv6 0 0 1216 26 8 : tunables 0 0 0 : slabdata 0 0 0 kmalloc-8192 0 0 8192 4 8 : tunables 0 0 0 : slabdata 0 0 0 signal_cache 28 28 1152 28 8 : tunables 0 0 0 : slabdata 1 1 0 sighand_cache 15 15 2112 15 8 : tunables 0 0 0 : slabdata 1 1 0 sysfs_dir_cache 36 36 112 36 1 : tunables 0 0 0 : slabdata 1 1 0 task_struct 8 8 4016 8 8 : tunables 0 0 0 : slabdata 1 1 0 ...... 经过反复验证，当使用 docker run –kernel-memory 参数启动的容器，在删除后也不会释放 cgroup memory 占用的位置 ，存在同样的问题。 基于该现象，对比 kubernetes 1.6 和 kubernetes 1.9 对 cgroup kernel memory 设置的差异。 在k8s 1.9 vendor库中，少了if d.config.KernelMemory != 0 { 这行代码的判断， 而在k8s的Github，1.6.4版本在官方的runc/libcontainerd增加了这行代码，但在1.9.0删掉了，导致默认就使用kmem 。 $ git diff remotes/origin/vip_v1.6.4.3 remotes/origin/vip_v1.9.0 -- vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/memory.go @@ -29,14 +35,18 @@ func (s *MemoryGroup) Apply(d *cgroupData) (err error) { path, err := d.path(&quot;memory&quot;) if err != nil &amp;&amp; !cgroups.IsNotFound(err) { return err + } else if path == &quot;&quot; { + return nil } if memoryAssigned(d.config) { - if path != &quot;&quot; { + if _, err := os.Stat(path); os.IsNotExist(err) { if err := os.MkdirAll(path, 0755); err != nil { return err } - } - if d.config.KernelMemory != 0 { // 删除了这行的判断，使得 1.9 默认就 enable cgroup kernel memory 特性 + // Only enable kernel memory accouting when this cgroup + // is created by libcontainer, otherwise we might get + // error when people use `cgroupsPath` to join an existed + // cgroup whose kernel memory is not initialized. if err := EnableKernelMemoryAccounting(path); err != nil { return err } 这就引发了 cgroup memory 也不能释放的问题（与通过 docker run –kernel-memory 打开的情况一样。） 其实根据 libcontainerd 推荐的 kernel 4.3 以上版本，打开 cgroup kernel memory 应该也是没问题的。可惜我们使用的kernel版本是3.10.0-514.16.1.el7.x86_64，kernel memory在这个版本是不稳定的。因此让我们踩了这个坑。 经验证，CentOS 7.4（3.10.0-693.11.1.el7）存在同样的问题。 4.寻根问底附上同事对该问题在 kernel 中追踪的结果。 memcg是Linux内核中用于管理cgroup中kernel 内存的模块，整个生命周期应该是跟随cgroup的，但是当在3.10.0-514.10.2.el7版本中，一旦开启了kmem_limit就可能会导致不能彻底删除memcg和对应的cssid。 在创建过程中，如果我们设定了kmem_limit就会激活memcg memcg和cssid的释放的调用链很长，其中在这个过程中，mem_cgroup_css_free的过程中会调用kmem_cgroup_destroy 而kmem_cgroup_destroy函数会检查memcg是否还占用memory，如果发现还占用，就不会调用mem_cgroup_put函数： 而mem_cgroup_put函数会将memcg的refcnt减去1，并查看是否等于0，如果为0即表示没有其他东西引用memcg，memcg可以放心释放 可惜当开启了kmem_limit后，这个refcnt不会等于0，导致永远无法调用到free_rcu去释放cssid。 5.解决问题经过以上分析，造成该故障的原因，是由于 kubelet 1.9 激活了 cgroup kernel-memory 特性，而在 CentOS 7.3 kernel 3.10.0-514.10.1.el7.x86_64 中对该特性支持不好。 导致删除容器后，仍有对 cgroup memory 的占用，没有执行 free_css_id() 函数的操作。 解决方法，就是修改 k8s 1.9 代码，再次禁止设置 cgroup kernel-memory 配置，保持关闭状态。 也可以将 kernel 升至 4.3 以上版本。 6.疑问1）为什么同样是 k8s 1.9 的版本，在不同的测试中没有问题 原因是第一次的 k8s 1.9 是在原来 k8s 1.6 的环境中，通过升级 kubelet 版本来测试的。而 /sys/fs/cgroup/kubepods 是由 k8s 1.6 创建的，升级到 k8s 1.9 后，启动服务时，判断到该路径已经存在，就没有再创建新的。 所以，也就没有激活 cgroup kernel-memory 特性。 接下来创建的 POD 会继承该路径的 cgroup memory 属性，也没有激活 cgroup kernel-memoy ，所以没有引发问题。 相反，在重启 k8s 1.9 node 后，kubelet 新建了 /sys/fs/cgroup/kubepods ，激活了 cgroup kernel-memoy ，导致后续的 POD 在删除时也有问题。 2）cgroup kernel memory 激活后可以关闭吗？ 按 kernel 的说明，以及 kernel 代码，激活的方式，就是传递非 -1 的值（0 也是激活）来激活。而且最小单位是 4096 PageSize 的大小。 在激活 cgroup kernel memory 后，是不能关闭的，只能通过设置 -1 关闭限制，但还是会继续计数。 After a group is first limited, it will be kept being accounted until it is removed. The memory limitation itself, can of course be removed by writing -1 to memory.kmem.limit_in_bytes. In this case, kmem will be accounted, but not limited. 另外，在 CentOS 7.3 kernel 中，激活 cgroup kernel memory 后，即使通过 -1 设置不限制使用，但还是会频繁出现 SLUB 的报警，以及 Java crash 的问题。 Mar 26 18:36:59 ns-k8s-noah-s0054 kernel: SLUB: Unable to allocate memory on node -1 (gfp=0x8020) Mar 26 18:36:59 ns-k8s-noah-s0054 kernel: cache: ip6_dst_cache(1995:6b6bc0c9f30123084a409d89a300b017d26ee5e2c3ac8a02c295c378f3dbfa5f), object size: 448, buffer size: 448, default order: 2, min order: 0 3）判断 cgroup kernel memory 是否激活的方式 查看对应 POD container 下的 memory.kmem.slabinfo。 开启： # cat memory.kmem.slabinfo cat: memory.kmem.slabinfo: Input/output error 关闭： # cat memory.kmem.slabinfo slabinfo - version: 2.1 # name &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt; 需要注意的是，如果给 cgroup memory.kmem.limit_in_bytes 设置 -1 ，其结果为： # cat memory.kmem.limit_in_bytes 9223372036854771712 这个值在关闭时和不限制 kernel memory 时是一样的，不能作为 kernel memory 是否激活的判断条件。 参考资料/usr/share/doc/kernel-doc-3.10.0/Documentation/cgroups/memory.txt application crash due to k8s 1.9.x open the kernel memory accounting by default #61937 Docker leaking cgroups causing no space left on device? #29638","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://yoursite.com/tags/Kubernetes/"}]},{"title":"性能调优概述","date":"2018-04-01T14:40:48.000Z","path":"2018/04/01/performance-tuning-overview.html","text":"什么是性能调优？(what) 在说什么是性能调优之前我们先来说一下，计算机的体系结构。 如上图，简单来说包括三块：硬件、操作系统、应用程序。其实，性能调优就是调节这些内容，包括硬件、操作系统、应用程序。其中，这三大方面中又包含了若干的内容。 硬件包括：CPU、内存、磁盘、网卡、其它……， 操作系统包括：进程、虚拟内存、文件系统、网络、其它……， 应用程序我就不用说了大家都懂，常见的有Apache、MySQL、Nginx、Memcahed等。 那什么是性能调优呢？ 性能调优就是对计算机硬件、操作系统和应用有相当深入的了解，调节三者之间的关系，实现整个系统（包括硬件、操作系统、应用）的性能最大化，并能不断的满足现有的业务需求。 为什么需要性能调优？(why)下面我们来说一说为什么需要性能调优，其实说到底就两原因：一是为了获得更好的系统性能（就是你现有的系统运行的还不错，但优化一下可以运行的更好）。二是通过性能调优来满足不断增加的业务需求。为了更直观的帮助大家来理解为什么要性能调优？我们分别从三个方面来说： 硬件选型（根据服务器应用类型来选购服务器） 操作系统发行版本 （选择发行版本） 应用程序 （Nginx、MySQL等） 1.硬件选型不管你是租服务器也好还是自己买服务器也好都要遇到一个问题，我们选择什么样硬件配置的服务器。 一般我们是根据应用类型来选择服务器，因为你不可能一种硬件配置来满足所有的应用需求，因为每个应用的具体需求不一样。下面我们来看一下在项目实施中有哪些应用类型： 负载均衡：性能要求相对较低，因为只负责转发数据，但要保证选一性能突出的网卡即可。（推荐配置：CPU E5620 x １ 内存 ８G 硬盘 500G（RAID5）) web 服务器：一般只处理一些静态页面或者图片等，因此要求也不是很高，主流的服务器都可以。（推荐配置：CPU E5620 x １ 内存 16G 硬盘 500G（RAID5）） 应用服务器：一般应用程序服器，他承担网站功能的实现，在架构中占有比较重的位置，特别是网站架构中只有一台应用服务器，对CPU、内存、磁盘要求都比较高。（推荐配置：CPU E5620 x 2 内存 32G 硬盘 500G（RAID10）） 缓存服务器：分为前端页面缓存与后面数据缓存，他们典型的应用分别是Varnish与Memcached，对内存的要求比较大，一般我们配置服务器时使用较大有内存。（推荐配置：CPU E5620 x 1 内存 32G 硬盘 500G（RAID10）） 数据库服务器：数据服务器对CPU、内存、磁盘的要求都很高，一但某个硬件是短板都会带来性能问题。（推荐配置：CPU E5620 x 2 内存 64G 固态硬盘 500G（RAID10）） 备份服务器：备份服务器一般就没有什么要求，但有点可以肯定是必须有足够大的硬盘空间。（推荐配置：CPU E5620 x 1 内存 4G 硬盘 2TB（RAID5）） 监控服务器：一般也没什么需要，普通的PC服务器就可以。（推荐配置：CPU E5620 x 1 内存 4G 硬盘 500（RAID5）） 其它服务器：至于其它服务器就看各位的具体需要具体分析了。 这下各位知道什么是硬件的性能调优了吧，根据你具体的应用，进行具体分析特别是像MySQL这样的服务器，对CPU、内存、磁盘要求都比较高。 所以，对硬件的性能调优我们必须做到选择合适的硬件配置。这是网站架架构或者项目实施首先要解决的问题！ 2.操作系统有本书叫《Linux Performance Tuning》（Linux 性能调优）这本书是老外写的，作者是 Fernando Apesteguia 。 为什么我们需要性能调优？他得出的结论是这样的: “当一个发行版打包发送到客户手中的时候，它是为了完全兼容市场中大部分计算机而设计的。这是一个相当混杂的硬件集合（硬盘，显卡，网卡，等等）。 所以Red Hat，SUSE，Ubuntu和其他的一些发行版厂商选择了一些保守的设置来确保安装成功。” 简单说，你的操作系统已经运行的不错了，但是你可以调节它获得更高的性能，比如你有个高性能的磁盘，但你的操作系统中一些选项参数默认没有启动，就不能实现这些高级功能来提高硬盘性能。 还有我想说就是对操作系统发行版选择的问题，RedHat或CentOS这些操作系统在项目实施或网站架构中用的比较多，主要针对企业应用而开发的操作系统。 而Ubuntu之类的操作系统对桌面支持的比较好，所以选择发行版本时得注意。（一般企业中用的比较多的是CentOS）再有就是我们一般不要选择最新的发行版，因为刚出来的发行版相对来说bug还比较多，不要先当“小白鼠”了， 比如：刚刚出来CentOS 7 等过一段时间稳定了再使用，目前我们可以选择 CentOS 6.4 或 6.5即可。 （但新版本也有很多好处，新版本中加入了很多新功能，去掉习已知bug，对于一些不重要的应用，可尝试使用新的操作系统） 3.应用程序最后，我们得来说说应用程序了，我们先来简单看到一下Apache的MPM配置文件: prefork 模型： &lt;IfModule prefork.c&gt; StartServers 8 MinSpareServers 5 MaxSpareServers 20 ServerLimit 256 MaxClients 256 MaxRequestsPerChild 4000 &lt;/IfModule&gt; 大家可以从上面的配置文件中可以看出，apache 开始启动时启用 8个进程，最小 5个进程，最大20个进程，每个进程限制请求数为256个，最多可以接受请求 4000个，超过这个限制数自动销毁。 worker 模型： &lt;IfModule worker.c&gt; StartServers 2 MaxClients 150 MinSpareThreads 25 MaxSpareThreads 75 ThreadsPerChild 25 MaxRequestsPerChild 0 &lt;/IfModule&gt; 在看一下，worker模型的配置文件，默认启动2个进程，每个进程可以接受的请求为150个，每个进程中最小线程数25个，最大线程数为75个，默认线程数25个，每个线程可以接受的请求没有限制为0。 好了，大家看完上面的配置文件，可以看出默认的Apache配置文件，设置的比较保守，只适于一些中小网站，想要获得高性能的Apache服务器还必须进行性能调优，包括apache编译选项，配置文件优化等，具体的调优我们在这里先不细说。 通过我们上面的讲解，我们分别从硬件、操作系统、应用程序，这三个方面入手和大家谈谈为何需要性能调优，相信大家已经知道并了解，相信大家都迫不及待了吧。 嘿嘿，我们先不急还有很多问题没有说清楚，下面我们和大家来说说，什么时候需要性能调优？ 什么时候需要性能调优？(when)一般分为两个时间段： 上线前（基本优化） 上线后（持续优化） 为什么这样说呢，一般我们在项目实施到项目上线这段时间，不但要准备硬件服务器、安装操作系统、环境搭建，还有个很重要的问题就是进行性能优化，包括操作系统优化和应用环境优化等，我称上线前的优化为基本优化也称为经验优化。 根据你做过的项目和你工作中的经验对上线前的服务器或架构进行基本的性能优化来满足业务需求。 再有就是项目上线后的优化，在上线前我们已经经过基本的性能优化，解决大部分的性能问题，但毕竟上线前的所以测试都是模拟测试并进行相关的性能优化，与上线后的真实环境还是有相当大的区. 我们首先要做的就是对上线后的项目进行性能监控包括服务器性能监控和服务性能监控。 服务器性能监控包括： CPU使用率、CPU负载、内存使用率、磁盘I/O、磁盘空间使用率、网络流量、系统进程等，服务性能监控包括apache、nginx、mysql。 以上架构中所有的服务都需要进行性能监控，一但发现有问题我们都得去进行性能优化，在这个过程中我称为持续优化也称为监控优化。下面我们来具体的说一下，具体什么地方需要性能调优？ 什么地方需要性能调优？(where)在上面我们说性能调优只说一些大的方面，包括硬件、操作系统、应用程序这三大块。 其实还有一块就是程序本身的优化，开发人员根据需求开发出来的程序本身就需要性能优化，但对于我们运维人员来说接触的比较少而已。下面我们就来看看这三大块： 硬件 （CPU、内存、磁盘、网卡） 操作系统（进程、文件系统、内核 ……） 应用程序（Nginx、MySQL ……） 1.硬件硬件优化一般也包括两块： 上线前（硬件选型） 上线后（硬件扩展） 一般项目搭建时都需要根据具体的应用进行硬件配置选型，在这方面需要一定的运维经验刚接触的朋友可以在这方面有点欠缺，但没事一般做过一两个项目以后，对硬件配置选型也就会了。 但有个不成文的经验，硬件配置还是越高越好。 我们为什么说需要根据具体的应用来选型呢，一方是什么样的应用需要什么样的硬件配置，还有点很重要就是节约成本，钱得要在刀刃上不该花的钱我们不能乱花，也是为公司节约成本，实现资源利用最大化。 上面我们说的是项目搭建初期，你运气比较好项目一开始你就在这边。 一般有经验的运维工程师在硬件选型是不会有问题的，所以我们在性能优化时就不考虑硬件这块，从理论上讲我们服务器硬件配置一般不会出现在这种性能问题上。 但是呢，由于我们业务做的越来越好，项目创建初期没有考虑到会有这么大的性能需要（访问量），现在有的硬件不能满足业务需求，所我们这时需要更换更好的CPU、更大的内存和更快的磁盘。 至于如何找出硬件是性能瓶颈我们先在这里不细说，在后面的文章中我们将会细说。最后我们来看一张硬件架构图，能帮你更好的理解硬件优化，如下图（Dell R 710 架构）： 2.操作系统下面我们来说操作系统，其实绝大部分的优化都在操作系统和应用程序的优化，除了上线前的硬件选型和上线后的硬件扩展，下面我们就来看看操作系统优化包括哪些： 操作系统安装优化 系统初始化 进程调优 内存调优 IO 调优 文件系统调优 网络调化 下面我们来看一张图，可以更直观的帮且我们理解，如下图： 3.应用程序最后我们来说说应用程序优化，这里我们来说一下MySQL优化例子，让大家更直观的了解。 MySQL 编译安装优化 MySQL 配置文件优化 索引优化 MySQL 引擎优化 查询缓存优化 SQL 语句优化 优化表类型（MyISAM或InnoDB） 锁机制优化 MySQL 服务器优化（换SSD） 通达上面的对硬件、操作系统、应用程序的具体说明相信，大家对性能优化有了更深层次的了解，下面我们来说一个重要的问题，什么人来进行性能优化？ 什么人来进行性能调优？(who)一说起性能优化我们第一个想到的是运维工程师，他们来进行优化。 其实我想说，这么说是片面的性能优化不仅仅是运维工程师的事。 其实呢，性能优化是一个团队的事。我为什么这么说呢？ 下面我们就来说一下，大家想啊一公司需要做一项目，我们就拿最常见的电子商务中商城的项目来说吧，公司确认由于业务需要我们需要在网上做一个建材商城，那项目的具体流程是什么呢？可能不是很详细，但大体过程是样的： 运营提出需求 产品整理需求 开发开发具体的业务应用 运维搭建开发环境 QA 进行项目测试 运维进行项目上线 监控进行项目监控 开发一个具体的应用需要运营部、产品部、开发部、运维部、QA （测试）、监控等所以有部门的参加。 同样的一个项目（业务）存在性能问题，不会只是运维部门需要性能调优而是所以部门一起解决这个性能问题，这是缺一不可的。 可能出现在产品，也可能出现在程序上（*.php）,也可能是业务需要本身就有问题，也可能是运维的环境搭建有问题。但参加性能调优的更多的是开发、运维、测试和监控。 怎么样进行性能调优？(How)下面进入正题了我们说一说怎么进行性能调优，具体步骤如下： 性能指标 –&gt; 确认衡量标准 性能测试 –&gt; 验证性能指标 性能分析 –&gt; 找出性能瓶颈 性能调优 –&gt; 解决性能问题 性能监控 –&gt; 检验调优效果 1.性能指标上面我们说了，我们优化的目的是为了获得更好的性能，那么性能指标是什么呢？我们怎么样来衡量，一般衡量一个项目（这里指的网站）的指标有三个： 吞吐量 –&gt; 是单位时间内完成的用户或系统的请求数量。 并发数 –&gt; 同时能接受多少用户的访问请求 响应时间 –&gt; 用户发出请求到收到响应的时间间隔。 2.性能测试我们做产品或者说项目（更直白的说是网站）目的是为了让用户使用，我们得先站在用户的角度分析一下，用户需要关注哪些性能。 对于用户来说，当点击一个按钮、链接或发出一个操作指令，到系统把请求处理好发给用户并用网页的形式展现出来为止，这个过程中所消耗的时间是用户对这个网站性能的直观印象。 也就是我们所说的响应时间，当响应时间较小时，用户体验相对来说就会好，当然用户体验的响应时间包括个人主观因素和客观响应时间。 在网站开发与搭建时，我们就需要考虑到如何更好地结合这两部分达到用户最佳的体验。用户关注的是用户操作的相应时间。 其次，我们站在运维的角度考虑需要关注的性能点。再次，我们得站在开发（设计）人员角度去考虑网站性能。最后，由QA测试与反馈我们网站性能。 经过上述的说明，我们来测试系统的性能，需要我们收集系统的吞吐量、并发数、响应时间这三个重要的指标。具体步骤是： 确认吞吐量、并发数、响应时间这三个值 找到或开发相应的性能测试工具 进行性能测试 反馈结果并提交测试报告 结果，有两个一种是达到我们预期的性能目标，这样我们就不需要性能优化任务完成可以交给运维上线，只需要进行相关的性能监控，方便上线后进行性能优化。 另一种是没有达到我们预期的目标，我们要查找性能瓶颈并进行性能优化。 3.性能分析通过上面的性能测试，我们发现网站没有达到我们预期定义的性能目标，这时我们需要做的就是对现有的系统（服务器）进行监控，包括硬件与软件监控，为性能调优提供有效的性能监控数据。 下面我们重点来说一下，用什么工具能找出性能瓶颈： 硬件： 用vmstat、sar、iostat检测是否是CPU瓶颈 用free、vmstat检测是否是内存瓶颈 用iostat检测是否是磁盘I/O瓶颈 用netstat检测是否是网络带宽瓶 操作系统： 进程 文件系统 SWAP 分区 内核参数调整 应用程序（MySQL等）： mysqlreport 性能分析报告 mysqlsla 慢查询日志分析 4.性能调优 确定调优目标 具体调优步骤 检测调优结果 确定调优目标 我们性能优化的目标是网站性能提高10%还是20%，不能老大说今天你给我优化一下网站性能，你就能使用网站性能翻一倍。 首先，你要问他我们需要达到一个怎么的目标。 然后，我们要了解一下整个环境（架构）包括代码（当然你需要了解一下业务逻辑，大致了解一下，肯定没坏处），有时间多和开发沟通一下，问问代码中有多少坑要填，这很重要。 往往他们优一下代码中的SQL查询，比你优化系统多少天都来的有效果，哈哈。 具体调优步骤 如果你不懂系统的参数，你千万不要对系统的参数进行随意的改动，不然你会后悔。 每次只对一种系统资源进行系统调试，如CPU、或内存、磁盘。 每次改动尽量少的参数设置，推荐每次修改一个设置。 分析一项系统资源时，使用多种工具，往往有意想不到的结果。 不及胜于过之（宁愿少做一点，不要做过头了，性能已达到要求就不要随意乱动，做好你的监控）。 检测调优结果 每次性能调优后必须对性能进程检测，如Web服务器的ab工具，就是一个很好的检测工具，每次调优后都能看到具体的变化。 5.性能监控 性能监控这个很重要，具体包括服务器性能监控和具体服务的性能监控。下面我们说一说具体有哪些性能监控指标： 服务器的性能监控 CPU 使用率 CPU负载 内存使用率 磁盘I/O 网络流量 磁盘空间 系统进程 服务的性能监控（MySQL） MySQL查询吞吐率，包括Change DB、Select、Insert、Update、Delete MySQL持久连接利用率 MySQL查询缓存空间使用率 MySQL查询缓存命中率 MySQL缓存查询数 MySQL索引缓存命中率 MySQL索引读取统计 MySQL连接吞吐率 MySQL连接缓存命中率 MySQL并发连接数，包括最大允许连接数、实际最大连接数、当前连接数、活跃连接数、缓存连接数 MySQL流量统计 MySQL表统计锁定","tags":[{"name":"性能调优","slug":"性能调优","permalink":"http://yoursite.com/tags/性能调优/"}]},{"title":"使用Istio简化微服务系列三：如何才能做“金丝雀部署”，并通过Istio增加流量？【转载】","date":"2018-03-31T04:06:27.000Z","path":"2018/03/31/simplifying-microservices-with-istio-in-kubernetes-3.html","text":"本系列的第二部分中（使用Istio简化微服务系列二：如何通过HTTPS与外部服务进行通信？），我们学会了使用 Istio egress rules 来控制服务网格之外的服务的访问。 在这一部分中，我们将看到如何才能做“金丝雀部署”（Canary Deployments），并通过 Istio 增加流量。 背景：在过去的文章中，我详细解释了我们如何使用 Kubernetes 进行蓝/绿部署。这是一种部署技术，在此技术中，我们部署了与应用程序当前版本和新版本相同的生产环境。这项技术使我们能够执行零停机时间部署（Zero Downtime Deployments， 简称：ZDD），以确保我们的用户在切换到新版本时不受影响。有了这两个版本（当前版本和新版本）也使我们能够在新版本出现任何问题时进行回滚。 我们还需要的是能够将流量增加(或下降)到我们的应用程序的新版本，并监控它以确保没有负面影响。实现这一点的一种方法是使用“金丝雀部署”或“金丝雀”发行版。 一个不太有趣的事实：当矿工们带着金丝雀进入矿场时，任何有毒气体都会首先杀死金丝雀，并以此警告矿工们离开矿井。 同样地，在应用程序部署世界中，使用“金丝雀部署”，我们可以将应用程序的新版本部署到生产中，并只向这个新部署发送一小部分流量。这个新版本将与当前版本并行运行，并在我们将所有流量切换到新版本之前提醒我们注意任何问题。 例如：我们应用程序的 v1 可以占到90%的流量，而 v2 可以得到另外的10%。如果一切看起来都很好，我们可以将 v2 流量增加到25%，50%，最终达到100%。Istio Canary 部署的另一个优势是，我们可以根据请求中的自定义标头来增加流量。例如，在我们的应用程序的v2中设置了一个特定 cookie 头值的流量的10%。 注意：虽然“金丝雀部署”“可以”与 A/B 测试一起使用，以查看用户如何从业务度量的角度对新版本做出反应，但其背后的真正动机是确保应用程序从功能角度上满意地执行。此外，企业所有者可能希望在更长的时间内(例如:许多天甚至几周)进行 A/B 测试，而不是金丝雀码头可能采取的措施。因此，把它们分开是明智的。 Let’s see it in action从第一部分我们知道，我们的 PetService 与 PetDetailsService(v1) 和 PetMedicalHistoryService(v1) 进行了会谈。对 PetService的调用的输出如下: $ curl http://108.59.82.93/pet/123 { &quot;petDetails&quot;: { &quot;petName&quot;: &quot;Maximus&quot;, &quot;petAge&quot;: 5, &quot;petOwner&quot;: &quot;Nithin Mallya&quot;, &quot;petBreed&quot;: &quot;Dog&quot; }, &quot;petMedicalHistory&quot;: { &quot;vaccinationList&quot;: [ &quot;Bordetella, Leptospirosis, Rabies, Lyme Disease&quot; ] } } 在上面的回复中，你会注意到 petBreed 说“狗”。然而，Maximus 是德国牧羊犬，这时候我们需要修改 PetDetailsService，以便正确返回品种。 因此，我们创建了 PetDetailsService 的 v2，它将返回“德国牧羊犬”。但是，我们希望确保在将所有流量驱动到 v2 之前，我们可以使用一小部分用户来测试这个服务的 v2。 在下面的图1中，我们看到流量被配置成这样，50%的请求将被定向到 v1 和50%到 v2，我们的 “金丝雀部署”。(它可以是任意数字，取决于变化的大小，并尽量减少负面影响)。 步骤1、创建 PetDetails Service 的 v2 版本并像以前一样部署它。 （请参阅 petdetailservice / kube 文件夹下的 petinfo.yaml） $ kubectl get pods NAME READY STATUS RESTARTS AGE petdetailsservice-v1-2831216563-qnl10 2/2 Running 0 19h petdetailsservice-v2-2943472296-nhdxt 2/2 Running 0 2h petmedicalhistoryservice-v1-28468096-hd7ld 2/2 Running 0 19h petservice-v1-1652684438-3l112 2/2 Running 0 19h 2、创建一个RouteRule，将流量分成 petdetailsservice 的50％（v1）和50％（v2），如下所示： cat &lt;&lt;EOF | istioctl create -f - apiVersion: config.istio.io/v1alpha2 kind: RouteRule metadata: name: petdetailsservice-default spec: destination: name: petdetailsservice route: - labels: version: v1 weight: 50 - labels: version: v2 weight: 50 EOF $ istioctl get routerule NAME KIND NAMESPACE petdetailsservice-default RouteRule.v1alpha2.config.istio.io default 3、现在，如果你访问 PetService，你应该看到替代请求分别返回“Dog”和“German Shepherd Dog”，如下所示： $ curl http://108.59.82.93/pet/123 { &quot;petDetails&quot;: { &quot;petName&quot;: &quot;Maximus&quot;, &quot;petAge&quot;: 5, &quot;petOwner&quot;: &quot;Nithin Mallya&quot;, &quot;petBreed&quot;: &quot;Dog&quot; }, &quot;petMedicalHistory&quot;: { &quot;vaccinationList&quot;: [ &quot;Bordetella, Leptospirosis, Rabies, Lyme Disease&quot; ] } } $ curl http://108.59.82.93/pet/123 { &quot;petDetails&quot;: { &quot;petName&quot;: &quot;Maximus&quot;, &quot;petAge&quot;: 5, &quot;petOwner&quot;: &quot;Nithin Mallya&quot;, &quot;petBreed&quot;: &quot;German Shepherd Dog&quot; }, &quot;petMedicalHistory&quot;: { &quot;vaccinationList&quot;: [ &quot;Bordetella, Leptospirosis, Rabies, Lyme Disease&quot; ] } } It works！ 这引出了一个问题：我们不能用 Kubernetes Canary Deployments 来做到这一点吗？简短的答案是肯定的。 然而，这样做的步骤更复杂，也有局限性: 你仍然需要创建 PetDetailsService (v1和v2)的两个部署，但是你需要手动限制在部署过程中 v2 副本的数量，以维护 v1:v2 比。例如:你可以使用10个副本部署 v1，并将v2 部署到2个副本，以获得10:2的负载平衡，等等。 由于所有的 pod 无论版本是否相同，Kubernetes 集群中的流量负载平衡仍然受到随机性的影响。 基于业务量的自动伸缩 pods 也是有问题的，因为我们需要单独的 autoscale 的2个部署，它可以根据每个服务的流量负载分配来做出不同的行为。 如果我们想根据某些标准（如 request headers）仅允许某些用户使用/限制流量，则Kubernetes Canary Deployments可能无法实现此目标。 结论你刚刚看到了创建一个“金丝雀部署”和增加 Istio 的流量是多么容易。 参考链接： DevOxx Istio presentation by Ray Tsang: https://www.youtube.com/watch?v=AGztKw580yQ&amp;t=231s Github link to this example: https://github.com/nmallya/istiodemo","tags":[{"name":"Istio简化微服务系列","slug":"Istio简化微服务系列","permalink":"http://yoursite.com/tags/Istio简化微服务系列/"}]},{"title":"使用Istio简化微服务系列二：如何通过HTTPS与外部服务进行通信？【转载】","date":"2018-03-31T03:50:47.000Z","path":"2018/03/31/simplifying-microservices-with-istio-in-kubernetes-2.html","text":"在本系列第一部分（使用Istio简化微服务系列一：如何用Isito解决Spring Cloud Netflix部署微服务），我们展示了如何使用 Istio 简化微服务间的通信。 在这一部分中，我们将看到 Istio 服务网格中的服务是如何通过HTTPS与外部服务进行通信的。 在上面的图1，PetService（直接与 PetDetailsService 和 PetMedicalHistoryService 服务进行通信）现在将也会调用外部的服务，外部服务位于 https://thedogapi.co.uk，返回 dog 图片的 url 地址。 Istio 服务网格与外部的服务通信展示在下面的图 2 所示。 与往常一样，在服务网格内的服务通信通过HTTP协议代理 与使用 HTTPS 协议的外部服务通信，内部服务仍然是发送 HTTP 请求，HTTP 请求会被以边缘（sidecar）方式部署的代理所拦截，并作为 TLS 协议的发起方与外部的服务在加密通道上进行通信 Github repo(https://github.com/nmallya/istiodemo) petservice 代码如下所示： 备注：展示我们如何调用 DogAPI https，地址http://api.thedogapi.co.uk:443/v2/dog.php 让我们看一下当运行以下命令时会发生什么？其中 108.59.82.93 为 入口（Ingress）IP 地址 （参见 第一部分） curl http://108.59.82.93/pet/123 响应内容如下： { &quot;petDetails&quot;: { &quot;petName&quot;: &quot;Maximus&quot;, &quot;petAge&quot;: 5, &quot;petOwner&quot;: &quot;Nithin Mallya&quot;, &quot;petBreed&quot;: &quot;Dog&quot; }, &quot;petMedicalHistory&quot;: { &quot;vaccinationList&quot;: [ &quot;Bordetella, Leptospirosis, Rabies, Lyme Disease&quot; ] }, &quot;dogAPIResponse&quot;: { &quot;message&quot;: &quot;request to https://api.thedogapi.co.uk/v2/dog.php failed, reason: read ECONNRESET&quot;, &quot;type&quot;: &quot;system&quot;, &quot;errno&quot;: &quot;ECONNRESET&quot;, &quot;code&quot;: &quot;ECONNRESET&quot; } } 你会注意到当 petservice 访问位于 https://api.thedogapi.co.uk 的外部服务时，上述的响应内容中 dogAPIResponse（不是最原始的名字）部分有一个错误信息。 这是因为所有外部的流量（egress）在默认情况下被阻止。在前面的文章中解释过边车（sidecar）代理只允许集群内的通信。 备注：正如我在第一部分提及的那样，当我们想要管控服务与外部服务通信的方式和阻止任何未经授权的访问的时候，这个限制非常有用。 医疗保健/金融系统可以特别利用此功能来保护 PHI / PII 数据不会被无意地甚至恶意地从内部服务中共享。 为了启用出口（egress）流量，你需要创建如下所示的出口规则（egress rule）： cat &lt;&lt;EOF | istioctl create -f - apiVersion: config.istio.io/v1alpha2 kind: EgressRule metadata: name: dogapi-egress-rule spec: destination: service: api.thedogapi.co.uk ports: - port: 443 protocol: https EOF 为了检查该出口规则（egress rule）是否已被创建，你可以运行下面的命令，你应该看到出口规则（egress rule）dogapi-egress-rule 已经被创建。 kubectl get egressrule NAME AGE dogapi-egress-rule 5m 我们再次运行上面的 curl 命令： $ curl http://108.59.82.93/pet/123 { &quot;petDetails&quot;: { &quot;petName&quot;: &quot;Maximus&quot;, &quot;petAge&quot;: 5, &quot;petOwner&quot;: &quot;Nithin Mallya&quot;, &quot;petBreed&quot;: &quot;Dog&quot; }, &quot;petMedicalHistory&quot;: { &quot;vaccinationList&quot;: [ &quot;Bordetella, Leptospirosis, Rabies, Lyme Disease&quot; ] }, &quot;dogAPIResponse&quot;: { &quot;count&quot;: 1, &quot;api_version&quot;: &quot;v2&quot;, &quot;error&quot;: null, &quot;data&quot;: [ { &quot;id&quot;: &quot;rCaz-LNuzCC&quot;, &quot;url&quot;: &quot;https://i.thedogapi.co.uk/rCaz-LNuzCC.jpg&quot;, &quot;time&quot;: &quot;2017-08-30T21:43:03.0&quot;, &quot;format&quot;: &quot;jpg&quot;, &quot;verified&quot;: &quot;1&quot;, &quot;checked&quot;: &quot;1&quot; } ], &quot;response_code&quot;: 200 } } 结论：我们看到如何通过创建明确规则，来启用从服务网格（service mesh ）到外部服务的通信。 在后续的文章中，我们将会看到如何实施其他重要的任务比如流量路由（traffic routing）和流量变化（ramping），使用断路器（circuit breaker）等。 参考链接： Istio 官方地址 https://istio.io/ DevOxx 的RayTsang的Istio演讲材料: https://www.youtube.com/watch?v=AGztKw580yQ&amp;t=231s 案例的Github: https://github.com/nmallya/istiodemo Kubernetes: https://kubernetes.io/ DogAPI 网址: https://thedogapi.co.uk/ 原文链接：https://medium.com/google-cloud/simplifying-microservices-with-istio-in-google-kubernetes-engine-part-ii-7461b1833089","tags":[{"name":"Istio简化微服务系列","slug":"Istio简化微服务系列","permalink":"http://yoursite.com/tags/Istio简化微服务系列/"}]},{"title":"使用Istio简化微服务系列一：如何用Isito解决Spring Cloud Netflix部署微服务的挑战?【转载】","date":"2018-03-31T02:57:27.000Z","path":"2018/03/31/simplifying-microservices-with-istio-in-kubernetes-1.html","text":"概述Istio 简化了服务间的通信，流量涨落，容错，性能监控，跟踪等太多太多。如何利用它来帮我们从各微服务中抽象萃取出基础架构和功能切面？ 我写的这些关于 Istio 的文章是 Istio官网文档的子集。读官网文档可了解更多。 注意:：如果你很熟悉微服务，请跳过背景介绍这段。 在本系列的第一部分，将涵盖如下内容： 背景： 单体应用及微服务介绍 Spring Cloud Netflix Stack及其优势 Istio 介绍 Istio的服务-服务通信举例 背景过去，我们运维着“能做一切”的大型单体应用程序。 这是一种将产品推向市场的很好的方式，因为刚开始我们也只需要让我们的第一个应用上线。而且我们总是可以回头再来改进它的。部署一个大应用总是比构建和部署多个小块要容易。 然而，这样的应用开发将导致“爆炸式的”工作量（我们经过数月的工作后将再次部署整个应用），并且增量变更将因为构建/测试/部署/发布周期等的复杂特性而来来回回折腾很长时间。但是，如果你是产品负责人，尤其是在部署一个新版本后发现一个严重的 Bug，那么这就不仅仅是多少钱的问题。 这甚至可能导致整个应用回滚。相对于比较小的组件来说，将这样的一个大型应用部署到云上并弹性扩展它们也并不容易。 进入微服务微服务是运行在自己的进程中的可独立部署的服务套件。 他们通常使用 HTTP 资源进行通信，每个服务通常负责整个应用中的某一个单一的领域。 在流行的电子商务目录例子中，你可以有一个商品条目服务，一个审核服务和一个评价服务，每个都只专注一个领域。 用这种方法来帮助分布式团队各自贡献各种服务，而不需要在每个服务变更时去构建/测试/部署整个应用，而且调试也无需进入彼此的代码。 将服务部署到云上也更容易，因为独立的服务就能按需进行自动弹性扩展。 用这种方法让多语言服务（使用不同语言编写的服务）也成为可能，这样我们就可以让 Java/C++ 服务执行更多的计算密集型工作，让 Rails / Node.js 服务更多来支持前端应用等等。 Spring Cloud Netflix:随着微服务的流行，简化服务的创建和管理的框架如雨后春笋。 我个人在2015年最喜欢的是 Netflix OSS 栈（Spring Cloud Netflix），它让我用一个非常简单的方式，通过 Spring Tool Suite IDE 来创建 Java 微服务。 我可以通过 Netflix 套件获得以下功能（图1）： 通过 Eureka 进行服务注册- 用于注册和发现服务 用 Ribbon 做客户端的负载均衡- 客户端可以选择将其请求发送到哪个服务器。 声明 REST 客户端 Feign 与其他服务交谈。在内部，使用 Ribbon。 API 网关用 Zuul —单一入口点来管理所有 API 调用，并按路由规则路由到微服务。 Hystrix 做熔断器 — 处理容错能力以及在短时间内关闭通信信道（断开回路）并在目标服务宕机时返回用户友好的响应。 用 Hystrix 和 Turbine 做仪表板 —— 可视化流量和熔断 当然，这种构建和部署应用的方法也带来了它的挑战。 挑战部署：怎样才能通过一种统一一致的方式将我们的服务部署到云中，并确保它们始终可用，并让它们按需进行自动弹性扩展？ 横切关注点：如何对每个微服务代码改动很少甚至不改代码的情况下能获得更多我们所看到的 Spring Cloud Netflix 中所实现的功能？ 另外，如何处理用不同语言编写的服务？ 解决方案部署：Kubernetes 已经为在 Google Kubernetes Engine（GKE）中高效部署和编排 Docker 容器铺平了道路。 Kubernetes 抽象出基础架构，并让我们通过 API 与之进行交互。 请参阅本文末尾的链接以获取更多详细信息。 横切关注点：我们可以用 Istio。 Istio 官网上的解释称：“ Istio 提供了一种简单的方法，来创建一个提供负载均衡、服务间认证、监控等的服务网络，且不需要对服务代码进行任何更改。 通过在整个环境中部署专门的 sidecar 代理服务，来拦截微服务间的所有网络通信，整个配置和管理通过 Istio的控制面板来做。” Istio介绍：换句话说，通过Istio，我们可以创建我们的微服务，并将它们与“轻量级 Sidecar 代理”一起部署（下图2），以处理我们的许多横切需求，例如： 服务到服务的通信 追踪 熔断（类 Hystrix 功能）和重试 性能监控和仪表板（类似于 Hystrix 和 Turbine 仪表板） 流量路由（例如：发送 x％ 流量到 V2 版本的应用实例），金丝雀部署 一个额外的红利（特别是如果您正在处理医疗保健中的 PHI 等敏感数据时）出站（Istio 服务网格之外的外部可调用服务）需要明确配置，并且可以阻止在服务网格之外的做特殊调用的服务。 在上图2中，我们已经去掉了图1中的许多组件，并添加了一个新组件（Envoy Proxy）。 任何服务（A）如需与另一个服务（B）交谈，则提前对它的代理做路由规则预配置，以路由到对方的代理进行通信。 代理与代理交谈。 由于所有通信都是通过代理进行的，所以很容易监控流量，收集指标，根据需要使用熔断规则等。对横切面的声明式的配置规则和策略，无需更改任何服务代码，让我们可以更专注于最重要的事情：构建高业务价值的高质量的服务。 从高的层面看，Istio 有如下组件： Envoy：一个高性能，低空间占用的代理，支持服务之间的通信，并有助于负载平衡，服务发现等； 混合器：负责整个生态（服务网格）中所有服务的访问控制策略，并收集通过 Envoy 或其他服务发送的遥测信息； Pilot：帮助发现服务，流量缓慢调整和容错（熔断，重试等）； Istio-Auth ：用于服务间认证以及都使用 TLS 的终端用户认证。本文中的示例不使用 Istio-Auth。 用Istio进行服务—服务通信：让我们在练习中了解它！ 我们将举一个简单的例子，展示3个通过 Envoy 代理进行通信的微服务。它们已经用 Node.js 写好，但如前所述，你可以用任何语言。 宠物服务：通过调用 PetDetailsService 和 PetMedicalHistoryService 来返回宠物的信息和病史。 它将在9080端口上运行。 宠物详细信息服务：返回宠物信息，如姓名，年龄，品种，拥有者等，它将在端口9081上运行。 宠物医疗历史信息服务：返回宠物的病史（疫苗接种）。 它将在9082端口运行。 步骤：在 GKE中创建一个 Kubernetes 集群（我叫 nithinistiocluster）。 确保缺省服务帐户具有以下权限：roles/container.admin（Kubernetes Engine Admin）。 按照 https://istio.io/docs/setup/kubernetes/quick-start.html 中的说明安装 istio。 现在，我们准备将我们的应用程序（上述3个服务）部署到 GKE，并将边车代理注入到部署中。 在 github 仓库中，您将看到4个目录（安装各种组件时创建的istio目录和我的微服务的3个目录）。 对于每个微服务，我在 petinfo.yaml 文件的 kube 目录中创建了相应的 Kubernetes部署和服务。 服务名为宠物服务，宠物详细信息服务和宠物医疗历史信息服务。 由于PetService可以公开访问，因此它有一个指向 petservice 的 Kubernetes Ingress。 你可以转到每个服务目录，在 deploy.sh 文件中更新项目和群集名称并运行它。 它构建服务，创建 Docker 镜像，将其上传到Google Container Registry，然后运行 istioctl 以注入 Envoy 代理。 例如，对于 PetService，它看起来像： #!/usr/bin/env bash export PROJECT=nithinistioproject export CONTAINER_VERSION=feb4v2 export IMAGE=gcr.io/$PROJECT/petservice:$CONTAINER_VERSION export BUILD_HOME=. gcloud config set project $PROJECT gcloud container clusters get-credentials nithinistiocluster --zone us-central1-a --project $PROJECT echo $IMAGE docker build -t petservice -f &quot;${PWD}/Dockerfile&quot; $BUILD_HOME echo &apos;Successfully built &apos; $IMAGE docker tag petservice $IMAGE echo &apos;Successfully tagged &apos; $IMAGE #push to google container registry gcloud docker -- push $IMAGE echo &apos;Successfully pushed to Google Container Registry &apos; $IMAGE # inject envoy proxy kubectl apply -f &lt;(istioctl kube-inject -f &quot;${PWD}/kube/petinfo.yaml&quot;) 在上面的代码中，最后一行显示了我们如何使用 Istio 命令行工具（istioctl）来将代理注入到我们的各种 Kubernetes 部署中。 Petservice 目录下的 petinfo.yaml 文件包含服务、部署和 Ingress的配置。 看起来像： apiVersion: v1 kind: Service metadata: name: petservice labels: app: petservice spec: ports: - port: 9080 name: http selector: app: petservice --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: petservice-v1 spec: replicas: 1 template: metadata: labels: app: petservice version: v1 spec: containers: - name: petservice image: gcr.io/nithinistioproject/petservice:feb4v2 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 --- ########################################################################### # Ingress resource (gateway) ########################################################################## apiVersion: extensions/v1beta1 kind: Ingress metadata: name: gateway annotations: kubernetes.io/ingress.class: &quot;istio&quot; spec: rules: - http: paths: - path: /pet/.* backend: serviceName: petservice servicePort: 9080 --- 一旦运行了 deploy.sh，就可以通过执行以下命令来检查确认部署、服务和 Ingress 是否已经创建： mallyn01$ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE petdetailsservice-v1 1 1 1 1 1h petmedicalhistoryservice-v1 1 1 1 1 58m petservice-v1 1 1 1 1 54m mallyn01$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.51.240.1 &lt;none&gt; 443/TCP 2d petdetailsservice ClusterIP 10.51.255.10 &lt;none&gt; 9081/TCP 1h petmedicalhistoryservice ClusterIP 10.51.244.19 &lt;none&gt; 9082/TCP 59m petservice ClusterIP 10.51.242.18 &lt;none&gt; 9080/TCP 1h petservice mallyn01$ kubectl get ing NAME HOSTS ADDRESS PORTS AGE gateway * 108.59.82.93 80 1h mallyn01$ kubectl get pods NAME READY STATUS RESTARTS AGE petdetailsservice-v1-5bb8c65577-jmn6r 2/2 Running 0 12h petmedicalhistoryservice-v1-5757f98898-tq5j8 2/2 Running 0 12h petservice-v1-587696b469-qttqk 2/2 Running 0 12h 当查看控制台中 pod 的信息，即使你只为每个容器部署了一项服务，但仍会注意到有2/2个容器正在运行。 另一个容器是 istioctl 命令注入的边缘代理。 5、 一旦上述所有内容都运行完毕，您可以使用 Ingress 的 IP 地址去调用示例端点来获取 Pet 的详细信息。 mallyn01$ curl http://108.59.82.93/pet/123 { &quot;petDetails&quot;: { &quot;petName&quot;: &quot;Maximus&quot;, &quot;petAge&quot;: 5, &quot;petOwner&quot;: &quot;Nithin Mallya&quot;, &quot;petBreed&quot;: &quot;Dog&quot; }, &quot;petMedicalHistory&quot;: { &quot;vaccinationList&quot;: [ &quot;Bordetella, Leptospirosis, Rabies, Lyme Disease&quot; ] } } 注意： 由于 PetService 调用 PetDetailsService 和 PetMedicalHistoryService，实际的调用将如下所示： fetch(&apos;http://petdetailsservice:9081/pet/123/details&apos;) .then(res =&gt; res.text()) .then(body =&gt; console.log(body)); ; fetch(&apos;http://petmedicalhistoryservice:9082/pet/123/medicalhistory&apos;) .then(res =&gt; res.text()) .then(body =&gt; console.log(body)); ; 结论：我们覆盖了大量内容 （但这只是第一部分！！）在随后的部分中，将详细介绍如何使用其他 Istio 特性，例如将流量逐步迁移到一个新升级的版本上，使用性能监控仪表板等等。 特别感谢 Ray Tsang 的关于 Istio 的演讲材料 。 参考链接： The Istio home page https://istio.io/ DevOxx 的Ray Tsang的 Istio 演讲材料: https://www.youtube.com/watch?v=AGztKw580yQ&amp;t=231s 案例的Github link: https://github.com/nmallya/istiodemo Kubernetes: https://kubernetes.io/ 微服务: https://martinfowler.com/articles/microservices.html Spring Cloud Netflix: https://github.com/spring-cloud/spring-cloud-netflix 原文链接：https://medium.com/google-cloud/simplifying-microservices-with-istio-in-google-kubernetes-engine-part-i-849555f922b8","tags":[{"name":"Istio简化微服务系列","slug":"Istio简化微服务系列","permalink":"http://yoursite.com/tags/Istio简化微服务系列/"}]},{"title":"微服务拆分那点事【转载】","date":"2018-03-30T05:10:48.000Z","path":"2018/03/30/Microservices-split.html","text":"背景最近参与了两个项目的开发，两个项目都有多组件，各自服务功能清晰等特点，也就是所谓的微服务，再结合以前的一些单体项目的开发经验，这里主要探讨一下我所理解的微服务和单体项目的优缺点。 我的理解其实所谓这些服务的拆分与否都是与很多因素有关系，比如：该项目的开发人员数目，该项目的运维敏感度，项目的紧急程度，开发人员的技术熟练程度，微服务架构的基础储备程度等等 因为我们的最终目的是将项目快速完整的实现好，而不是为了显得逼格高而微服务，不是为了人多每人分点活，而故意拆成微服务。总之不能为了微服务而微服务。 比如该项目总工就一个人开发，然后该项目你重启一下服务，对用户接入没啥敏感性，本来就是个没有并发的对内系统，那就完全没必要拆成微服务，就写个单体的，把接入层，数据处理层等通过模块化的代码方式拆开就行了，没必要增加复杂度，加上一些rpc，把一个单体服务拆成四五个微服务，然后增加自己的运维成本和实现成本。 如果该项目是多人协作的，有一定的并发度，对用户接入比较敏感，不能随便重启，并且开发者都对微服务有一定的经验，并且底层rpc，连接池等初始化库都有积累，然后有比较丰富的rpc多服务运维经验，那就可以拆微服务，拆完之后，服务的水平扩展一般是线性的，可以动态的根据流量扩容和缩容。 底层其他非接入层的服务，比如数据处理服务，session服务的重启与上线都不会影响整个项目的接入。同时就提高了该项目的容错性。整体项目的开发进度都能以服务为维度，各自负责一个服务，快速迭代与滚动更新上线。而不像单体应用，一般上线与迭代总是牵一发而动全身。 微服务&amp;单体应用的对比所以整体从以下几个维度，我来对比一下优缺点 微服务 单体 开发人数 较多 较少 技术复杂度 较高 一般 开发进度 分工协作较快，单组件快速迭代 一般，互相等待 功能职责 组件拆分明确 较模糊 水平扩展 很方便，直接改配置堆机器 不支持 组件重启 不影响其他组件 牵一发动全身 运维复杂度 较高 简单 适合业务场景 高并发，大流量 没并发，对内系统居多 美誉度 听着高大上 没啥波动 怎么拆分接下来根据一个具体的项目实例，看看如何将一个单体项目，拆分成微服务。 该项目是前两天刚做的一个共享积分项目 项目背景该项目主要工作两部分，一部分，矿机上报信息给服务端。第二部分：服务端根据矿机上报信息计算分配换算成工作量，然后按工作量百分比分配相应的积分。 同时该项目对接别的用户系统和boss系统等等。 下面是该项目的简单架构图： 该项目总共三人：三人都参与过微服务的开发，有基础库的储备。 时间比较紧，一周的开发周期。 一开始该项目其实因为复杂度不是太高，其实完全可以做成一个单体应用就行。 比如项目目录： src下， 一个collector目录收集矿机上报，一个finance目录处理一些金融数据。再来一个API目录处理接入请求就行了。外加一些util的公共组件。 然后一个人开发就行了。两周的时间应该能调通。 但是前面也说了，单体应用有诸多的弊端。并且主要还剩两人，你一个人用两周的时间，如果拆成微服务，三人都有相关经验，一个礼拜肯定就可以保质保量的完成相应组件的开发与测试。 因为微服务的每个组件其实都是一个独立的单体应用。组件之间的开发是没有关联的，都是依赖公共库。互相之间的调用都是通过rpc，所以开发是可以并行开发。互不干扰。效率肯定是非常高的。 所以现在我们开始简单将这个单体应用拆成微服务。 第一步：根据服务职责拆分其实微服务的拆分最根本是一些代码职责的拆分和抽象，这一步和我们模块化的时候思路是一样的。 比如该项目，矿机在不断地上报数据，然后我们通过上报的带宽给分配积分额度。这个其实细分其中的职责，我们可以看到我们需要一个收集上报数据的模块，只负责收集数据，这里抽象了各种数据来源，比如矿机的，比如其他业务接口获取的。都统一到collect模块。这个职责就很明确了。 同时这些数据收集上来以后得集中运算，算完之后得通过内部分配算法，给矿机分配积分额度。这其实是一个类似经济系统的职责，该系统只负责处理金融信息，是个finance经济系统。职责也很明确。 这两个模块对外暴露各种API接口。这个可以抽出来，单独一个接入组件，负责对外统一处理所有的API请求。所以单独起一个center组件。 这个项目相对功能不复杂，所以拆分完，也就三个组件，职责已经比较明确了。 第二步：公共库的初始化我们把公共的库都放在common里面，这里面包括了log，config，errors等基础库，还有redis，mongo，mysql等db的连接池初始化，还有rpc的连接池初始化，这里或者用grpc或者用户自己基于go自带的rpc的二次封装等。还有trace等用于追踪请求方便日志查询的基本库。 这些基础库是我们做微服务的必备，一般在一个新项目的时候，在前期需求讨论完之后，编码前期，我们会先把这些公共库的初始化工作都做了，比如db的一些连接池初始化不同项目稍微有一些不同。rpc等连接池代码基本都是能复用的。其他的公共库，直接拖到新项目里就能开搞。 第三步：组件之间接口的定义在初始化完公共库之后，我们先不着急写代码，先把组件之间的接口定义好。这里比如我们三个组件 center，collector，finance三个组件 center是接入层，这里统一处理所有的API请求。http或者https，具体API接口是业务相关。这里不做描述。接入层可以做很轻量的数据处理，不宜过重。不做有状态的数据存储。是一个无状态服务，最终的数据处理通过rpc传给后端相应的组件，基本是一个纯转发的组件。 在配置文件中已经配好了，相应的组件的rpc地址，rpc公共库中我们二次封装了基本的请求逻辑优先级，第一同机器，第二同机房，第三跨机房。 collect和finance组件都对center暴露了grpc pb接口，因为请求从center进来，会通过rpc转到相应组件做单独处理。collect系统和finance系统之间，是单向的数据流动，finance系统需要从collect系统获取数据去统一运算。所以collect系统还需要给finance系统暴露rpc接口。 具体的grpc pb接口有自己的定义语言，比较简单容易上手，在相应的目录下新建proto文件，定义完接口以后，让自动生成pb.go即可，最后代码交互都是走的pb.go里面的结构体。具体的pb编写参考文档即可。 第四步：开始分工写自己的组件了到此开始编写代码了。每个人都是相对独立的开发，因为接口定义好了，公共库也都已经初始化完毕，然后开发就是完全并行了。编写完之后，自己的组件可以依靠单元测试，做一些基本的测试。然后等联调即可。 总结以上只是一个相对比较简单的项目拆分。这里也主要说的是一个拆分的思路，和具体实现微服务的时候一些基本工程流程。如果项目比较复杂，可能拆分出来的组件数目就相对较多。本文也主要是聊聊拆分的时候一些我理解的原则，具体实现细节，其实微服务还有很多有意思的地方，比如公共库中一些db的连接池初始化和rpc的连接池初始化，配置的集中管理，动态加载等。单独抽出来都是挺有研究意义的。 本文转载自360 公司 Web 平台部运维开发团队","tags":[{"name":"微服务拆分","slug":"微服务拆分","permalink":"http://yoursite.com/tags/微服务拆分/"}]},{"title":"应用容器化和与Kubernetes适配的7条军规及最佳实践【转载】","date":"2018-03-30T01:08:53.000Z","path":"2018/03/30/Container-7-regulations.html","text":"导读本文来自于Red Hat咨询顾问Bilgin Ibryam所编写的一篇白皮书，名为《PRINCIPLES OF CONTAINER-BASED APPLICATION DESIGN》。已被Kubernetes官网转载。白皮书在Red Hat官网的下载地址：https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper 本文是对这篇文章的学习和整理。 先回顾经典的软件设计原则： 保持简单，愚蠢（KISS） 不要重复自己（DRY） 适可而止 （YAGNI） 关注点分离（SoC） 单一责任, 开放封闭, 里氏替换, 迪米特法则，接口分离, 依赖倒置（SOLID） 然后是Red Hat的云原生容器设计原则： 单一关注性原则（SCP） 高度可观测性原则（HOP） 生命周期一致性原则（LCP） 镜像不可变性原则（IIP） 进程可处置性原则（PDP） 自包含性原则（S-CP） 运行时约束性原则（RCP） 很多组织都理解云原生的重要性和必要性，但是并不知道从哪里开始。那么请确保：云原生平台和容器化应用能无缝的运行在一起，并且具备抵御故障的能力，甚至在底层的基础架构出现宕机的时候，也能通过过弹性扩展的方式表现出可靠性。本文描述了容器化应用时需要遵循的基本准则，实施这些原则有助于使之与云原生平台Kubernetes更加适配。 1、单一关注性原则SINGLE CONCERN PRINCIPLE（SCP） 在许多方面，单一关注性原则与来自SOLID的SRP是类似的，它建议一个类应该只有一个责任。SRP背后的动机是每个责任是变更的一个轴心，一个类应该有，且也只有一个需要改变的理由。SCP原则中的“关注”一词强调关注是一种更高层次的抽象的责任，而且它更好地将范围描述为一个容器而不是一个类。虽然SRP的主要动机是变化原因的唯一性，而SCP的主要动机是容器镜像重用和可替换性。如果你创建一个解决单个问题的容器，并且以功能完整的方式来实现，不同应用程序中的容器镜像重用的可能性就会更高。 因此，SCP原则规定每个集容器都应该解决一个问题，并做得很好。 实现这一点，通常比在面向对象的世界中实现SRP更容易，容器通常管理的一个单一的进程，大多数情况下一个进程解决一个问题。 如果你的容器化微服务需要解决多个问题，它可以使用这样的模式，将多个容器用sidecar和init-containers的模式合并成一个部署单元（pod），这样每个容器仍然是处理单个问题。同样，您可以替换处理同样问题的容器。 例如，将Web服务器容器或队列实现容器，更新为更具可扩展性的容器。 2、高度可观测性原则HIGH OBSERVABILITY PRINCIPLE（HOP） 容器提供了一种统一的方式来打包和运行应用程序，将它们视为一个黑盒子对象。 但任何旨在成为云原生公民的容器都必须提供API支持，要为运行时环境编写接口（API），以观察容器的健康状况和行为。 这是自动化容器更新和生命周期回收的基本先决条件和统一的方式，从而提高系统的弹性和用户体验。 实际上，您的容器化应用程序必须至少为其提供不同类型的健康检查的API–活动和就绪等状态。更好的应用程序的行为则必须提供其他手段来观察容器化应用程序的状态。应用程序应该将重要事件记录到标准错误（STDERR）和标准输出（STDOUT）中，从而通过统一的日志聚合工具（诸如Fluentd和Logstash之类的工具）进行分析，并与跟踪和指标收集库相结合，例如OpenTracing，Prometheus等。 将您的应用程序视为黑盒子，但实施所有必要的API以帮助平台对其进行观测，并以最佳方式管理您的应用程序。 3、生命周期一致性原则LIFE-CYCLE CONFORMANCE PRINCIPLE（LCP） HOP规定了你的容器提供供平台观测的API。 LCP则规定：您的应用程序有办法读取来自平台的事件。 此外，除了获得事件以外，容器还应该对这些事件相应地作出反应。这就是此原则名字由来。这几乎就像在应用程序通过一个“写入API”与平台进行交互。 来自管理平台的各种事件都是为了帮助您管理您的容器的生命周期的。决定处理哪些事件取决于您的应用程序 以及是否对这些事件做出反应。 但有些事件比其他事件更重要。例如，任何需要一个干净的关闭进程，这就需要捕获信号：终止（SIGTERM）消息，并尽可能迅速关闭。 这是为了避免通过强制关闭信号：kill（SIGKILL），之后跟随一个SIGTERM。 还有其他事件，例如PostStart和PreStop，可能对您的应用程序生命周期管理也非常重要。 例如，某些应用程序需要在服务之前进行预热请求和一些需要在关闭干净之前释放资源。 4、镜像不可变性原则IMAGE IMMUTABILITY PRINCIPLE（IIP） 容器化的应用程序是不可变更的，镜像一旦完成了构建，预计在不同的环境中运行都不会改变。这意味着在因外部环境的不同，在需要的时候需要使用外部手法处理所依赖的外部配置数据，而不是每个环境修改或者构建不同的容器。而容器应用程序中的任何变更，都应该因此触发构建新的容器镜像，并在所有环境中重用它。相同于这个原理的，不可变服务器和不可变基础架构的概念也很受欢迎，并且对于服务器/主机管理也是如此。 在遵循IIP原则的情况下，应该防止为不同的环境创建相似的容器镜像，要始终坚持为所有环境只配置一个容器镜像。 这个原则允许在应用程序更新期间，采用自动回滚和前滚等做法，这是云原生自动化的重要方面。 5、进程可处置性原则PROCESS DISPOSABILITY PRINCIPLE（PDP） 迁移到容器应用程序的主要动机之一是：容器需要尽可能做到临时性，并做好在任何时候被另一个容器实例替换的准备。需要更换容器的原因有很多，比如：健康检查失败、缩容、应用程序将容器迁移到不同的主机，平台资源匮乏或其它的问题。 这意味着容器化的应用程序必须保持其状态为向外扩展的或分布式和冗余的。这也意味着应用程序应该快速启动和关闭，甚至为彻底的硬件故障做好准备。 实施这一原则的另一个有用的做法是创建小容器。 容器在云原生环境可以自动调度并在不同的主机上启动。较小的容器可以实现更快启动时间，因为在重新启动之前容器镜像需要被物理地复制到主机系统。 6、自包含性原则SELF-CONTAINMENT PRINCIPLE（S-CP） 这个原则规定一个容器应该在构建时包含所有需要的东西。容器的存在应该仅仅依赖于Linux®内核，在并添加相关额外的库，在容器构建时加入它们。除了库之外，它还应该包含语言运行时，应用程序平台（如果需要），以及运行所需的其他依赖关系，等运行容器化应用所需要的诸如此类的东西。 唯一的例外是：由于不同环境之间差异，并且只能在运行时提供的配置; 例如，通过Kubernetes提供的ConfigMap。 某些应用程序由多个容器组件组成。 例如，容器化的Web应用程序也可能需要数据库容器。 根据这个原则，并不建议合并两个容器。相反，它建议的是数据库容器只包含运行数据库所需的所有内容，Web应用程序容器只包含运行Web应用程序所需的所有内容，如Web服务器。 在运行时，Web应用程序容器将根据需要依赖于并访问数据库容器。 7、运行时约束性原则RUNTIME CONFINEMENT PRINCIPLE（RCP） S-CP从构建时的角度查看容器，并关注于生成的二进制文件及其内容。但是容器不仅仅是磁盘上一个只有尺寸大小的单一维度的黑盒子。 容器运行时有多个维度，例如内存使用维度，CPU使用维度等资源消耗维度。 这个RCP原则建议每个容器申报资源需求，并发送信息到平台。它应该分享容器的资源配置文件，从CPU，内存，网络，磁盘的角度声明。这影响到平台如何执行调度，自动扩展，容量 管理以及容器常规的服务级别协议（SLA）等。 除了向平台声明容器的资源需求之外，还有一点也很重要， 应用被约束在使用所声明的资源需求内。如果应用程序对资源的使用保持在约束的范围内，则当资源匮乏发生时，平台不太可能将其终止和迁移。 结论云原生不仅仅是一种最终状态 – 它也是一种工作方式。 本份白皮书描述了一系列容器应用的基本原则，必须遵守才能成为优秀的云原生公民。 除了这些原则之外，创建良好的容器应用程序还需要熟悉其他容器相关的最佳实践和技术。 尽管上述原则非常根本，适用于大多数用例，下面列出的最佳实践在应用和不应用的时候，则需要判断力。以下是一些与容器相关的更常见的最佳实践： 镜像要尽可能的小。 通过清理临时文件，并避免安装不必要的软件包来构建小尺寸镜像。 这减少了容器的尺寸，构建时间和复制容器镜像的网络传输时间。 支持任意用户ID。 避免使用sudo命令或要求特定用户名运行你的容器。 标记重要的端口。 虽然可以在运行时指定端口号，然而使用EXPOSE命令在运行的时候指定，则可以让镜像的使用者更轻松。 为持久数据使用卷。 在容器摧毁之后还需要保存的容器数据的，必须将数据写入一个数据卷。 设置镜像元数据。 以标签和注释形式存在的镜像元数据可以使您的容器镜像更加实用，从而为使用您的容器的开发人员提供了更好的体验。 使主机和镜像同步。 一些容器应用需要容器在某些属性（如时间和机器ID）上与主机同步。 这里是指向各种模式和最佳实践的资源的链接，以帮助您能有效地实现上述目标： https://www.slideshare.net/luebken/container-patternshttps://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practiceshttp://docs.projectatomic.io/container-best-practiceshttps://docs.openshift.com/enterprise/3.0/creating_images/guidelines.htmlhttps://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdfhttps://leanpub.com/k8spatterns/https://12factor.net","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://yoursite.com/tags/Kubernetes/"}]},{"title":"微服务入门系列(五)：服务部署【转载】","date":"2018-03-28T07:35:15.000Z","path":"2018/03/28/Service-deployment.html","text":"当我们完成业务代码的开发后，就需要进入部署阶段。在部署过程中，我们将会引入持续集成、持续交付、持续部署，并且阐述如何在微服务中使用他们。 1. 持续集成、持续部署、持续交付在介绍这三个概念之前，我们首先来了解下使用了这三个概念之后的软件开发流程，如下图所示： 首先是代码的开发阶段，当代码完成开发后需要提交至代码仓库，此时需要对代码进行编译、打包，打包后的产物被称为“构建物”，如：对Web项目打包之后生成的war包、jar包就是一种构建物。此时的构建物虽然没有语法错误，但其质量是无法保证的，必须经过一系列严格的测试之后才能具有部署到生产环境的资格。我们一般会给系统分配多套环境，如开发环境、测试环境、预发环境、生产环境。每套环境都有它测试标准，当构建物完成了一套环境的测试，并达到交付标准时，就会自动进入下一个环境。构建物依次会经过这四套环境，构建物每完成一套环境的验证，就具备交付给下一套环境的资格。当完成预发环境的验证后，就具备的上线的资格。 测试和交付过程是相互伴随的，每一套环境都有各自的测试标准。如在开发环境中，当代码提交后需要通过编译、打包生成构建物，在编译的过程中会对代码进行单元测试，如果有任何测试用例没通过，整个构建流程就会被中止。此时开发人员需要立即修复问题，并重新提交代码、重新编译打包。 当单元测试通过之后，构建物就具备了进入测试环境的资格，此时它会被自动部署到测试环境，进行新一轮的测试。在测试环境中，一般需要完成接口测试和人工测试。接口测试由自动化脚本完成，这个过程完成后还需要人工进行功能性测试。人工测试完成后，需要手动触发进入下一个阶段。 此时构建物将会被部署到预发环境。预发环境是一种“类生产环境”，它和生产环境的服务器配置需要保持高度一致。在预发环境中，一般需要对构建物进行性能测试，了解其性能指标是否能满足上线的要求。当通过预发验证后，构建物已经具备了上线的资格，此时它可以随时上线。 上述过程涵盖了持续集成、持续交付、持续部署，那么下面我们就从理论角度来介绍这三个概念。 1.1 持续集成“集成”指的是修改后/新增的代码向代码仓库合并的过程，而“持续集成”指的是代码高频率合并。这样有什么好处呢？大家不妨想一想，如果我们集成代码的频率变高了，那么每次集成的代码量就会变少，由于每次集成的时候都会进行单元测试，从而当出现问题的时候问题出现的范围就被缩小的，这样就能快速定位到出错的地方，寻找问题就更容易了。此外，频繁集成能够使问题尽早地暴露，这样解决问题的成本也就越低。因为在软件测试中有这样一条定律，时间和bug修复的成本成正比，也就是时间越长，bug修复的成本也就越大。所以持续集成能够尽早发现问题，并能够及时修复问题，这对于软件的质量是非常重要的。 1.2 持续部署“持续部署”指的是当存在多套环境时，当构建物完成上一套环境的测试后，自动部署到下一套环境并进行一系列的测试，直到构建物满足上线的要求为止。 1.3 持续交付当系统通过了所有的测试之后，就具备了部署到生产环境的资格，这个过程也就被称为“交付”。“持续交付”指的是每个版本的构建物都具有上线的资格，这就要求每当代码库中有新的版本后，都需要自动触发构建、测试、部署、交付等一系列流程，当构建物在某个阶段的测试未通过时，就需要开发人员立即解决这个问题，并重新构建，从而保证每个版本的构建物都具备上线的资格，可以随时部署到生产环境中。 2. 微服务与持续集成当我们了解了持续集成后，下面来介绍微服务如何与持续集成相整合。当我们对系统进行了微服务化后，原本单一的系统被拆分成多个可独立运行的微服务。单服务系统的持续集成较为简单，代码库、构建和构建物之间都是一对一的关系。然而，当我们将系统微服务化后，持续集成就变得复杂了。下面介绍两种在微服务中使用持续集成的方法，分别是单库多构建和多库多构建，并依次介绍这两种方式的优缺点及使用场景。 2.1 单库多构建“单库”指的是单个代码仓库，即整个系统的多个模块的代码均由一个代码仓库维护。“多构建”指的是持续集成平台中的构建项目会有多个，每个构建都会生成一个构建物，如下如所示： 在这种持续集成的模式中，整个项目的所有代码均在同一个代码仓库中维护。但在持续集成平台中，每一项服务都有各自独立的构建，从而持续集成平台能够为每一项服务产出各自的构建物。 这种持续集成的模式在微服务架构中显然是不合理的。首先，一个系统的可能会有很多服务构成，如果将这些服务的代码均在同一个代码仓库中维护，那么一个程序员在开发服务A代码的时候很有可能会因为疏忽，修改了服务B的代码，此时服务B构建之后就会存在安全隐患，如果这个问题在服务B上线前被发现，那么还好，但无疑增加了额外的工作量；但如果这个问题及其隐讳，导致之前的测试用例没有覆盖到，从而服务B会带着这个问题进入生产环境，这可能会给企业带来巨大的损失。所以，在微服务架构中，尽量选择多库多构建模式来实现持续集成，它将带来更大的安全性。 虽然这种模式不合理，但它也有存在的必要性，当我们在项目建设初期的时候，这种模式会给我们带来更多的便利性。因为项目在建设初期，服务之间的边界往往是比较模糊的，而且需要经过一段时间的演化才能够构建出稳定的边界。所以如果在项目建设初期直接使用微服务架构，那么服务边界频繁地调整会极大增加系统开发的复杂度，你要知道，在多个系统之间调整边界比在单个系统的多个模块之间调整边界的成本要高很多。所以在项目建设初期，我们可以使用单服务结构，服务内部采用模块作为未来各个微服务的边界，当系统演化出较为清晰、稳定的边界后再将系统拆分成多个微服务。此时代码在同一个代码仓库中维护是合理的，这也符合敏捷开发中快速迭代的理念。 2.2 多库多构建 当系我们的系统拥有了稳定、清晰的边界后，就可以将系统向微服务架构演进。与此同时，持续集成模式也可以从单库多构建向多库多构建演进。 在多库多构建模式中，每项服务都有各自独立的代码仓库，代码仓库之间互不干扰。开发团队只需关注属于自己的某几项服务的代码仓库即可。每一项服务都有各自独立的构建。这种方式逻辑清晰，维护成本较低，而且能避免单库多构建模式中出现的影响其他服务的问题。 3. 微服务构建物持续集成平台对源码编译、打包后生成的产物称为“构建物”。根据打包的粒度不同，可以将构建物分为如下三种：平台构建物、操作系统构建物和镜像构建物。 3.1 平台构建物平台构建物指的是由某一特定平台生成的构建物，比如JVM平台生成的Jar包、War包，Python生成的egg等都属于平台构建物。但平台构建物运行需要部署在特定的容器中，如war需要运行在Servlet容器中，而Servlet容器又依赖的JVM环境。所以若要部署平台构建物，则需要先给它们提供好运行所需的环境。 3.2 操作系统构建物操作系统构建物是将系统打包成一个操作系统可执行程序，，如CentOS的RPM包、Windows的MSI包等。这些安装包可以在操作系统上直接安装运行。但和平台构建物相同的是，操作系统构建物往往也需要依赖于其他环境，所以也需要在部署之前搭建好安装包所需的依赖。此外，配置操作系统构建物的复杂度较大，构建的成本较高，所以一般不使用这种方式，这里仅作介绍。 3.3 镜像构建物平台构建物和操作系统构建物都有一个共同的缺点就是需要安装构建物运行的额外依赖，增加部署复杂度，而镜像构建物能很好地解决这个问题。 我们可以把镜像理解成一个小型操作系统，这个操作系统中包含了系统运行所需的所有依赖，并将系统也部署在这个“操作系统”中。这样当持续集成平台构建完这个镜像后，就可以直接运行它，无需任何依赖的安装，从而极大简化了构建的复杂度。但是，镜像往往比较庞大，构建镜像的过程也较长，从而当我们将生成的镜像从持续集成服务器发布到部署服务器的时间将会很长，这无疑降低了部署的效率。不过好在Docker的出现解决了这一问题。持续集成平台在构建过程中并不需要生成一个镜像，而只需生成一个镜像的Dockerfile文件即可。Dockerfile文件用命令定义了镜像所包含的内容，以及镜像创建的过程。从而持续集成服务器只需将这个体积较小的镜像文件发布到部署服务器上即可。然后部署服务器会通过docker build命令基于这个Dockerfile文件创建镜像，并创建该镜像的容器，从而完成服务的部署。 相对于平台构建物和操作系统构建物而言，镜像构建物在部署时不需要安装额外的环境依赖，它把环境依赖的配置都在持续集成平台构建Dockerfile文件时完成，从而简化了部署的过程。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(四)：数据库的服务化切分【转载】","date":"2018-03-28T07:17:47.000Z","path":"2018/03/28/Database-Service-Segmentation.html","text":"1. 什么是“分库分表”？随着大数据时代的到来，业务系统的数据量日益增大，数据存储能力逐渐成为影响系统性能的瓶颈。目前主流的关系型数据库单表存储上限为1000万条记录，而这一存储能力显然已经无法满足大数据背景下的业务系统存储要求了。随着微服务架构、分布式存储等概念的出现，数据存储问题也渐渐迎来了转机。而数据分片是目前解决海量数据持久化存储与高效查询的一种重要手段。数据分库分表的过程在系统设计阶段完成，要求系统设计人员根据系统预期的业务量，将未来可能出现瓶颈的数据库、数据表按照一定规则拆分成多个库、多张表。这些数据库和数据表需要部署在不同的服务器上，从而将数据读写压力分摊至集群中的各个节点，提升数据库整体处理能力，避免出现读写瓶颈的现象。 目前数据分片的方式一共有两种：离散分片和连续分片。 离散分片是按照数据的某一字段哈希取模后进行分片存储。只要哈希算法选择得当，数据就会均匀地分布在不同的分片中，从而将读写压力平均分配给所有分片，整体上提升数据的读写能力。然而，离散存储要求数据之间有较强的独立性，但实际业务系统并非如此，不同分片之间的数据往往存在一定的关联性，因此在某些场景下需要跨分片连接查询。由于目前所有的关系型数据库出于安全性考虑，均不支持跨库连接。因此，跨库操作需要由数据分库分表中间件来完成，这极大影响数据的查询效率。此外，当数据存储能力出现瓶颈需要扩容时，离散分片规则需要将所有数据重新进行哈希取模运算，这无疑成为限制系统可扩展性的一个重要因素。虽然，一致性哈希能在一定程度上减少系统扩容时的数据迁移，但数据迁移问题仍然不可避免。对于一个已经上线运行的系统而言，系统停止对外服务进行数据迁移的代价太大。 第二种数据分片的方式即为连续分片，它能解决系统扩容时产生的数据迁移问题。这种方式要求数据按照时间或连续自增主键连续存储。从而一段时间内的数据或相邻主键的数据会被存储在同一个分片中。当需要增加分片时，不会影响现有的分片。因此，连续分片能解决扩容所带来的数据迁移问题。但是，数据的存储时间和读写频率往往呈正比，也就是大量的读写往往都集中在最新存储的那一部分数据，这就会导致热点问题，并不能起到分摊读写压力的初衷。 2. 数据库扩展的几种方式数据库扩展一共有四种分配方式，分别是：垂直分库、垂直分表、水平分表、水平数据分片。每一种策略都有各自的适用场景。 1、垂直分库垂直分库即是将一个完整的数据库根据业务功能拆分成多个独立的数据库，这些数据库可以运行在不同的服务器上，从而提升数据库整体的数据读写性能。这种方式在微服务架构中非常常用。微服务架构的核心思想是将一个完整的应用按照业务功能拆分成多个可独立运行的子系统，这些子系统称为“微服务”，各个服务之间通过RPC接口通信，这样的结构使得系统耦合度更低、更易于扩展。垂直分库的理念与微服务的理念不谋而合，可以将原本完整的数据按照微服务拆分系统的方式，拆分成多个独立的数据库，使得每个微服务系统都有各自独立的数据库，从而可以避免单个数据库节点压力过大，影响系统的整体性能，如下图所示。 2、垂直分表垂直分表如果一张表的字段非常多，那么很有可能会引起数据的跨页存储，这会造成数据库额外的性能开销，而垂直分表可以解决这个问题。垂直分表就是将一张表中不常用的字段拆分到另一张表中，从而保证第一章表中的字段较少，避免出现数据库跨页存储的问题，从而提升查询效率。而另一张表中的数据通过外键与第一张表进行关联，如下图所示。 3、水平分表如果一张表中的记录数过多（超过1000万条记录），那么会对数据库的读写性能产生较大的影响，虽然此时仍然能够正确地读写，但读写的速度已经到了业务无法忍受的地步，此时就需要使用水平分表来解决这个问题。水平分表是将一张含有很多记录数的表水平切分，拆分成几张结构相同的表。举个例子，假设一张订单表目前存储了2000万条订单的数据，导致数据读写效率极低。此时可以采用水平分表的方式，将订单表拆分成100张结构相同的订单表，分别叫做order_1、order_2……、order_100。然后可以根据订单所属用户的id进行哈希取模后均匀地存储在这100张表中，从而每张表中只存储了20万条订单记录，极大提升了订单的读写效率，如下图所示。 当然，如果拆分出来的表都存储在同一个数据库节点上，那么当请求量过大的时候，毕竟单台服务器的处理能力是有限的，数据库仍然会成为系统的瓶颈，所以为了解决这个问题，就出现了水平数据分片的解决方案。 4、水平分库分表水平数据分片与数据分片区别在于：水平数据分片首先将数据表进行水平拆分，然后按照某一分片规则存储在多台数据库服务器上。从而将单库的压力分摊到了多库上，从而避免因为数据库硬件资源有限导致的数据库性能瓶颈，如下图所示。 分库分表的几种方式目前常用的数据分片策略有两种，分别是连续分片和离散分片。 1、离散分片离散分片是指将数据打散之后均匀地存储在逻辑表的各个分片中，从而使的对同一张逻辑表的数据读取操作均匀地落在不同库的不同表上，从而提高读写速度。离散分片一般以哈希取模的方式实现。比如：一张逻辑表有4个分片，那么在读写数据的时候，中间件首先会取得分片字段的哈希值，然后再模以4，从而计算出该条记录所在的分片。在这种方法中，只要哈希算法选的好，那么数据分片将会比较均匀，从而数据读写就会比较均匀地落在各个分片上，从而就有较高的读写效率。但是，这种方式也存在一个最大的缺陷——数据库扩容成本较高。采用这种方式，如果需要再增加分片，原先的分片算法将失效，并且所有记录都需要重新计算所在分片的位置。对于一个已经上线的系统来说，行级别的数据迁移成本相当高，而且由于数据迁移期间系统仍在运行，仍有新数据产生，从而无法保证迁移过程数据的一致性。如果为了避免这个问题而停机迁移，那必然会对业务造成巨大影响。当然，如果为了避免数据迁移，在一开始的时候就分片较多的分片，那需要承担较高的费用，这对于中小公司来说是无法承受的。 2、连续分片连续分片指的是按照某一种分片规则，将某一个区间内的数据存储在同一个分片上。比如按照时间分片，每个月生成一张物理表。那么在读写数据时，直接根据当前时间就可以找到数据所在的分片。再比如可以按照记录ID分片，这种分片方式要求ID需要连续递增。由于Mysql数据库单表支持最大的记录数约为1000万，因此我们可以根据记录的ID，使得每个分片存储1000万条记录，当目前的记录数即将到达存储上限时，我们只需增加分片即可，原有的数据无需迁移。连续分片的一个最大好处就是方便扩容，因为它不需要任何的数据迁移。但是，连续分片有个最大的缺点就是热点问题。连续分片使得新插入的数据集中在同一个分片上，而往往新插入的数据读写频率较高，因此，读写操作都会集中在最新的分片上，从而无法体现数据分片的优势。 引入分库分表中间件后面临的问题1、跨库操作在关系型数据库中，多张表之间往往存在关联，我们在开发过程中需要使用JOIN操作进行多表连接。但是当我们使用了分库分表模式后，由于数据库厂商处于安全考虑，不允许跨库JOIN操作，从而如果需要连接的两张表被分到不同的库中后，就无法使用SQL提供的JOIN关键字来实现表连接，我们可能需要在业务系统层面，通过多次SQL查询，完成数据的组装和拼接。这一方面会增加业务系统的复杂度，另一方面会增加业务系统的负载。因此，当我们使用分库分表模式时，需要根据具体的业务场景，合理地设置分片策略、设置分片字段，这将会在本文的后续章节中介绍。 2、分布式事务我们知道，数据库提供了事务的功能，以保证数据一致性。然而，这种事务只是针对单数据库而言的，数据库厂商并未提供跨库事务。因此，当我们使用了分库分表之后，就需要我们在业务系统层面实现分布式事务。关于分布式事务的详细内容，可以参考笔者的另一篇文章《常用的分布式事务解决方案》。 现有分库分表中间件的横向对比 Cobar实现数据库的透明分库，让开发人员能够在无感知的情况下操纵数据库集群，从而简化数据库的编程模型。然而Cobar仅实现了分库功能，并未实现分表功能。分库可以解决单库IO、CPU、内存的瓶颈，但无法解决单表数据量过大的问题。此外，Cobar是一个独立运行的系统，它处在应用系统与数据库系统之间，因此增加了额外的部署复杂度，增加了运维成本。 为了解决上述问题，Cobar还推出了一个Cobar-Client项目，它只是一个安装在应用程序的Jar包，并不是一个独立运行的系统，一定程度上降低了系统的复杂度。但和Cobar一样，仍然只支持分库，并不支持分表，也不支持读写分离。 MyCat是基于Cobar二次开发的数据库中间件，和Cobar相比，它增加了读写分离的功能，并修复了Cobar的一些bug。但是，MyCat和Cobar一样，都是一套需要独立部署的系统，因此会增加部署的复杂度，提高了后期系统运维的成本。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(三)：微服务架构下的分布式事务解决方案【转载】","date":"2018-03-28T05:55:06.000Z","path":"2018/03/28/Distributed-transaction-solutions.html","text":"分布式事务的解决方案有如下几种： 全局消息 基于可靠消息服务的分布式事务 TCC 最大努力通知 方案1：全局事务（DTP模型）全局事务基于DTP模型实现。DTP是由X/Open组织提出的一种分布式事务模型——X/Open Distributed Transaction Processing Reference Model。它规定了要实现分布式事务，需要三种角色： AP：Application 应用系统它就是我们开发的业务系统，在我们开发的过程中，可以使用资源管理器提供的事务接口来实现分布式事务。 TM：Transaction Manager 事务管理器 分布式事务的实现由事务管理器来完成，它会提供分布式事务的操作接口供我们的业务系统调用。这些接口称为TX接口。 事务管理器还管理着所有的资源管理器，通过它们提供的XA接口来同一调度这些资源管理器，以实现分布式事务。 DTP只是一套实现分布式事务的规范，并没有定义具体如何实现分布式事务，TM可以采用2PC、3PC、Paxos等协议实现分布式事务。 RM：Resource Manager 资源管理器 能够提供数据服务的对象都可以是资源管理器，比如：数据库、消息中间件、缓存等。大部分场景下，数据库即为分布式事务中的资源管理器。 资源管理器能够提供单数据库的事务能力，它们通过XA接口，将本数据库的提交、回滚等能力提供给事务管理器调用，以帮助事务管理器实现分布式的事务管理。 XA是DTP模型定义的接口，用于向事务管理器提供该资源管理器(该数据库)的提交、回滚等能力。 DTP只是一套实现分布式事务的规范，RM具体的实现是由数据库厂商来完成的。 有没有基于DTP模型的分布式事务中间件？ DTP模型有啥优缺点？ 方案2：基于可靠消息服务的分布式事务这种实现分布式事务的方式需要通过消息中间件来实现。假设有A和B两个系统，分别可以处理任务A和任务B。此时系统A中存在一个业务流程，需要将任务A和任务B在同一个事务中处理。下面来介绍基于消息中间件来实现这种分布式事务。 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程可以得出如下几个结论： 消息中间件扮演者分布式事务协调者的角色。 系统A完成任务A后，到任务B执行完成之间，会存在一定的时间差。在这个时间差内，整个系统处于数据不一致的状态，但这短暂的不一致性是可以接受的，因为经过短暂的时间后，系统又可以保持数据一致性，满足BASE理论。 上述过程中，如果任务A处理失败，那么需要进入回滚流程，如下图所示： 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 此时系统又处于一致性状态，因为任务A和任务B都没有执行。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？ 答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交 若获得的状态是“提交”，则将该消息投递给系统B。 回滚 若获得的状态是“回滚”，则直接将条消息丢弃。 处理中 若获得的状态是“处理中”，则继续等待。 消息中间件的超时询问机制能够防止上游系统因在传输过程中丢失Commit/Rollback指令而导致的系统不一致情况，而且能降低上游系统的阻塞时间，上游系统只要发出Commit/Rollback指令后便可以处理其他任务，无需等待确认应答。而Commit/Rollback指令丢失的情况通过超时询问机制来弥补，这样大大降低上游系统的阻塞时间，提升系统的并发度。 下面来说一说消息投递过程的可靠性保证。当上游系统执行完任务并向消息中间件提交了Commit指令后，便可以处理其他任务了，此时它可以认为事务已经完成，接下来消息中间件一定会保证消息被下游系统成功消费掉！那么这是怎么做到的呢？这由消息中间件的投递流程来保证。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 有的同学可能要问：消息投递失败后为什么不回滚消息，而是不断尝试重新投递？ 这就涉及到整套分布式事务系统的实现成本问题。 我们知道，当系统A将向消息中间件发送Commit指令后，它便去做别的事情了。如果此时消息投递失败，需要回滚的话，就需要让系统A事先提供回滚接口，这无疑增加了额外的开发成本，业务系统的复杂度也将提高。对于一个业务系统的设计目标是，在保证性能的前提下，最大限度地降低系统复杂度，从而能够降低系统的运维成本。 不知大家是否发现，上游系统A向消息中间件提交Commit/Rollback消息采用的是异步方式，也就是当上游系统提交完消息后便可以去做别的事情，接下来提交、回滚就完全交给消息中间件来完成，并且完全信任消息中间件，认为它一定能正确地完成事务的提交或回滚。然而，消息中间件向下游系统投递消息的过程是同步的。也就是消息中间件将消息投递给下游系统后，它会阻塞等待，等下游系统成功处理完任务返回确认应答后才取消阻塞等待。为什么这两者在设计上是不一致的呢？ 首先，上游系统和消息中间件之间采用异步通信是为了提高系统并发度。业务系统直接和用户打交道，用户体验尤为重要，因此这种异步通信方式能够极大程度地降低用户等待时间。此外，异步通信相对于同步通信而言，没有了长时间的阻塞等待，因此系统的并发性也大大增加。但异步通信可能会引起Commit/Rollback指令丢失的问题，这就由消息中间件的超时询问机制来弥补。 那么，消息中间件和下游系统之间为什么要采用同步通信呢？ 异步能提升系统性能，但随之会增加系统复杂度；而同步虽然降低系统并发度，但实现成本较低。因此，在对并发度要求不是很高的情况下，或者服务器资源较为充裕的情况下，我们可以选择同步来降低系统的复杂度。 我们知道，消息中间件是一个独立于业务系统的第三方中间件，它不和任何业务系统产生直接的耦合，它也不和用户产生直接的关联，它一般部署在独立的服务器集群上，具有良好的可扩展性，所以不必太过于担心它的性能，如果处理速度无法满足我们的要求，可以增加机器来解决。而且，即使消息中间件处理速度有一定的延迟那也是可以接受的，因为前面所介绍的BASE理论就告诉我们了，我们追求的是最终一致性，而非实时一致性，因此消息中间件产生的时延导致事务短暂的不一致是可以接受的。 方案3：最大努力通知（定期校对）最大努力通知也被称为定期校对，其实在方案二中已经包含，这里再单独介绍，主要是为了知识体系的完整性。这种方案也需要消息中间件的参与，其过程如下： 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。如果这量步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第二种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。 方案4：TCC（两阶段型、补偿型）TCC即为Try Confirm Cancel，它属于补偿型分布式事务。顾名思义，TCC实现分布式事务一共有三个步骤： Try：尝试待执行的业务 这个过程并未执行业务，只是完成所有业务的一致性检查，并预留好执行所需的全部资源 Confirm：执行业务 这个过程真正开始执行业务，由于Try阶段已经完成了一致性检查，因此本过程直接执行，而不做任何检查。并且在执行的过程中，会使用到Try阶段预留的业务资源。 Cancel：取消执行的业务 若业务执行失败，则进入Cancel阶段，它会释放所有占用的业务资源，并回滚Confirm阶段执行的操作。 下面以一个转账的例子来解释下TCC实现分布式事务的过程。 假设用户A用他的账户余额给用户B发一个100元的红包，并且余额系统和红包系统是两个独立的系统。 Try 创建一条转账流水，并将流水的状态设为交易中 将用户A的账户中扣除100元（预留业务资源） Try成功之后，便进入Confirm阶段 Try过程发生任何异常，均进入Cancel阶段 Confirm 向B用户的红包账户中增加100元 将流水的状态设为交易已完成 Confirm过程发生任何异常，均进入Cancel阶段 Confirm过程执行成功，则该事务结束 Cancel 将用户A的账户增加100元 将流水的状态设为交易失败 在传统事务机制中，业务逻辑的执行和事务的处理，是在不同的阶段由不同的部件来完成的：业务逻辑部分访问资源实现数据存储，其处理是由业务系统负责；事务处理部分通过协调资源管理器以实现事务管理，其处理由事务管理器来负责。二者没有太多交互的地方，所以，传统事务管理器的事务处理逻辑，仅需要着眼于事务完成（commit/rollback）阶段，而不必关注业务执行阶段。 TCC全局事务必须基于RM本地事务来实现全局事务TCC服务是由Try/Confirm/Cancel业务构成的，其Try/Confirm/Cancel业务在执行时，会访问资源管理器（Resource Manager，下文简称RM）来存取数据。这些存取操作，必须要参与RM本地事务，以使其更改的数据要么都commit，要么都rollback。 这一点不难理解，考虑一下如下场景： 假设图中的服务B没有基于RM本地事务（以RDBS为例，可通过设置auto-commit为true来模拟），那么一旦[B:Try]操作中途执行失败，TCC事务框架后续决定回滚全局事务时，该[B:Cancel]则需要判断[B:Try]中哪些操作已经写到DB、哪些操作还没有写到DB：假设[B:Try]业务有5个写库操作，[B:Cancel]业务则需要逐个判断这5个操作是否生效，并将生效的操作执行反向操作。 不幸的是，由于[B:Cancel]业务也有n（0&lt;=n&lt;=5）个反向的写库操作，此时一旦[B:Cancel]也中途出错，则后续的[B:Cancel]执行任务更加繁重。因为，相比第一次[B:Cancel]操作，后续的[B:Cancel]操作还需要判断先前的[B:Cancel]操作的n（0&lt;=n&lt;=5）个写库中哪几个已经执行、哪几个还没有执行，这就涉及到了幂等性问题。而对幂等性的保障，又很可能还需要涉及额外的写库操作，该写库操作又会因为没有RM本地事务的支持而存在类似问题。。。可想而知，如果不基于RM本地事务，TCC事务框架是无法有效的管理TCC全局事务的。 反之，基于RM本地事务的TCC事务，这种情况则会很容易处理：[B:Try]操作中途执行失败，TCC事务框架将其参与RM本地事务直接rollback即可。后续TCC事务框架决定回滚全局事务时，在知道“[B:Try]操作涉及的RM本地事务已经rollback”的情况下，根本无需执行[B:Cancel]操作。 换句话说，基于RM本地事务实现TCC事务框架时，一个TCC型服务的cancel业务要么执行，要么不执行，不需要考虑部分执行的情况。 TCC事务框架应该提供Confirm/Cancel服务的幂等性保障一般认为，服务的幂等性，是指针对同一个服务的多次(n&gt;1)请求和对它的单次(n=1)请求，二者具有相同的副作用。 在TCC事务模型中，Confirm/Cancel业务可能会被重复调用，其原因很多。比如，全局事务在提交/回滚时会调用各TCC服务的Confirm/Cancel业务逻辑。执行这些Confirm/Cancel业务时，可能会出现如网络中断的故障而使得全局事务不能完成。因此，故障恢复机制后续仍然会重新提交/回滚这些未完成的全局事务，这样就会再次调用参与该全局事务的各TCC服务的Confirm/Cancel业务逻辑。 既然Confirm/Cancel业务可能会被多次调用，就需要保障其幂等性。那么，应该由TCC事务框架来提供幂等性保障？还是应该由业务系统自行来保障幂等性呢？个人认为，应该是由TCC事务框架来提供幂等性保障。如果仅仅只是极个别服务存在这个问题的话，那么由业务系统来负责也是可以的；然而，这是一类公共问题，毫无疑问，所有TCC服务的Confirm/Cancel业务存在幂等性问题。TCC服务的公共问题应该由TCC事务框架来解决；而且，考虑一下由业务系统来负责幂等性需要考虑的问题，就会发现，这无疑增大了业务系统的复杂度。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(二)：微服务架构下的分布式事务基础入门【转载】","date":"2018-03-26T10:04:42.000Z","path":"2018/03/26/Distributed-transaction-basics.html","text":"众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？本文首先会为大家梳理分布式事务的基本概念和理论基础，然后介绍几种目前常用的分布式事务解决方案。废话不多说，那就开始吧～ 1. 什么是事务？事务由一组操作构成，我们希望这组操作能够全部正确执行，如果这一组操作中的任意一个步骤发生错误，那么就需要回滚之前已经完成的操作。也就是同一个事务中的所有操作，要么全都正确执行，要么全都不要执行。 2. 事务的四大特性 ACID说到事务，就不得不提一下事务著名的四大特性。 1、原子性 原子性要求，事务是一个不可分割的执行单元，事务中的所有操作要么全都执行，要么全都不执行。 2、一致性 一致性要求，事务在开始前和结束后，数据库的完整性约束没有被破坏。 3、隔离性 事务的执行是相互独立的，它们不会相互干扰，一个事务不会看到另一个正在运行过程中的事务的数据。 4、持久性 持久性要求，一个事务完成之后，事务的执行结果必须是持久化保存的。即使数据库发生崩溃，在数据库恢复后事务提交的结果仍然不会丢失。 注意：事务只能保证数据库的高可靠性，即数据库本身发生问题后，事务提交后的数据仍然能恢复；而如果不是数据库本身的故障，如硬盘损坏了，那么事务提交的数据可能就丢失了。这属于『高可用性』的范畴。因此，事务只能保证数据库的『高可靠性』，而『高可用性』需要整个系统共同配合实现。 3. 事务的隔离级别这里扩展一下，对事务的隔离性做一个详细的解释。 在事务的四大特性ACID中，要求的隔离性是一种严格意义上的隔离，也就是多个事务是串行执行的，彼此之间不会受到任何干扰。这确实能够完全保证数据的安全性，但在实际业务系统中，这种方式性能不高。因此，数据库定义了四种隔离级别，隔离级别和数据库的性能是呈反比的，隔离级别越低，数据库性能越高，而隔离级别越高，数据库性能越差。 3.1 事务并发执行会出现的问题我们先来看一下在不同的隔离级别下，数据库可能会出现的问题： 1、更新丢失 当有两个并发执行的事务，更新同一行数据，那么有可能一个事务会把另一个事务的更新覆盖掉。 当数据库没有加任何锁操作的情况下会发生。 2、脏读 一个事务读到另一个尚未提交的事务中的数据。 该数据可能会被回滚从而失效。 如果第一个事务拿着失效的数据去处理那就发生错误了。 3、不可重复读 不可重复度的含义：一个事务对同一行数据读了两次，却得到了不同的结果。它具体分为如下两种情况： 虚读：在事务1两次读取同一记录的过程中，事务2对该记录进行了修改，从而事务1第二次读到了不一样的记录。 幻读：事务1在两次查询的过程中，事务2对该表进行了插入、删除操作，从而事务1第二次查询的结果发生了变化。 不可重复读 与 脏读 的区别？脏读读到的是尚未提交的数据，而不可重复读读到的是已经提交的数据，只不过在两次读的过程中数据被另一个事务改过了。 3.2 数据库的四种隔离级别数据库一共有如下四种隔离级别： 1、Read uncommitted 读未提交 在该级别下，一个事务对一行数据修改的过程中，不允许另一个事务对该行数据进行修改，但允许另一个事务对该行数据读。 因此本级别下，不会出现更新丢失，但会出现脏读、不可重复读。 2、Read committed 读提交 在该级别下，未提交的写事务不允许其他事务访问该行，因此不会出现脏读；但是读取数据的事务允许其他事务的访问该行数据，因此会出现不可重复读的情况。 3、Repeatable read 重复读 在该级别下，读事务禁止写事务，但允许读事务，因此不会出现同一事务两次读到不同的数据的情况（不可重复读），且写事务禁止其他一切事务。 4、Serializable 序列化 该级别要求所有事务都必须串行执行，因此能避免一切因并发引起的问题，但效率很低。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 4. 什么是分布式事务？到此为止，所介绍的事务都是基于单数据库的本地事务，目前的数据库仅支持单库事务，并不支持跨库事务。而随着微服务架构的普及，一个大型业务系统往往由若干个子系统构成，这些子系统又拥有各自独立的数据库。往往一个业务流程需要由多个子系统共同完成，而且这些操作可能需要在一个事务中完成。在微服务系统中，这些业务场景是普遍存在的。此时，我们就需要在数据库之上通过某种手段，实现支持跨数据库的事务支持，这也就是大家常说的“分布式事务”。 这里举一个分布式事务的典型例子——用户下单过程。 当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下： 用户通过商品系统浏览商品，他看中了某一项商品，便点击下单 此时订单系统会生成一条订单 订单创建成功后，支付系统提供支付功能 当支付完成后，由积分系统为该用户增加积分 上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。 5. CAP理论CAP理论说的是：在一个分布式系统中，最多只能满足C、A、P中的两个需求。 CAP的含义： C：Consistency 一致性 同一数据的多个副本是否实时相同。 A：Availability 可用性 可用性：一定时间内 &amp; 系统返回一个明确的结果 则称为该系统可用。 P：Partition tolerance 分区容错性 将同一服务分布在多个系统中，从而保证某一个系统宕机，仍然有其他系统提供相同的服务。 CAP理论告诉我们，在分布式系统中，C、A、P三个条件中我们最多只能选择两个。那么问题来了，究竟选择哪两个条件较为合适呢？ 对于一个业务系统来说，可用性和分区容错性是必须要满足的两个条件，并且这两者是相辅相成的。业务系统之所以使用分布式系统，主要原因有两个： 提升整体性能 当业务量猛增，单个服务器已经无法满足我们的业务需求的时候，就需要使用分布式系统，使用多个节点提供相同的功能，从而整体上提升系统的性能，这就是使用分布式系统的第一个原因。 实现分区容错性 单一节点 或 多个节点处于相同的网络环境下，那么会存在一定的风险，万一该机房断电、该地区发生自然灾害，那么业务系统就全面瘫痪了。为了防止这一问题，采用分布式系统，将多个子系统分布在不同的地域、不同的机房中，从而保证系统高可用性。 这说明分区容错性是分布式系统的根本，如果分区容错性不能满足，那使用分布式系统将失去意义。 此外，可用性对业务系统也尤为重要。在大谈用户体验的今天，如果业务系统时常出现“系统异常”、响应时间过长等情况，这使得用户对系统的好感度大打折扣，在互联网行业竞争激烈的今天，相同领域的竞争者不甚枚举，系统的间歇性不可用会立马导致用户流向竞争对手。因此，我们只能通过牺牲一致性来换取系统的可用性和分区容错性。这也就是下面要介绍的BASE理论。 6. BASE理论CAP理论告诉我们一个悲惨但不得不接受的事实——我们只能在C、A、P中选择两个条件。而对于业务系统而言，我们往往选择牺牲一致性来换取系统的可用性和分区容错性。不过这里要指出的是，所谓的“牺牲一致性”并不是完全放弃数据一致性，而是牺牲强一致性换取弱一致性。下面来介绍下BASE理论。 BA：Basic Available 基本可用 “一定时间”可以适当延长 当举行大促时，响应时间可以适当延长 给部分用户返回一个降级页面 给部分用户直接返回一个降级页面，从而缓解服务器压力。但要注意，返回降级页面仍然是返回明确结果。 整个系统在某些不可抗力的情况下，仍然能够保证“可用性”，即一定时间内仍然能够返回一个明确的结果。只不过“基本可用”和“高可用”的区别是： S：Soft State：柔性状态 同一数据的不同副本的状态，可以不需要实时一致。 E：Eventual Consisstency：最终一致性 同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后仍然是一致的。 7. 酸碱平衡ACID能够保证事务的强一致性，即数据是实时一致的。这在本地事务中是没有问题的，在分布式事务中，强一致性会极大影响分布式系统的性能，因此分布式系统中遵循BASE理论即可。但分布式系统的不同业务场景对一致性的要求也不同。如交易场景下，就要求强一致性，此时就需要遵循ACID理论，而在注册成功后发送短信验证码等场景下，并不需要实时一致，因此遵循BASE理论即可。因此要根据具体业务场景，在ACID和BASE之间寻求平衡。 8. 分布式事务协议下面介绍几种实现分布式事务的协议。 8.1 两阶段提交协议 2PC分布式系统的一个难点是如何保证架构下多个节点在进行事务性操作的时候保持一致性。为实现这个目的，二阶段提交算法的成立基于以下假设： 该分布式系统中，存在一个节点作为协调者(Coordinator)，其他节点作为参与者(Cohorts)。且节点之间可以进行网络通信。 所有节点都采用预写式日志，且日志被写入后即被保持在可靠的存储设备上，即使节点损坏不会导致日志数据的消失。 所有节点不会永久性损坏，即使损坏后仍然可以恢复。 1. 第一阶段（投票阶段）： 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 2. 第二阶段（提交执行阶段）：当协调者节点从所有参与者节点获得的相应消息都为”同意”时： 协调者节点向所有参与者节点发出”正式提交(commit)”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的： 执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。（没有多少容错机制） 协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。（这个可以依赖后面要讲的Paxos协议实现HA） 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 为此，Dale Skeen和Michael Stonebraker在“A Formal Model of Crash Recovery in a Distributed System”中提出了三阶段提交协议（3PC）。 8.2 三阶段提交协议 3PC与两阶段提交不同的是，三阶段提交有两个改动点。 引入超时机制。同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。 1. CanCommit阶段3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 1、事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 2、响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No 2. PreCommit阶段协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。 1、发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。 2、事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。 3、响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 1、发送中断请求 协调者向所有参与者发送abort请求。 2、中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 3. doCommit阶段该阶段进行真正的事务提交，也可以分为以下两种情况。 3.1 执行提交 1、发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。 2、事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 3、响应反馈 事务提交完之后，向协调者发送Ack响应。 4、完成事务 协调者接收到所有参与者的ack响应之后，完成事务。 3.2 中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 1、发送中断请求 协调者向所有参与者发送abort请求 2、事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 3、反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息 4、中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"微服务入门系列(一)：走进微服务的世界【转载】","date":"2018-03-25T03:55:31.000Z","path":"2018/03/25/Into-Microservices.html","text":"1. 什么是微服务？我们首先给出微服务的定义，然后再对该定义给出详细的解释。 微服务就是一些可独立运行、可协同工作的小的服务。 从概念中我们可以提取三个关键词：可独立运行、可协同工作、小。这三个词高度概括了微服务的核心特性。下面我们就对这三个词作详细解释。 1、可独立运行 微服务是一个个可以独立开发、独立部署、独立运行的系统或者进程。 2、可协同工作 采用了微服务架构后，整个系统被拆分成多个微服务，这些服务之间往往不是完全独立的，在业务上存在一定的耦合，即一个服务可能需要使用另一个服务所提供的功能。这就是所谓的“可协同工作”。与单服务应用不同的是，多个微服务之间的调用时通过RPC通信来实现，而非单服务的本地调用，所以通信的成本相对要高一些，但带来的好处也是可观的。 3、小而美 微服务的思想是，将一个拥有复杂功能的庞大系统，按照业务功能，拆分成多个相互独立的子系统，这些子系统则被称为“微服务”。每个微服务只承担某一项职责，从而相对于单服务应用来说，微服务的体积是“小”的。小也就意味着每个服务承担的职责变少，根据单一职责原则，我们在系统设计时，要尽量使得每一项服务只承担一项职责，从而实现系统的“高内聚”。 2. 微服务的优点1. 易于扩展在单服务应用中，如果目前性能到达瓶颈，无法支撑目前的业务量，此时一般采用集群模式，即增加服务器集群的节点，并将这个单服务应用“复制”到所有的节点上，从而提升整体性能。然而这种扩展的粒度是比较粗糙的。如果只是系统中某一小部分存在性能问题，在单服务应用中，也要将整个应用进行扩展，这种方式简单粗暴，无法对症下药。而当我们使用了微服务架构后，如果某一项服务的性能到达瓶颈，那么我们只需要增加该服务的节点数即可，其他服务无需变化。这种扩展更加具有针对性，能够充分利用计算机硬件/软件资源。而且只扩展单个服务影响的范围较小，从而系统出错的概率也就越低。 2. 部署简单对于单服务应用而言，所有代码均在一个项目中，从而导致任何微小的改变都需要将整个项目打包、发布、部署，而这一系列操作的代价是高昂的。长此以往，团队为了降低发布的频率，会使得每次发布都伴随着大量的修改，修改越多也就意味着出错的概率也越大。当我们采用微服务架构以后，每个服务只承担少数职责，从而每次只需要发布发生修改的系统，其他系统依然能够正常运行，波及范围较小。此外，相对于单服务应用而言，每个微服务系统修改的代码相对较少，从而部署后出现错误的概率也相对较低。 3. 技术异构性对于单服务应用而言，一个系统的所有模块均整合在一个项目中，所以这些模块只能选择相同的技术。但有些时候，单一技术没办法满足不同的业务需求。如对于项目的算法团队而言，函数试编程语言可能更适合算法的开发，而对于业务开发团队而言，类似于Java的强类型语言具有更高的稳定性。然而在单服务应用中只能互相权衡，选择同一种语言，而当我们使用微服务结构后，这个问题就能够引刃而解。我们将一个完整的系统拆分成了多个独立的服务，从而每个服务都可以根据各自不同的特点，选择最为合适的技术体系。 当然，并不是所有的微服务系统都具备技术异构性，要实现技术异构性，必须保证所有服务都提供通用接口。我们知道，在微服务系统中，服务之间采用RPC接口通信，而实现RPC通信的方式有很多。有一些RPC通信方式与语言强耦合，如Java的RMI技术，它就要求通信的双方都必须采用Java语言开发。当然，也有一些RPC通信方式与语言无关，如基于HTTP协议的REST。这种通信方式对通信双方所采用的语言没有做任何限制，只要通信过程中传输的数据遵循REST规范即可。当然，与语言无关也就意味着通信双方没有类型检查，从而会提高出错的概率。所以，究竟选择与语言无关的RPC通信方式，还是选择与语言强耦合的RPC通信方式，需要我们根据实际的业务场景合理地分析。","tags":[{"name":"微服务入门系列","slug":"微服务入门系列","permalink":"http://yoursite.com/tags/微服务入门系列/"}]},{"title":"Kubernetes中的亲和性【转载】","date":"2018-03-24T02:12:08.000Z","path":"2018/03/24/Affinity-in-Kubernetes.html","text":"现实中应用的运行对于kubernetes在亲和性上提出了一些要求，可以归类到以下几个方面： Pod固定调度到某些节点之上 Pod不会调度到某些节点之上 Pod的多副本调度到相同的节点之上 Pod的多副本调度到不同的节点之上 实践下面我们将通过例子的方式来说明在kubernetes需要去设置亲和性实现上面要求． Pod调动到某些节点上Pod的定义中通过nodeSelector指定label标签，pod将会只调度到具有该标签的node之上 apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd 这个例子中pod只会调度到具有disktype=ssd的node上面． 节点亲和性/反亲和性Affinity/anti-affinity node 相对于nodeSelector机制更加的灵活和丰富 表达的语法：支持In,NotIn,Exists,DoesNotExist,Gt,Lt． 支持soft(preference)和hard(requirement),hard表示pod sheduler到某个node上，则必须满足亲和性设置．soft表示scheduler的时候，无法满足节点的时候，会选择非nodeSelector匹配的节点． nodeAffinity的基础上添加多个nodeSelectorTerms字段，调度的时候Node只需要nodeSelectorTerms中的某一个符合条件就符合nodeAffinity的规则．在nodeSelectorTerms中添加matchExpressions，需要可以调度的Node是满足matchExpressions中表示的所有规则． apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: k8s.gcr.io/pause:2.0 Pod间的亲和性和反亲和性基于已经运行在Node 上pod的labels来决定需要新创建的Pods是否可以调度到node节点上，配置的时候可以指定那个namespace中的pod需要满足pod的亲和性．可以通过topologyKey来指定topology domain, 可以指定为node／cloud provider zone／cloud provider region的范围． 表达的语法：支持In, NotIn, Exists, DoesNotExist Pod的亲和性和反亲和性可以分成 requiredDuringSchedulingIgnoredDuringExecution #硬要求 preferredDuringSchedulingIgnoredDuringExecution ＃软要求 类似上面node的亲和策略类似，requiredDuringSchedulingIgnoredDuringExecution亲和性可以用于约束不同服务的pod在同一个topology domain的Nod上．preferredDuringSchedulingIgnoredDuringExecution反亲和性可以将服务的pod分散到不同的topology domain的Node上． topologyKey可以设置成如下几种类型 kubernetes.io/hostname ＃Node failure-domain.beta.kubernetes.io/zone ＃Zone failure-domain.beta.kubernetes.io/region #Region 可以设置node上的label的值来表示node的name,zone,region等信息，pod的规则中指定topologykey的值表示指定topology范围内的node上运行的pod满足指定规则 apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname containers: - name: with-pod-affinity image: k8s.gcr.io/pause:2.0 利用社区官方的例子来进一步的说明，例子中指定了pod的亲和性和反亲和性，preferredDuringSchedulingIgnoredDuringExecution指定的规则是pod将会调度到的node尽量会满足如下条件： node上具有failure-domain.beta.kubernetes.io/zone，并且具有相同failure-domain.beta.kubernetes.io/zone的值的node上运行有一个pod,它符合label为securtity=S1. preferredDuringSchedulingIgnoredDuringExecution规则表示将不会调度到node上运行有security=S2的pod．如果这里我们将topologyKey＝failure-domain.beta.kubernetes.io/zone，那么pod将不会调度到node满足的条件是：node上具有failure-domain.beta.kubernetes.io/zone相同的Value,并且这些相同zone下的node上运行有security=S2的pod. Notice:对于topologyKey字段具有如下约束 对于亲和性以及RequiredDuringScheduling的反亲和性，topologyKey需要指定 对于RequiredDuringScheduling的反亲和性，LimitPodHardAntiAffinityTopology的准入控制限制topologyKey为kubernetes.io/hostname,可以通过修改或者disable解除该约束 对于PreferredDuringScheduling的反亲和性，空的topologyKey表示kubernetes.io/hostname, failure-domain.beta.kubernetes.io/zone and failure-domain.beta.kubernetes.io/region的组合． topologyKey在遵循其他约束的基础上可以设置成其他的key. 规则中可以指定匹配pod所在namespace,如果定义了但是为空，它表示所有namespace范围内的pod. 常用的场景一些更加常用的场景见例子所示 例子一 apiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: redis-server image: redis:3.2-alpine 创建了一个Deployment,副本数为３，指定了反亲和规则如上所示，pod的label为app:store,那么pod调度的时候将不会调度到node上已经运行了label为app:store的pod了，这样就会使得Deployment的三副本分别部署在不同的host的node上． 例子二 apiVersion: apps/v1 kind: Deployment metadata: name: web-server spec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: &quot;kubernetes.io/hostname&quot; podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: web-app image: nginx:1.12-alpine 在一个例子中基础之上，要求pod的亲和性满足requiredDuringSchedulingIgnoredDuringExecution中topologyKey=”kubernetes.io/hostname”,并且node上需要运行有app=store的label. 运行完例子一，例子二，那么pod的分布如下所示 $kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE redis-cache-1450370735-6dzlj 1/1 Running 0 8m 10.192.4.2 kube-node-3 redis-cache-1450370735-j2j96 1/1 Running 0 8m 10.192.2.2 kube-node-1 redis-cache-1450370735-z73mh 1/1 Running 0 8m 10.192.3.1 kube-node-2 web-server-1287567482-5d4dz 1/1 Running 0 7m 10.192.2.3 kube-node-1 web-server-1287567482-6f7v5 1/1 Running 0 7m 10.192.4.3 kube-node-3 web-server-1287567482-s330j 1/1 Running 0 7m 10.192.3.2 kube-node-2 例子三 apiVersion: apps/v1beta1 # for versions before 1.6.0 use extensions/v1beta1 kind: Deployment metadata: name: web-server spec: replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web-store topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: web-app image: hub.easystack.io/library/nginx:1.9.0 在一些应用中，pod副本之间需要共享cache,需要将pod运行在一个节点之上 web-server-77bfb4575f-bhxvg 1/1 Running 0 11s 10.233.66.79 hzc-slave2 app=web-store,pod-template-hash=3369601319 web-server-77bfb4575f-mkfd9 1/1 Running 0 11s 10.233.66.80 hzc-slave2 app=web-store,pod-template-hash=3369601319 web-server-77bfb4575f-wgjq6 1/1 Running 0 11s 10.233.66.78 hzc-slave2 app=web-store,pod-template-hash=3369601319 参考https://github.com/davidkbainbridge/demo-affinityhttps://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-featurehttps://medium.com/kokster/scheduling-in-kubernetes-part-2-pod-affinity-c2b217312ae1","tags":[{"name":"亲和性","slug":"亲和性","permalink":"http://yoursite.com/tags/亲和性/"}]},{"title":"Pipeline语法","date":"2018-03-23T09:23:48.000Z","path":"2018/03/23/Pipeline-syntax.html","text":"介绍本文基于入门介绍，仅仅是一个语法参考。至于如何在特定的例子中运用Pipeline语法，请参考Jenkins Pipeline。从插件Pipeline plugin的2.5版本开始，Pipeline支持两种格式的语法。对于它们之间的区别请参考语法对比。 正如在入门介绍里说的，流水线最主要的就是”步骤“。基本上，就是步骤来告诉Jenkins该干什么，它是申明式和脚本式流水线语法的基础。 你可以在流水线步骤参考中，找到一份可用的步骤列表。 申明Pipeline申明式流水线是最近添加到Jenkins流水线功能中的，这种语法更加简单。 所有合法的申明式流水线必须在 pipeline 代码块中，例如： pipeline { /* insert Declarative Pipeline here */ } 在申明式流水线中，基本的语句和表达式是遵循 Groovy语法 ，但是有以下几个例外： 流水线的顶层必须是一个代码块： pipeline { } 不需分号作为语句的分隔符。每个语句单独占一行 只能包括段落、步骤、或者赋值语句 属性引用语句被当作无参数的方法调用。例如：input会当作方法input() 段落在申明式流水线中，通常包括一个或者多个指令或者步骤。 代理agent代理指定了整个流水线或者特定的阶段的运行环境。它必须在pipeline块的顶层定义，而在阶段中是可选的。 参数为了支持多种情况的流水线使用场景，代理（agent）支持几种不同类型的参数。这些参数既可以在顶层的pipeline块也可以在每个阶段中使用。 any在任意可用的代理上执行流水线。例如： agent any none当在顶层的pipeline块中使用时，不会有全局的代理分配给整个流水线，每个阶段都需要包含个人的代理。例如：agent none label根据Jenkins环境中提供的标签，确定一个可用的代理来chiding流水线或者阶段。例如：agent { label ‘my-defined-label’ } nodeagent { node { label ‘labelName’ } } 和 agent { label ‘labelName’ }一样，但是 node 允许增加选项（例如 customWorkspace） docker在指定的容器里执行流水线或者阶段，容器会被动态分配到预先配置好的基于Docker的流水线节点，或者通过label参数来匹配。docker也有一个可选参数args，该参数会直接传递给docker run来执行；还有一个alwaysPull 选项，及时镜像名已经存在了依然会强制执行docker pull。 例如： agent { docker &apos;maven:3-alpine&apos; } 或者： agent { docker { image &apos;maven:3-alpine&apos; label &apos;my-defined-label&apos; args &apos;-v /tmp:/tmp&apos; } } dockerfile由码线中的Dockerfile构建出来的容器，执行流水线或者阶段。为了使用该特性，Jenkinsfile 必须是在多分支流水线或者从SCM中加载。约定Dockerfile 在码线的根目录中，可以是agent { dockerfile true } 。如果Dockerfile 在另外一个目录中，可以使用参数dir ：agent { dockerfile { dir ‘someSubDir’ } } 。如果Dockerfile 有其他的名称，你可以通过参数filename 指定文件名称。你可以通过参数additionalBuildArgs 给命令docker build … 传递额外的选项，例如agent { dockerfile { additionalBuildArgs ‘–build-arg foo=bar’ } } 。例如：一个码线有文件build/Dockerfile.build，并需要一个构建参数version： agent { // Equivalent to &quot;docker build -f Dockerfile.build --build-arg version=1.0.2 ./build/ dockerfile { filename &apos;Dockerfile.build&apos; dir &apos;build&apos; label &apos;my-defined-label&apos; additionalBuildArgs &apos;--build-arg version=1.0.2&apos; } 通用选项Common Options有一些选项可以在多种代理实现中使用。没有指定的话，就不是必须的。 label字符串。可以在流水线或者 stage上。 该选项可以在 node，docker 和 dockerfile中使用，但对于 node是必须的。 customWorkspace字符串。指定工作空间，而不使用默认的。可以是相对于节点上的根工作空间，也可以是绝对路径。例如： agent { node { label &apos;my-defined-label&apos; customWorkspace &apos;/some/other/path&apos; } } 该选项可以用在 node， docker 和 dockerfile。 reuseNode布尔值，默认为false。如果为true，则在相同的工作空间中运行，而不是每次创建新的。该选项可以在 docker 和 dockerfile中使用，而且只有在 agent 配置到单独的 stage中才能使用。 Jenkinsfile (Declarative Pipeline) pipeline { agent { docker &apos;maven:3-alpine&apos; } stages { stage(&apos;Example Build&apos;) { steps { sh &apos;mvn -B clean verify&apos; } } } } Jenkinsfile (Declarative Pipeline) pipeline { agent none stages { stage(&apos;Example Build&apos;) { agent { docker &apos;maven:3-alpine&apos; } steps { echo &apos;Hello, Maven&apos; sh &apos;mvn --version&apos; } } stage(&apos;Example Test&apos;) { agent { docker &apos;openjdk:8-jre&apos; } steps { echo &apos;Hello, JDK&apos; sh &apos;java -version&apos; } } } } post 阶段stages 步骤steps 指令Directive 环境environment 选项options 参数parameters 触发器triggers 阶段stage 工具tools 输入input 条件when 指令when 允许流水线根据条件来决定是否要执行特定的阶段。指令when 必须至少包含一个条件。如果指令when 包含多个条件，所有的条件都必须为true才可以会执行该阶段。这和allOf 条件是类似的（请参考下面的例子）。 更复杂的结构可以使用嵌套：not， allOf，或 anyOf。可以嵌套任意深度。 内置条件：分支branch当匹配分支名称时执行，例如： when { branch ‘master’ }。这只有在多分支流水线中才可以使用。 环境environment当指定的环境变量值和给定的一样时执行，例如： when { environment name: ‘DEPLOY_TO’, value: ‘production’ } 表达式expression当Groovy表达式为true时，例如： when { expression { return params.DEBUG_BUILD } } not当嵌套条件值为false时执行。必须包含一个条件。例如： when { not { branch ‘master’ } } allOf当嵌套条件为true时执行。必须至少包含一个。例如： when { allOf { branch ‘master’; environment name: ‘DEPLOY_TO’, value: ‘production’ } } anyOf当任意一个表达式为true时。必须至少包含一个。例如： when { anyOf { branch ‘master’; branch ‘staging’ } } 在进入阶段的代理节点之前计算when表达式 默认情况下，when 条件是在进入阶段的代理之后计算。然而，通过增加选项beforeAgent 可以改变。如果把选项beforeAgent 设置为true，就会首先计算when 条件，只有在值为true时才会进入。 示例1： Jenkinsfile (Declarative Pipeline) pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 示例2： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 示例3： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { allOf { branch &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例4： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { branch &apos;production&apos; anyOf { environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例5： pipeline { agent any stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { when { expression { BRANCH_NAME ==~ /(production|staging)/ } anyOf { environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos; environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos; } } steps { echo &apos;Deploying&apos; } } } } 示例6： pipeline { agent none stages { stage(&apos;Example Build&apos;) { steps { echo &apos;Hello World&apos; } } stage(&apos;Example Deploy&apos;) { agent { label &quot;some-label&quot; } when { beforeAgent true branch &apos;production&apos; } steps { echo &apos;Deploying&apos; } } } } 并发parallel阶段是可以并行执行的。注意，在阶段内必须要只能有一个steps 或 parallel。任何包含parallel 的阶段不能包括agent 或 tools，也不包括steps。 另外，当有一个任务失败后，你可以强制整个并行失败。只要设置参数failFast 为true就可以。 示例： pipeline { agent any stages { stage(&apos;Non-Parallel Stage&apos;) { steps { echo &apos;This stage will be executed first.&apos; } } stage(&apos;Parallel Stage&apos;) { when { branch &apos;master&apos; } failFast true parallel { stage(&apos;Branch A&apos;) { agent { label &quot;for-branch-a&quot; } steps { echo &quot;On Branch A&quot; } } stage(&apos;Branch B&apos;) { agent { label &quot;for-branch-b&quot; } steps { echo &quot;On Branch B&quot; } } } } } } 步骤脚本script 可以在申明时的流水线中执行脚本时步骤。大多数情况下 script 是用不到的。 示例： pipeline { agent any stages { stage(&apos;Example&apos;) { steps { echo &apos;Hello World&apos; script { def browsers = [&apos;chrome&apos;, &apos;firefox&apos;] for (int i = 0; i &amp;lt; browsers.size(); ++i) { echo &quot;Testing the ${browsers[i]} browser&quot; } } } } } } 脚本化流水线脚本化流水线可以使用普通的Groovy语法，因此，它可以实现很强大的功能。 在Jenkins流水线刚被开发出来时，采用Groovy作为基础。Jenkins已经很长时间内采用嵌入式的Groovy引擎提供了高级脚本功能给管理员和普通用户。也就是说，基于Groovy脚本的流水线指的就是脚本化流水线。 流程控制步骤语法对比参考https://jenkins.io/doc/book/pipeline/syntax/","tags":[{"name":"Pipeline","slug":"Pipeline","permalink":"http://yoursite.com/tags/Pipeline/"}]},{"title":"Jenkins Pipeline","date":"2018-03-22T05:02:36.000Z","path":"2018/03/22/Jenkins-Pipeline.html","text":"介绍本文介绍如何在Jenkins中使用pipeline插件。 Jenkins安装启动后，还需要安装一些插件才可以使用pipeline（流水线）的特性。你可以在系统管理–插件管理–可选插件中搜索Pipeline进行按钮；要提醒一下的是，Jenkins会自己查找依赖的插件，所以你可能看到安装的插件不只一个。Pipeline插件的wiki地址是https://wiki.jenkins.io/display/JENKINS/Pipeline+Plugin。另外，你可以通过这里https://plugins.jenkins.io/workflow-aggregator，查看该插件的依赖关系，并找到Pipeline插件在Github上的托管地址。 什么是PipelineJenkins Pipeline是一套插件，支持实现和持续集成作为流水线应用到Jenkins。Pipeline提供了一套可扩展的工具。 Pipeline大致可以分为：节点、阶段、步骤。步骤是具体的功能表达式，例如：执行shell命令等。阶段，你可以理解为步骤的集合。而节点则是包含阶段，它规定了这些阶段（步骤）都会在哪些slave上运行。 节点，可以是一个普通的slave，也可以运行在Docker容器中。 为什么要用Pipeline根本上来说，Jenkins是一个支持很多自动化模式的引擎。Pipeline增加了一套强大的工具到Jenkins中，支持用户从简单持续集成到全面的持续集成。通过模块化一些列相关的任务，用户可以利用很多Pipeline的特性。 代码：Pipelines通过代码来实现，并通常可以由版本控制系统（svn、git等）来管理。 可暂停：Pipelines可以暂停（停止），并且可以在运行之前接收人工输入或者等待同意。 Pipeline表达式Step是一个单一任务，告诉Jenkins该做什么。例如，在step中执行shell命令make。当一个插件扩展了Pipeline DSL，就意味着可以使用新的step。 Node大多数工作是在一个或者多个节点（node）中完成的。 语法Jenkins的流水线（pipeline）采用groovy语法来编写。逻辑判断、循环、异常等功能都是具备的，另外，熟悉groovy的人就明白这和Java的写法有一定的相似。 下面我介绍一些流水线的步骤（或者函数），首先介绍的是在插件workflow-basic-steps-plugin中的。我们从插件的名称上也能看到，这些流水线步骤大多是基础、简单的。首先，给出我研究时的版本信息： &lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt;` &lt;artifactId&gt;workflow-basic-steps&lt;/artifactId&gt; &lt;version&gt;2.7-SNAPSHOT&lt;/version&gt; 以便各位依据本文可以进一步学习Jenkins流水线插件的源码。 环境变量node { echo env.JENKINS_HOME sh &apos;echo $JENKINS_HOME&apos; echo env.JOB_NAME echo env.NODE_NAME echo env.NODE_LABELS echo env.WORKSPACE echo env.JENKINS_URL echo env.BUILD_URL env.SUREN_VER = &apos;12&apos; echo env.SUREN_VER } 上面的示例中，给出了如何使用内置的环境变量和自定义环境变量的做法 node() { env.JDK_HOME = &quot;${tool &apos;8u131&apos;}&quot; env.PATH=&quot;${env.JDK_HOME}/bin:${env.PATH}&quot; echo env.JDK_HOME echo env.PATH sh &apos;java -version&apos; } node(&apos;bimpm_deploytodev&apos;) { def pass_bin = &apos;/opt/pass/bin&apos; env.PASS_BIN = pass_bin stage(&apos;Clean&apos;) { sh &apos;rm -rfv $PASS_BIN&apos; } } 工具node() { tool name: &apos;JDK8_Linux&apos;, type: &apos;jdk&apos; tool name: &apos;maven339_linux_dir&apos;, type: &apos;maven&apos; echo &apos;hello&apos; } 上面的pipeline指定需要工具jdk和maven的名称（在Global Tool Configuration中配置）。 对应的实现类为ToolStep，该类被final关键字所修饰，因此是不能做扩展的了。 属性node { echo &apos;hello&apos; } properties([ buildDiscarder( logRotator( artifactDaysToKeepStr: &apos;&apos;, artifactNumToKeepStr: &apos;&apos;, daysToKeepStr: &apos;5&apos;, numToKeepStr: &apos;10&apos; ) ), pipelineTriggers([ cron(&apos;H 3,12,17 * * *&apos;) ]) ]) 拷贝成品node { stage(&apos;Copy&apos;) { step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true, projectName: &apos;BIM_PMJF/BIM-PMJF-BUILD/BIM_PMJF_DISCOVERY&apos;, selector: [$class: &apos;StatusBuildSelector&apos;, stable: false], target: &apos;/opt/pass/bin&apos;]) } } 我们通常会在一个Job里实现工程构建，在另外的Job里做程序的部署，这时候就可以用到Jenkins的成品特性。它可以实现在多个slave之间拷贝成品。实现类为ArtifactArchiverStep。 循环node(&apos;suren&apos;) { def dev_path = &apos;/opt/suren/bin&apos; def services = [ [ &apos;name&apos;: &apos;admin&apos;, &apos;project&apos;: &apos;admin&apos;, &apos;port&apos;: &apos;7002&apos;, &apos;jarName&apos;: &apos;admin&apos; ] ]; stage(&apos;Copy Artifact&apos;) { for(service in services){ step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true, projectName: service.project, selector: [$class: &apos;StatusBuildSelector&apos;, stable: false], target: dev_path + &apos;/&apos; + service.name ]) } } stage(&apos;Stop Service&apos;) { for(service in services){ sh &apos;fuser -n tcp -k &apos; + service.port + &apos; &gt; redirection &amp;&apos; } } stage(&apos;Start Service&apos;) { for(service in services){ sh &apos;cd &apos; + pass_bin + &apos;/&apos; + service.name + &apos; &amp;&amp; nohup nice java -server -Xms128m -Xmx384m \\ -jar &apos; + service.jarName + &apos;.jar \\ --server.port=&apos; + service.port + &apos; $&gt; initServer.log 2&gt;&amp;1 &amp;&apos; } } } 上面的例子，展示了如何在jenkins pipeline中调用循环语句，实现批量操作。 参数化构建properties([[$class: &apos;JobRestrictionProperty&apos;], parameters([run(description: &apos;&apos;, filter: &apos;ALL&apos;, name: &apos;Name&apos;, projectName: &apos;Project&apos;)]), pipelineTriggers([])] ) 为了能让我们的流水线定义更加具有通用性，除了可以在流水线中使用系统预定义的变量外，可以使用由用户动态输入的变量值。当流水线Job加入参数化后，在执行任务时候就必须有用户输入一系列值才可以执行。 并行node { stage(&apos;Start Service&apos;) { parallel &apos;test&apos;: { echo &apos;test&apos; }, &apos;deply&apos;: { echo &apos;deply&apos; } } parallel &apos;one&apos; : { stage(&apos;one&apos;) { echo &apos;one&apos; } }, &apos;two&apos; : { stage(&apos;two&apos;) { echo &apos;two&apos; } } } parallel &apos;one&apos;: { node{ stage(&apos;one&apos;) { echo &apos;one&apos; } } }, &apos;two&apos;: { node { stage(&apos;two&apos;) { echo &apos;two&apos; } } } Jenkins的流水线同时支持节点（node）、阶段（stage）和步骤（step）之间的并行执行。如果多个节点并发执行的话，并发数量会少于当前可用的执行器（exector）数量。 超时node { stage(&apos;stage2&apos;) { timeout(time: 600, unit: &apos;SECONDS&apos;) { sleep 20 echo &apos;2&apos; } } } 遇到可能执行时间会比较长的情况，可以通过超时来约定最长的执行时间。 对应的实现类为TimeoutStep。 下面介绍的函数在插件workflow-durable-task-step-plugin中，版本信息如下： &lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt; &lt;artifactId&gt;workflow-durable-task-step&lt;/artifactId&gt; &lt;version&gt;2.18-SNAPSHOT&lt;/version&gt; 工作空间当你希望在一个流水线中，对多个工程（例如git工程）做构建以及部署等操作，如果不切换工作空间的话就会发生代码错乱的问题。你可以参考下面的示例代码来解决这个问题： node{ stage(&apos;suren&apos;){ ws(&apos;suren-a-work&apos;) { pwd } ws(&apos;suren-b-work&apos;) { pwd } } } 实现类为WorkspaceStep，使用final修饰，无法扩展。 执行节点Jenkins里可能会配置很多节点（node），而不一定所有的节点都满足你的构建环境要求，这时候就需要来指定节点了： node(&apos;local&apos;) { echo &apos;hello&apos; } properties([ buildDiscarder( logRotator( artifactDaysToKeepStr: &apos;&apos;, artifactNumToKeepStr: &apos;&apos;, daysToKeepStr: &apos;5&apos;, numToKeepStr: &apos;10&apos; ) ), pipelineTriggers([ cron(&apos;H 3,12,17 * * *&apos;) ]) ]) 上面的pipeline指定了运行节点的label为local。 实现类为ExecutorStep，使用final修饰，无法扩展。 异常捕获node{ stage(&apos;suren&apos;){ try{ trigger }catch(error){ echo error.getMessage() } } } 这里调用了一个不存在的流水线函数，然后使用catch来捕获并打印错误信息。 stashnode { stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true) } node(&apos;jenkins-slave&apos;) { unstash(name: &apos;test&apos;) } pipeline的文件存储（stash）这个功能，可以在流水线需要运行在多个节点（node）的情况下使用。stash和unstash会把存储的文件从一个节点转移到另一个节点上。上面给出的例子中，把所有的xml文件从master转移到了当前执行任务的slaver节点上。 node(&apos;jenkins-slave&apos;){ checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]]) } node { def path = JENKINS_HOME + &apos;/jobs/&apos; + JOB_NAME + &apos;/builds/&apos; + BUILD_ID echo path dir(path){ stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true) } } node(&apos;jenkins-slave&apos;) { unstash(name: &apos;test&apos;) } 敏感信息我们可以利用Jenkins的Credentials机制，在Pipeline中传递密码等敏感信息，例如： pipeline { agent any stages{ stage(&apos;test&apos;) { steps{ withCredentials([usernamePassword(credentialsId: &apos;aaa&apos;, passwordVariable: &apos;passwd&apos;, usernameVariable: &apos;user&apos;)]) { sh &apos;&apos;&apos;echo $user $passwd&apos;&apos;&apos; } } } } } 文件读取很多情况下，我们需要读取文件内容。 获取pom.xml版本号，获取groupId（需要的插件Pipeline Utility Steps）： node { stage(&apos;test&apos;){ checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]]) pom = readMavenPom file: &apos;pom.xml&apos; echo pom.version echo pom.groupId echo pom.artifactId } } 使用Docker在docker容器中执行任务： pipeline { agent { docker { image &apos;eclipse/mysql&apos; args &apos;-e MYSQL_ROOT_PASSWORD=root&apos; } } stages { stage(&apos;test&apos;) { steps { sh &apos;mysql&apos; } } } } withDockerContainer withDockerServer dockerFingerprintRun withDockerRegistry dockerFingerprintFrom 其他插件另外有一些比较好的Jenkins流水线插件，给出推荐： 支持从SCM加载库文件 https://github.com/suren-jenkins/workflow-remote-loader-plugin 远程调试安装环境： sudo apt-get install -y npm jenkins-pipeline --file test.groovy --url http://localhost:8080/jenkins/job/MyJob --credentials admin:123456 参考https://github.com/spring-cloud/spring-cloud-pipelines","tags":[{"name":"Pipeline","slug":"Pipeline","permalink":"http://yoursite.com/tags/Pipeline/"}]},{"title":"初探Jenkins X","date":"2018-03-21T13:42:25.000Z","path":"2018/03/21/Preliminary-Jenkins-X.html","text":"Jenkins 于 3月21日 发布了名为Jenkins X的项目，这一项目对开发人员和云端的 CI/CD 环境之间的交互过程进行了审视和反思，结合自动化、工具链以及 DevOps 最佳实践。为开发团队提供了新的生产效率增长点。 Jenkins X是什么？“X”注定是一个不平凡的名字，Jenkins X 对于整个Jenkins生态而言也是不平凡的存在。 简而言之，Jenkins X 是一个高度集成化的CI/CD平台，基于Jenkins和Kubernetes实现，旨在解决微服务体系架构下的云原生应用的持续交付的问题，简化整个云原生应用的开发、运行和部署过程。 你猜的没错，Jenkins X 只能在Kubernetes集群上运行，这有并不意外。Kubernetes已然成为了容器编排的一枝独秀，各大厂商纷纷转向Kubernetes，发布了自己的公有云、操作系统或PaaS平台。 另外，微服务和云原生应用解决方案也日臻成熟，以Spring Boot为代表的一系列体系框架也开始走到舞台中央。 与此同时，随着应用架构的细分和服务间的解耦，服务具备了独立发布的能力，这也使得微服务架构下的持续交付成为业界所关注的热门领域，我们需要更加灵活的CI/CD自动化解决方案，以应对越发快速的交付需求。 注：Jenkins的企业版CloudBees，已经加入CNCF（云原生计算）基金会 看到这里，你是不是觉得Jenkins X 就是个基于Kubernetes的持续交付平台呢？ 那你就大错特错了，因为Jenkins X想要实现的远非如此而已！ 试想如下场景： 越来越多的工具和实践，工程师们需要会写Kubernetes YAML，Dockerfile，Jenkinsfile，对微服务、云原生、Kubernetes和Jenkins非常熟悉。 臣妾做不到呀！ 而在Jenkins X的世界中，这一切都是通过命令完成。 可以说Jenkins X重新思考了未来云原生应用下研发工程师和CI/CD的交付方式，通过整合工具，自动化和DevOps最佳实践，改善了研发过程中的复杂环节，让研发可以专注于价值创造，其他的事情通通交给Jenkins X来帮你解决。 神奇吗？ 的确，在第一次看到项目演示的时候，我也惊叹世界的变化如此之快，在Jenkins X的设计中，整合了Helm，Draft，GitOps，以及Nexus，chartmuseum，monocular等诸多新系统和工具，从而实现自动构建编译环境，生成容器镜像，流水线，自动化部署，并通过简单的Review实现不同环境间的自动发布。 这一切都被完美的封装在简单的jx命令之后。同时你也无需担心对内部实现细节的失控，因为一切都被妥善的版本控制，可以自定义和修改，可以说Jenkins X为你实现了自动化的CI/CD和DevOps最佳实践，持续交付不再是难事，进而提升生产力，实现促进企业的业务成功！ Jenkins X 部分新特性1. 自动化一切：自动化CI/CD流水线 选择项目类型自动生成Jenkinsfile定义流水线 自动生成Dockerfile并打包容器镜像 自动创建Helm Chart并运行在Kubernetes集群 自动关联代码库和流水线，作为代码变更自动触发（基于Webhook实现） 自动版本号自动归档 2. Review代码一键部署应用：基于GitOps的环境部署 所有的环境，应用列表，版本，配置信息统一放在代码库中进行版本控制 通过Pull Request实现研发和运维的协同，完成应用部署升级（Promotion） 可自动部署和手动部署，在必要的时候增加手工Review 当然这些都封装在jx命令中实现 3. 自动生成预览环境和信息同步反馈 预览环境用于代码Review环节中临时创建 同Pull Request工作流程集成并实现信息同步和有效通知 验证完毕后自动清理 提交和应用状态自动同步到Github注释 自动生成release notes信息供验证 Jenkins X 核心组件 JenkinsJenkins X不是一个全新的Jenkins。 他依然使用Jenkins作为持续交付的核心引擎，实际上Jenkins X作为Jenkins的一个子项目存在，专注于云原生应用的CI/CD实现，同时也帮助Jenkins自身完成云原生应用的转型，毕竟现在越来越多的人在诟病单体应用的设计和文件存储系统。 在之前同Jenkins创始人和核心骨干的交流中，我们也了解到Jenkins已经开始着手改变。 HELMHelm是用于管理Kubernetes资源对象的工具，类似APT，YUM和HOMEBREW，他通过将Kubernetes的资源对象打包成Chart的形式，完成复杂应用的部署和版本控制，是目前业界流行的解决方案 DRAFTDraft是自动化应用构建和运行在Kubernetes上面的工具，具有语言识别能力，能够自动生成构建脚本，依赖，环境并打包成docker镜像并部署在Kubernetes集群上，加快代码开发节奏，而无需关心基础设施层面的技术实现 GitOpsGitOps是weaveworks推出的天才的应用部署解决方案，他将Git作为整个应用部署的单一可信数据源（SSOT），通过类似代码开发的Pull Request流程完成应用部署的Review和自动化实现，并且将部署配置信息纳入版本控制。 Jenkins X 安装试用先决条件工具 helm kubectl git Kubernetes 集群 互联网连接 Tiller 公网 IP github 账号 安装 jxhttp://jenkins-x.io/getting-started/install/ 提供了几种系统下的安装说明： OS X：brew tap jenkins-x/jx &amp;&amp; brew tap jenkins-x/jx Linux：curl -L https://github.com/jenkins-x/jx/releases/download/v1.1.10/jx-darwin-amd64.tar.gz | tar xzv &amp;&amp; mv jx /usr/local/bin jx installjx create cluster 支持多种公有云的创建。 配置好集群和对应的 kubeconfig 访问之后，就可以使用jx install进行安装了。 过程中几个需要注意的点： 如果 Tiller 的 SA 权限不足，会导致安装失败，可设置相应的 ClusterRole 进行解决。 安装过程会修改 kubeconfig 文件，因此建议做好备份。 为完整体验功能，建议听从安装器建议，安装 Ingress Controller。 Jenkins X 的环境管理以及代码拉取等功能需要和 Github 进行交互，因此会提问 GitHub 的 Token。 安装过程相对较长，可以使用watch kubectl get pods -n jx查看进程状况。 最后步骤会显示管理密码，注意复制保存。 安装完成JX 会为用户建立三个环境分别是，Dev、Staging 以及 Production。 运行命令jx console，会打开浏览器进入 Jenkins 登录页面。 登录之后我们会看到正在进行构建，如果是一个排队状态，可能是因为正在创建 Worker Pod，可以使用kubectl查询具体情况。 构建完成，会看到这一示例中包含了拉取、构建、Helm、环境等几个步骤，可以作为工作的基础环节来进行使用。 应用接下来就可以做几个善后工作 jx 支持插件，可以通过jx get addons查看支持的插件列表，进行安装。 根据实际工作需要，对缺省环境进行调整，安装所需软件。 对 Jenkins X 中的软件、集群进行安全加固。 使用import或者create spring/create quickstart，进行项目工作。 最后要注意的一点是，Jenkins X 目前的升级频率非常高。不建议生产使用。","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://yoursite.com/tags/Jenkins/"}]},{"title":"浅谈服务治理、微服务与Service Mesh（二） Spring Cloud从入门到精通到放弃【转载】","date":"2018-03-19T14:36:59.000Z","path":"2018/03/19/spring-cloud-start-to-give-up.html","text":"作为本系列文章的第二篇(第一篇链接请戳：浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生)，本文主要为大家介绍下微服务概念中非常火热的Spring Cloud开发框架。由于网上关于Spring Cloud的文章多如牛毛，为了让大家阅读后能有不一样的收获，因此本文将用一个相对轻松的叙述方式来为大家讲解一下Spring Cloud框架和微服务。虽然不可能通过一篇文章让大家对Spring Cloud做到从“入门到精通到放弃”，但是希望大家通过阅读本文能对Spring Cloud和微服务有一个更加清晰的认识和了解，为后面学习Service Mesh做好一个铺垫。 Spring Cloud 之“出身名门望族”作为当下最火热的微服务框架，Spring Cloud的名字可以说是无人不知、无人不晓，凭借之前Spring Framework的良好群众基础和Cloud这个具有时代感的名字，Spring Cloud一出现便被大家认知。 提到Spring Cloud，便会让人想起刚刚发布了2.0版本的Spring Boot。Spring Boot和Spring Cloud都是出自Pivotal公司，Spring Boot和Spring Cloud虽然火热，但是了解Pivotal公司的人在国内却是不多。实际上Pivotal公司在云计算、大数据、虚拟化等领域都有所建树，这里先给大家简单八卦下Pivotal的情况。 Pivotal公司是由EMC和VMware联合成立的一家公司，GE（通用电气）也对Pivotal进行了股权收购，同时GE也是Pivotal的一个重要大客户。除了Spring Framework、Spring Boot和Spring Cloud之外，我们日常开发中经常使用的Reids、RabbitMQ、Greenplum、Gemfire、Cloud Foundry等，目前都是归属于Pivotal公司的产品。其中Gemfire也是被中国铁路总公司12306使用的分布式内存数据库，也就是说你过年回家买不到火车票，这个锅Pivotal的Gemfire也会跟着一起背（开个小玩笑，哈哈）。 Spring Cloud 之“入门”Spring Cloud作为一个微服务的开发框架，其包括了很多的组件，包括：Spring Cloud Netflix（Eureka、Hystrix、Zuul、Archaius）、Spring Cloud Config、Spring Cloud Bus、Spring Cloud Cluster、Spring Cloud Consul、Spring Cloud Security、Spring Cloud Sleuth、Spring Cloud Data Flow、Spring Cloud Stream、Spring Cloud Task、Spring Cloud ZooKeeper、Spring Cloud Connectors、Spring Cloud Starters、Spring Cloud CLI等。 在上述组件中，Spring Cloud Netflix是一套微服务的核心框架，由互联网流媒体播放商Netflix开源后并入Spring Cloud大家庭，它提供了的微服务最基础的功能：服务发现（Service Discovery）、动态路由（Dynamic Routing）、负载均衡（Load Balancing）和边缘服务器（Edge Server）等。 Spring Boot是Spring的一套快速配置脚手架，可以基于Spring Boot快速开发单个微服务。Spring Boot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、生产级别的Spring应用。由于Spring Cloud是基于Spring Boot进行的开发，因此使用Spring Cloud就必须使用到Spring Boot。 下图是一个常见的关于Spring Cloud的架构图。下面此图为例，对Spring Cloud最常用的几个组件做一个简单的介绍： Eureka：服务注册中心，一个基于REST的服务，用于定位服务，以实现微服务架构中服务发现和故障转移。 Hystrix：熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。 Turbine：Turbine是聚合服务器发送事件流数据的一个工具，用来监控集群下Hystrix的Metrics情况。 Zuul：API网关，Zuul是在微服务中提供动态路由、监控、弹性、安全等边缘服务的框架。 Ribbon：提供微服务中的负载均衡功能，有多种负载均衡策略可供选择，可配合服务发现和断路器使用。 Feign：Feign是一种声明式、模板化的HTTP客户端。 Spring Cloud Config：配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。 Spring Cloud Security：基于Spring Security的安全工具包，为微服务的应用程序添加安全控制。 Spring Cloud Sleuth：日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。 除了上面介绍的基础组件外，常见的Spring Cloud组件还有非常多种，涉及到了微服务以及应用开发的方方面面： Spring Cloud Starters：Spring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。 Archaius：配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。 Consul：封装了Consul操作，Consul是一个服务发现与配置工具，与Docker容器可以无缝集成。 Spring Cloud Stream：数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 Spring Cloud CLI：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 Spring Cloud Task：提供云端计划任务管理、任务调度。 Spring Cloud Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与 Spring Cloud Config 联合实现热部署。 Spring Cloud Data Flow：大数据操作工具，作为Spring XD的替代产品，它是一个混合计算模型，结合了流数据与批量数据的处理方式。 Spring Cloud ZooKeeper：操作ZooKeeper的工具包，用于使用ZooKeeper方式的服务发现和配置管理。 Spring Cloud Connectors：便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。 Spring Cloud之“精通”Spring Cloud虽然集成了众多组件，可以构建一个完整的微服务应用，但是其中的各个组件却并非完美无缺，很多组件在实际应用中都存在诸多不足和缺陷。因此，需要我们对其中的一些组件进行替换和修改，方能构建一个强大、灵活、健壮的微服务架构应用。 配置中心Spring Cloud Config可以说是Spring Cloud家族中实现最Low的一个组件，直接采用了本地存储/SVN/Git的方式进行存储。同时，Spring Cloud Config也缺乏一个完整的可视化管理查询后台，当存在比较复杂的权限管理和版本管理需求时，Spring Cloud Config会显得非常力不从心。如果需要在配置修改后，能自动进行配置信息推送的话，使用Spring Cloud Config也无法满足要求，需要自行编写代码进行实现。 目前开源社区中，已经有了很多的开源配置中心实现方案，同时很多公司也自研了自己的配置中心方案。包括淘宝的统一配置中心Diamond（已经多年未更新）、百度的分布式配置管理平台Disconf、携程的开源分布式配置中心Apollo、360的分布式配置管理工具QConf等等。目前，笔者公司采用的是自己公司自研的配置中心，没有采用开源实现的主要原因是因为需要同时适配Spring Cloud和Dubbo等多种场景的应用。 注册中心作为Spring Cloud的服务注册中心，从分布式CAP理论来看，Eureka采用是AP型设计，强调的是注册中心的高可用性。和Dubbo常用的服务注册中心ZooKeeper相比，ZooKeeper则是采用的CP型设计，强调的是注册中心数据的一致性。 Eureka的设计确实简单易用，但是默认没有实现对注册中心数据的持久化。同时，在极端场景下，也会出现多个Eureka注册中心节点数据不一致，甚至服务注册数据丢失的情况。当然，从分布式CAP理论来看，理论上是没办法做到同时兼顾CAP三点的。目前也有一些互联网公司对Eureka进行了改造，支持了数据的持久化，但是尚不能完整的支持CAP的全部要求。 API网关API网关可以说是微服务需求最多，也是最有难点的一个组件。Spring Cloud中集成的Zuul应该说更多的是实现了服务的路由功能，对于负载均衡等其他功能，需要结合Ribbon等组件来实现。对于很多个性化的需求，需要开发者自己来进行编码实现。 和大部分基于Java的Web应用类似，Zuul也采用了Servlet架构，因此Zuul处理每个请求的方式是针对每个请求是用一个线程来处理。同时，由于Zuul是基于JVM的实现，因此性能也会在高并发访问场景下成为瓶颈。虽然网上一些文章评测Zuul和Nginx性能接近，但是在性能要求较高的场景下，JVM的内存管理和垃圾回收问题，仍然是一个很大的问题。所以在实际的应用场景中，通常会采用在多个Zuul几点前面再添加一层Nginx或者OpenResty来进行代理。 为了解决Zuul的性能问题，Netflix将自己的网关服务Zuul进行了升级，新的Zuul 2将HTTP请求的处理方式从同步变成了异步，并且新增诸如HTTP/2、websocket等功能。但是遗憾的是，开源版本的Zuul 2一直处于难产状态中，始终没有和大家正式见面。 熔断器微服务中对于服务的限流、降级、熔断的需求是多种多样的，需要在API网关和各个具体服务接口中分别进行控制，才能满足复杂场景下微服务架构的应用需求。 单独使用Spring Cloud中的Hystrix无法完整的满足上述的复杂需求，需要结合API网关，并通过Kubernetes对资源、进程和命名空间来提供隔离，并通过部分自定义编码方能实现对全部服务的限流、降级、熔断等需求。 监控系统无论是Spring Cloud中集成的Spring Cloud Sleuth，还是集成经典的ELK，都只是对日志级别的追踪和监控。在大中型微服务应用架构中，尤其是基于JVM的项目，还需要添加APM的监控机制，才能保证及时发现各种潜在的性能问题。 APM整体上主要完成3点功能：1.日志追踪、2.监控报警、3.性能统计。目前，国内外商业版本的APM方案已经有很多，开源版本的APM方案也开始丰富起来。国内开源的APM方案主要有：大众点评的CAT和Apache孵化中的SkyWalking。这里给大家重点推荐下SkyWalking，SkyWalking是针对分布式系统的应用性能监控系统，特别针对微服务、Cloud Native和容器化（Docker、Kubernetes、Mesos）架构，项目的关注度和发展速度都很快，中文文档资料也比较齐全。 Spring Cloud之“放弃”Spring Cloud可以说是一个完美的微服务入门框架，如果你是在一个中小型项目中应用Spring Cloud，那么你不需要太多的改造和适配，就可以实现微服务的基本功能。但是如果是在大型项目中实践微服务，可能会发现需要处理的问题还是比较多，尤其是项目中老代码比较多，没办法全部直接升级到Spring Boot框架下开发的话，你会非常希望能有一个侵入性更低的方案来实施微服务架构。在这种场景下，Service Mesh将会成为你的最佳选择，经过一段时间的发展，目前Service Mesh这个概念已经开始逐步被大家了解和认知。同时，一些Service Mesh的实现方案也逐步成熟和落地，例如Istio、Linkerd、Envoy等。在本系列文章的下一篇中，将为大家对Service Mesh概念做一个系统的介绍。但是在了解Service Mesh概念之前，还是建议大家先对微服务和Spring Cloud这些概念和框架有一个深入的了解，这样才能体会到应用Service Mesh的价值和意义。 Spring Cloud与Dubbo网上关于Spring Cloud和Dubbo对比的文章很多，大多数对比结果都是Spring Cloud压倒性优势战胜Dubbo，下表是对Dubbo和Spring Cloud做的一个基础功能的对比： 实际上，Dubbo的关注点在于服务治理，并不能算是一个真正的微服务框架。包括目前在开发中的Dubbo 3.0，也不能完整覆盖微服务的各项功能需求。而Spring Cloud一方面是针对微服务而设计，另外一方面Spring Cloud是通过集成各种组件的方式来实现微服务，因此理论上可以集成目前业内的绝大多数的微服务相关组件，从而实现微服务的全部功能。 而对Dubbo而言，如果一定要应用到微服务的使用场景中的话，上表中欠缺的大多数功能都可以通过集成第三方应用和组件的方式来实现，跟Spring Cloud相比主要的缺陷在于集成过程中的便利性和兼容性等问题。 Spring Cloud与Docker虽然网上也有很多文章写到如何使用Docker来实现微服务，但是事实上单独使用Docker是没办法完整的实现微服务的所有功能的。在实际上微服务架构中，Spring Cloud和Docker更多的是一种协作的关系，而不是一种竞争的关系。通过Docker容器化技术，可以更好的解决引入Spring Cloud微服务后带来的部署和运维的复杂性。 Spring Cloud生态圈中的Pivotal Cloud Foundry（PCF）作为PaaS实现，也提供一些类似于Docker的功能支持，但是无论上功能上还是易用性上和Docker还是存在比较大的差异。Pivotal Cloud Foundry和Docker之间的关系更多的是一种兼容关系，而不是竞争关系，Pivotal Cloud Foundry的主要竞争对手是Red Hat的OpenShift。目前，Pivotal Cloud Foundry支持的IaaS包括：AWS、AZURE、GCP、vSphere、OpenStack等。 Spring Cloud与Kubernetes网上也有一些“Spring Cloud与Kubernetes哪个更好”，“当已经有了Kubernetes之后，还需要使用Spring Cloud么”之类的文章。首先说笔者并不认为Spring Cloud与Kubernetes是竞争关系，但是也不否认二者确实在诸多功能上存在一些重合。下图是对Spring Cloud与Kubernetes在微服务架构中的一些基础功能上的对比： 通过对比可以看出，Spring Cloud和Kubernetes确实存在一些功能上的重合，但是二者的定位其实差别很大。Spring Cloud是一个基于Java语言的微服务开发框架，而Kubernetes是一个针对容器应用的自动化部署、伸缩和管理的开源系统，它兼容多种语言且提供了创建、运行、伸缩以及管理分布式系统的原语。Spring Cloud更多的是面向有Spring开发经验的Java语言开发者，而Kubernetes不是一个针对开发者的平台，它的目的是供有DevOps思想的IT人员使用。 为了区分Spring Cloud和Kubernetes两个项目的范围，下面这张图列出了几乎是端到端的微服务架构需求，从最底层的硬件，到最上层的DevOps和自服务经验，并且列出了如何关联到Spring Cloud和Kubernetes平台。 总结通过Spring Cloud、Docker和Kubernetes的组合，可以构建更加完整和强大的微服务架构程序。通过三者的整合，使用Spring Boot提供应用的打包，Docker和Kubernetes提供应用的部署和调度。Spring Cloud通过Hystrix线程池提供应用内的隔离，而Kubernetes通过资源、进程和命名空间来提供隔离。Spring Cloud为每个微服务提供健康终端，而Kubernetes执行健康检查，且把流量导到健康服务。Spring Cloud外部化配置并更新它们，而Kubernetes分发配置到每个微服务。 对于一名开发人员或者架构师来说，想要精通微服务设计与开发，能够在大中型项目中应用微服务架构，单纯掌握Spring Cloud是远远不够的，Docker和Kubernetes等都是需要学习和掌握的内容。同时，由于采用微服务架构后带来了分布式的相关问题，对于分布式系统理论也必须有一定的了解。当然，最重要的还是对系统业务的深入理解，对整体业务进行合理的规划和拆分，才能真正行之有效的应用微服务架构，构建高效、健壮、灵活、可扩展的微服务应用。 参考链接： https://springcloud.cc https://www.kubernetes.org.cn https://my.oschina.net/u/3677020/blog/1570248 http://blog.csdn.net/rickiyeat/article/details/60792925 http://www.uml.org.cn/wfw/201711271.asp https://projects.spring.io/spring-cloud/","tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生【转载】","date":"2018-03-19T09:44:57.000Z","path":"2018/03/19/dubbo-past-and-present.html","text":"本系列文章将为大家介绍当下最流行的服务治理、微服务等相关内容，从服务治理、SOA、微服务到最新的服务网格（Service Mesh）进行综合介绍和分析。易商阜极自2017年便积极引进微服务的先进理念，运用在项目实践中，为项目集成带来了显著效果。本文将以Dubbo为例，向为大家介绍SOA、服务治理等概念，以及Dubbo的基础知识和最新发展情况。 SOA与服务治理SOA（面向服务的体系结构）概念由来已久，在10多年前便开始进入到我们广大软件开发者的视线中。SOA是一种粗粒度、松耦合服务架构，服务之间通过简单、精确定义接口进行通讯，不涉及底层编程接口和通讯模型。SOA可以看作是B/S模型、Web Service技术之后的自然延伸。 服务治理，也称为SOA治理，是指用来管理SOA的采用和实现的过程。以下是在2006年时IBM对于服务治理要点的总结： 服务定义（服务的范围、接口和边界） 服务部署生命周期（各个生命周期阶段） 服务版本治理（包括兼容性） 服务迁移（启用和退役） 服务注册中心（依赖关系） 服务消息模型（规范数据模型） 服务监视（进行问题确定） 服务所有权（企业组织） 服务测试（重复测试） 服务安全（包括可接受的保护范围） 限于当时的技术发展水平，广大软件设计与开发人员对于SOA和服务治理的技术认知还主要停留在Web Service和ESB总线等技术和规范上，并没有真正在软件开发中得以充分落地。 Dubbo开源直到2011年10月27日，阿里巴巴开源了自己的SOA服务化治理方案的核心框架Dubbo，服务治理和SOA的设计理念开始逐渐在国内软件行业中落地，并被广泛应用。Dubbo作为阿里巴巴内部的SOA服务化治理方案的核心框架，在2012年时已经每天为2000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用，其中既有当当网、网易考拉等互联网公司，也有中国人寿、青岛海尔等传统企业。 Dubbo简介Dubbo是一个高性能服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，使得应用可通过高性能RPC实现服务的输出和输入功能，和Spring框架可以无缝集成。 作为一个分布式服务框架，以及SOA治理方案，Dubbo其功能主要包括：高性能NIO通讯及多协议集成，服务动态寻址与路由，软负载均衡与容错，依赖分析与服务降级等。Dubbo最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。 Dubbo包含远程通讯、集群容错和自动发现三个核心部分。提供透明化的远程方法调用，实现像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入。同时具备软负载均衡及容错机制，可在内网替代F5等硬件负载均衡器，降低成本，减少单点。可以实现服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者。 下图来自从Dubbo官网，描述了服务注册中心、服务提供方、服务消费方、服务监控中心之间的调用关系，具体如下图所示： 节点角色说明： 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 Dubbo总体架构Dubbo框架设计共划分了10层，最上面的Service层是留给实际使用Dubbo开发分布式服务的开发者实现业务逻辑的接口层。图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。 各层说明： Config配置层：对外配置接口，以ServiceConfig、ReferenceConfig为中心，可以直接初始化配置类，也可以通过Spring解析配置生成配置类。 Proxy服务代理层：服务接口透明代理，生成服务的客户端Stub和服务器端Skeleton，以ServiceProxy为中心，扩展接口为ProxyFactory。 Registry注册中心层：封装服务地址的注册与发现，以服务URL为中心，扩展接口为RegistryFactory、Registry、RegistryService。 Cluster路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以Invoker为中心，扩展接口为Cluster、Directory、Router、LoadBalance。 Monitor监控层：RPC调用次数和调用时间监控，以Statistics为中心，扩展接口为MonitorFactory、Monitor、MonitorService。 Protocol远程调用层：封将RPC调用，以Invocation、Result为中心，扩展接口为Protocol、Invoker、Exporter。 Exchange信息交换层：封装请求响应模式，同步转异步，以Request、Response为中心，扩展接口为Exchanger、ExchangeChannel、ExchangeClient、ExchangeServer。 Transport网络传输层：抽象MINA和Netty为统一接口，以Message为中心，扩展接口为Channel、Transporter、Client、Server、Codec。 Serialize数据序列化层：可复用的一些工具，扩展接口为Serialization、ObjectInput、ObjectOutput、ThreadPool。 模块分包 各模块说明： dubbo-common公共逻辑模块：包括Util类和通用模型。 dubbo-remoting远程通讯模块：相当于Dubbo协议的实现，如果RPC用 RMI协议则不需要使用此包。 dubbo-rpc远程调用模块：抽象各种协议，以及动态代理，只包含一对一的调用，不关心集群的管理。 dubbo-cluster集群模块：将多个服务提供方伪装为一个提供方，包括：负载均衡、容错、路由等，集群的地址列表可以是静态配置的，也可以是由注册中心下发。 dubbo-registry注册中心模块：基于注册中心下发地址的集群方式，以及对各种注册中心的抽象。 dubbo-monitor监控模块：统计服务调用次数、调用时间的、调用链跟踪的服务。 dubbo-config配置模块：是Dubbo对外的API，用户通过Config使用Dubbo，隐藏Dubbo所有细节。 dubbo-container容器模块：是一个Standlone的容器，以简单的Main加载Spring启动，因为服务通常不需要Tomcat/JBoss等Web容器的特性，没必要用Web容器去加载服务。 协议支持 Dubbo协议（默认协议） Hessian协议 HTTP协议 RMI协议 WebService协议 Thrift协议 Memcached协议 Redis协议 注册中心（1）Multicast注册中心：Multicast注册中心不需要启动任何中心节点，只要广播地址一样，就可以互相发现。组播受网络结构限制，只适合小规模应用或开发阶段使用。组播地址段：224.0.0.0 - 239.255.255.255。 （2）ZooKeeper注册中心（推荐）：ZooKeeper是Apacahe子项目，是一个树型的目录服务，支持变更推送，适合作为Dubbo服务的注册中心，可用于生产环境。 对上图流程说明如下： 服务提供者（Provider）启动时，向/dubbo/com.foo.BarService/providers目录下写入URL。 服务消费者（Consumer）启动时，订阅/dubbo/com.foo.BarService/providers目录下的URL，向/dubbo/com.foo.BarService/consumers目录下写入自己的URL。 监控中心（Monitor）启动时，订阅/dubbo/com.foo.BarService目录下的所有提供者和消费者URL。 （3）Redis注册中心：阿里内部并没有采用Redis做为注册中心，而是使用自己实现的基于数据库的注册中心，即：Redis注册中心并没有在阿里内部长时间运行的可靠性保障，此Redis桥接实现只为开源版本提供，其可靠性依赖于Redis本身的可靠性。 （4）Simple注册中心：Simple注册中心本身就是一个普通的Dubbo服务，可以减少第三方依赖，使整体通讯方式一致。只是简单实现，不支持集群，可作为自定义注册中心的参考，但不适合直接用于生产环境。 远程通信与信息交换远程通信需要指定通信双方所约定的协议，在保证通信双方理解协议语义的基础上，还要保证高效、稳定的消息传输。Dubbo继承了当前主流的网络通信框架，主要包括如下几个： Mina Netty（默认） Grizzly 停止维护从2012年10月23日Dubbo 2.5.3发布后，在Dubbo开源将满一周年之际，阿里基本停止了对Dubbo的主要升级。只在之后的2013年和2014年更新过2次对Dubbo 2.4的维护版本，然后停止了所有维护工作。Dubbo对Srping的支持也停留在了Spring 2.5.6版本上。 分支出现在阿里停止维护和升级Dubbo期间，当当网开始维护自己的Dubbo分支版本Dubbox，支持了新版本的Spring，并对外开源了Dubbox。同时，网易考拉也维护了自己的独立分支Dubbok，可惜并未对外开源。 重获新生经过多年漫长的等待，随着微服务的火热兴起，在国内外开发者对阿里不再升级维护Dubbo的吐槽声中，阿里终于开始重新对Dubbo的升级和维护工作。在2017年9月7日 ，阿里发布了Dubbo的2.5.4版本，距离上一个版本2.5.3发布已经接近快5年时间了。在随后的几个月中，阿里Dubbo开发团队以差不多每月一版本的速度开始快速升级迭代，修补了Dubbo老版本多年来存在的诸多bug，并对Spring等组件的支持进行了全面升级。 分支合并在2018年1月8日，Dubbo 2.6.0版本发布，新版本将之前当当网开源的Dubbo分支Dubbox进行了合并，实现了Dubbo版本的统一整合。 Dubbo与Spring Cloud阿里巴巴负责主导了 Dubbo 重启维护的研发工程师刘军在接受采访时表示：当前由于 RPC 协议、注册中心元数据不匹配等问题，在面临微服务基础框架选型时Dubbo与Spring Cloud是只能二选一，这也是为什么大家总是拿Dubbo和Spring Cloud做对比的原因之一。Dubbo之后会积极寻求适配到Spring Cloud生态，比如作为Spring Cloud的二进制通信方案来发挥Dubbo的性能优势，或者Dubbo通过模块化以及对http的支持适配到Spring Cloud。 未来展望2018年1月8日，Dubbo创始人之一梁飞在Dubbo交流群里透露了Dubbo 3.0正在动工的消息。Dubbo 3.0内核与Dubbo 2.0完全不同，但兼容Dubbo 2.0。Dubbo 3.0将以Streaming为内核，不再是Dubbo时代的RPC，但是RPC会在Dubbo 3.0中变成远程Streaming对接的一种可选形态。Dubbo 3.0将支持可选Service Mesh，多加一层IPC，这主要是为了兼容老系统，而内部则会优先尝试内嵌模式。代理模式Ops可独立升级框架，减少业务侵入，而内嵌模式可以带业务测试、部署节点少、稳定性检测方便。同时，可以将Dubbo 3.0启动为独立进程，由dubbo-mesh进行IPC，路由、负载均衡和熔断机制将由独立进程控制。 总结从 Dubbo 新版本的路线规划上可以看出，新版本的Dubbo在原有服务治理的功能基础上，将全面拥抱微服务和Service Mesh。同时，考虑到在阿里云已经有了Dubbo的商业版本，在未来一段时间内，Dubbo的更新与维护应该不会再长时间中断。在我们进行服务治理以及微服务架构设计时，新版本Dubbo对我们广大开发者来说都将会是一个不错的选择。 参考链接 http://dubbo.io https://github.com/alibaba/dubbo http://shiyanjun.cn/archives/325.html http://mp.weixin.qq.com/s/eVYx-tUIMYtAk5wP-qkdkw 原文链接：浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生","tags":[{"name":"Dubbo,Service Mesh,微服务,ZooKeeper","slug":"Dubbo-Service-Mesh-微服务-ZooKeeper","permalink":"http://yoursite.com/tags/Dubbo-Service-Mesh-微服务-ZooKeeper/"}]},{"title":"如何选择 Linux 上的跟踪器","date":"2018-03-18T14:38:01.000Z","path":"2018/03/18/choosing-a-linux-tracer.html","text":"tracer是一个高级的性能分析和诊断工具，但是不要让这名词唬住你，如果你使用过 strace 和 tcpdump，其实你就已经使用过跟踪器了。系统跟踪器可以获取更多的系统调用和数据包。它们通常能跟踪任意的内核和应用程序。 有太多的 Linux 跟踪器可以选择。每一种都有其官方的（或非官方的）的卡通的独角兽吉祥物，足够撑起一台”儿童剧”了。 那么我们应该使用哪个跟踪器呢？ 可分为两类：大多数人和性能/内核工程师。 对于大多数人大多数人 (开发者，系统管理员，开发管理者，运维人员，评测人员，等等) 不关心系统追踪器的细节。下面是对于追踪器你应该知道和做的： 1. 使用 perf_events 分析 CPU 性能使用 perf_events 做 CPU 性能分析。性能指标可以使用 flame graph 等工具做可视化。 git clone --depth 1 https://github.com/brendangregg/FlameGraph perf record -F 99 -a -g -- sleep 30 perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; perf.svg Linux perf_events (又称 “perf”，同命令名) 是 Linux 用户的官方跟踪器和性能分析器。内置于内核代码，有很好维护（近来获得快速增强），通常通过 linux-tools-common 软件包安装。 perf 有很多功能，如果只能推荐一个，我选择 CPU 性能分析。尽管这只是采样，而不是从技术上追踪事件。最难的部分是获取完整的栈和信息，我为 java 和 node.js 做的一个演讲 Linux Profiling at Netflix 中已经说过这个问题。 2. 了解其他的跟踪器正如我一个朋友说的：“你不需要知道如何操作 X 射线机器，但是一旦你吞了一枚硬币，你得知道这得去做 X 射线”，你应该了解各种跟踪器都能做什么，这样就能在你工作中真正需要跟踪器的时候，你既可以选择稍后学习使用，也可以雇相应的人来完成。 简短来说：几乎所有的东西都可以使用跟踪器来进行分析和跟踪。如，文件系统内部、TCP/IP 过程、设备驱动、应用程序内部。可以看一下我的个人网站上关于 ftrace 的文章，还有我写的关于 perf_events 文档介绍，可以做为一个追踪（或者性能分析）的例子。 3. 寻求前端支持工具如果你正想买一个能支持跟踪 Linux 的性能分析工具（有许多卖这类工具的公司）。想像一下，只需要直接点击一下界面就能“洞察”整个系统内核，包括隐藏的不同堆栈位置的热图，我在 Monitorama talk 中介绍了一个这样带图形界面的工具。 我开源了一些我自己开发的前端工具，尽管只是命令行界面而不是图形界面。这些工具也会让人们更加快速容易的使用跟踪器。比如下面的例子，用我的 perf_tool，跟踪一个新进程: # ./execsnoop Tracing exec()s. Ctrl-C to end. PID PPID ARGS 22898 22004 man ls 22905 22898 preconv -e UTF-8 22908 22898 pager -s 22907 22898 nroff -mandoc -rLL=164n -rLT=164n -Tutf8 [...] 在 Netflix 上，我们创建了一个 Vector，一个分析工具的实例，同时也是 Linux 上的跟踪器的最终前端。 对于性能/内核工程师我们的工作变的越来越困难，很多的人会问我们怎么样去追踪，哪种跟踪器可以用！为了正确理解一个跟踪器，你经常需要花上至少100个小时才能做到。理解所有的 linux 跟踪器去做出理性的选择是一个浩大的工程。（我可能是唯一一个快做到这件事情的人） 这里是我的建议，可以二选其一： A) 选中一个全能的跟踪器，并且使它标准化，这将涉及花费大量的时间去弄清楚它在测试环境中的细微差别和安全性。我现在推荐 SystemTap 的最新版本（可以从源代码构建）。我知道有些公司已经选用 LTTng，而且他们用的很好，尽管它不是非常的强大（虽然它更安全）。如果 Sysdig 可以增加追踪点tracepoint或者 kprobes，可以做为另一个候选。 B) 遵循我上面提供的流程图，它将意味着尽可能更多的使用 ftrace 或者 perf_event， 并整合 eBPF，之后其他的跟踪器像 SystemTap/LTTng 会去填补剩下的空白。 这就是我目前在 Netflix 做的工作。 对跟踪器的评价1. ftrace我喜欢用 ftrace，它是内核 hacker 的首选，内置于系统内核，可以使用跟踪点（静态检查点），能调用内核 kprobes 和 uprobes 调试工具。并且提供几个这样的功能：带可选过滤器和参数的事件追踪功能；在内核中进行统计的事件计数和定时功能；还有函数流程遍历的功能。可以看一下内核代码中 ftrace.txt 例子了解一下。ftrace 由 /sys 控制，仅支持单一的 root 用户使用（但是你可以通过缓冲区实例改成支持多用户）。某些时候 ftrace 的操作界面非常繁琐，但是的确非常“hack”，而且它有前端界面。ftace 的主要作者 Steven Rostedt 创建了 trace-cmd 命令工具，而我创建了 perf 的工具集。我对这个工具最大的不满就是它不可编程。举例来说，你不能保存和获取时间戳，不能计算延迟，不能把这些计算结果保存成直方图的形式。你需要转储事件至用户层，并且花一些时间去处理结果。ftrace 可以通过 eBPF 变成可编程的。 2. perf_eventsperf_events 是 Linux 用户的主要跟踪工具，它内置在内核源码中，通常通过 linux-tools-commom 安装。也称为“perf”，即其前端工具名称，它通常用来跟踪和转储信息到一个叫做 perf.data 的文件中，perf.data 文件相当于一个动态的缓冲区，用来保存之后需要处理的结果。ftrace 能做到的，perf_events 大都也可以做到，perf-events 不能做函数流程遍历，少了一点儿“hack”劲儿（但是对于安全/错误检查有更好的支持）。它可以进行 CPU 分析和性能统计，用户级堆栈解析，也可以使用对于跟踪每行局部变量产生的调试信息。它也支持多用户并发操作。和 ftrace 一样也不支持可编程。如果要我只推荐一款跟踪器，那一定是 perf 了。它能解决众多问题，并且它相对较安全。 3. eBPFextended Berkeley Packet Filter（eBPF）是一个可以在事件上运行程序的高效内核虚拟机（JIT）。它可能最终会提供 ftrace 和 perf_events 的内核编程，并强化其他的跟踪器。这是 Alexei Starovoitov 目前正在开发的，还没有完全集成，但是从4.1开始已经对一些优秀的工具有足够的内核支持了，如块设备 I/O 的延迟热图。可参考其主要作者 Alexei Starovoitov 的 BPF slides 和 eBPF samples。 4. SystemTapSystemTap 是最强大的跟踪器。它能做所有事情，如概要分析，跟踪点，探针，uprobes（来自SystemTap），USDT 和内核编程等。它将程序编译为内核模块，然后加载，这是一种获取安全的巧妙做法。它也是从 tree 发展而来，过去有很多问题（崩溃或冻结）。很多不是 SystemTap 本身的错——它常常是第一个使用某个内核追踪功能，也是第一个碰到 bug 的。SystemTap 的最新版本好多了（必须由源代码编译），但是很多人仍然会被早期版本吓到。如果你想用它，可先在测试环境中使用，并与 irc.freenode.net 上 的 #systemtap 开发人员交流。（Netflix 有容错机制，我们已经使用了 SystemTap，但是可能我们考虑的安全方面的问题比你们少。）我最大的不满是，它似乎认为你应该有内核 debug 信息，但是经常没有。实际上没有它也能做很多事情，但是缺少文档和例子（我必须自己全靠自己开始学习）。 5. LTTngLTTng 优化了事件采集，这比其他跟踪器做得好，它也支持几种事件类型，包括 USTD。它从 tree 发展而来，它的核心很简单：通过一组小规模的固定指令集将事件写入追踪缓冲区，这种方式使它安全、快速，缺点是它没有内核编码的简单途径。我一直听说这不是一个大问题，因为尽管需要后期处理，它也已经优化到可以充分的度量。此外，它还首创了一个不同的分析技术，对所有关注事件的更多黑盒记录将能够稍后以 GUI 的方式进行研究。我关心的是前期没有考虑到要录制的事件缺失问题如何解决，但我真正要做的是花更多时间来看它在实践中用的怎么样。这是我花的时间最少的一个跟踪器（没有什么特殊原因）。 6. Ktapktap 是一款前景很好的跟踪器，它使用内核中的 lua 虚拟机处理，在没有调试信息的情况下在嵌入式设备上运行的很好。这让它得到了关注，并在有一段时间似乎超过了 Linux 上所有的追踪器。然后 eBPF 开始集成到内核了，而 ktap 的集成会在可以使用 eBPF 替代它自己的虚拟机后才开始。因为 eBPF 仍将持续集成几个月，ktap 开发者要继续等上一段时间。我希望今年晚些时候它能重新开发。 7. dtrace4linuxdtrace4linux 主要是 Paul Fox 一个人在业余时间完成的，它是 Sun DTrace 的 Linux 版本。它引人瞩目，已经有一些供应器provider可以工作，但是从某种程度上来说还不完整，更多的是一种实验性的工具（不安全）。我认为，顾忌到许可证问题，人们会小心翼翼的为 dtrace4linux 贡献代码：由于当年 Sun 开源DTrace 使用的是 CDDL 协议，而 dtrace4linux 也不大可能最终进入 Linux kernel。Paul 的方法很可能会使其成为一个 add-on。我很乐意看到 Linux 平台上的 DTrace 和这个项目的完成，我认为当我加入 Netflix 后将会花些时间来协助完成这个项目。然而，我还是要继续使用内置的跟踪器，如 ftrace 和 perf_events。 8. OL DTraceOracle Linux DTrace 为了将 DTrace 引入 Linux，特别是为 Oracle Linux，做出了很大的努力。这些年来发布的多个版本表明了它的稳定进展。开发者们以一种对这个项目的前景看好的态度谈论着改进 DTrace 测试套件。很多有用的 供应器provider 已经完成了，如：syscall, profile, sdt, proc, sched 以及 USDT。我很期待 fbt（function boundary tracing，用于内核动态跟踪）的完成，它是 Linux 内核上非常棒的 供应器provider。OL DTrace 最终的成功将取决于人们对运行 Oracle Linux（为技术支持付费）有多大兴趣，另一方面取决于它是否完全开源：它的内核元件是开源的，而我没有看到它的用户级别代码。 9. sysdigsysdig 是一个使用类 tcpdump 语法在系统事件上操作的新跟踪器，它使用 lua 进行后期处理。它很优秀，它见证了系统跟踪领域的变革。它的局限性在于它只在当前进行系统调用，将所有事件转储为用户级别用于后期处理。你可以使用系统调用做很多事情，然而我还是很希望它能支持跟踪点、kprobe 和 uprobe。我还期待它能支持 eBPF 做内核摘要。目前，sysdig 开发者正在增加容器支持。留意这些内容。","tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]}]