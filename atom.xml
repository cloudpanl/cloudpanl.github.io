<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>每天进步一点点……</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-03-27T14:42:13.262Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Cloudpanl</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>微服务入门系列(二)：微服务架构下的分布式事务基础入门【转载】</title>
    <link href="http://yoursite.com/2018/03/26/Distributed-transaction-basics.html"/>
    <id>http://yoursite.com/2018/03/26/Distributed-transaction-basics.html</id>
    <published>2018-03-26T10:04:42.000Z</published>
    <updated>2018-03-27T14:42:13.262Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现了跨多个数据库的事务需求，这种事务即为“分布式事务”。那么在目前数据库不支持跨库事务的情况下，我们应该如何实现分布式事务呢？本文首先会为大家梳理分布式事务的基本概念和理论基础，然后介绍几种目前常用的分布式事务解决方案。废话不多说，那就开始吧～</p></blockquote><h1 id="1-什么是事务？"><a href="#1-什么是事务？" class="headerlink" title="1. 什么是事务？"></a>1. 什么是事务？</h1><p>事务由一组操作构成，我们希望这组操作能够全部正确执行，如果这一组操作中的任意一个步骤发生错误，那么就需要回滚之前已经完成的操作。也就是同一个事务中的所有操作，要么全都正确执行，要么全都不要执行。</p><h1 id="2-事务的四大特性-ACID"><a href="#2-事务的四大特性-ACID" class="headerlink" title="2. 事务的四大特性 ACID"></a>2. 事务的四大特性 ACID</h1><p>说到事务，就不得不提一下事务著名的四大特性。</p><p>1、原子性</p><p>原子性要求，事务是一个不可分割的执行单元，事务中的所有操作要么全都执行，要么全都不执行。</p><p>2、一致性</p><p>一致性要求，事务在开始前和结束后，数据库的完整性约束没有被破坏。</p><p>3、隔离性</p><p>事务的执行是相互独立的，它们不会相互干扰，一个事务不会看到另一个正在运行过程中的事务的数据。</p><p>4、持久性</p><p>持久性要求，一个事务完成之后，事务的执行结果必须是持久化保存的。即使数据库发生崩溃，在数据库恢复后事务提交的结果仍然不会丢失。</p><blockquote><p>注意：事务只能保证数据库的高可靠性，即数据库本身发生问题后，事务提交后的数据仍然能恢复；而如果不是数据库本身的故障，如硬盘损坏了，那么事务提交的数据可能就丢失了。这属于『高可用性』的范畴。因此，事务只能保证数据库的『高可靠性』，而『高可用性』需要整个系统共同配合实现。</p></blockquote><h1 id="3-事务的隔离级别"><a href="#3-事务的隔离级别" class="headerlink" title="3. 事务的隔离级别"></a>3. 事务的隔离级别</h1><p>这里扩展一下，对事务的<strong>隔离性</strong>做一个详细的解释。</p><p>在事务的四大特性ACID中，要求的隔离性是一种严格意义上的隔离，也就是多个事务是串行执行的，彼此之间不会受到任何干扰。这确实能够完全保证数据的安全性，但在实际业务系统中，这种方式性能不高。因此，数据库定义了四种隔离级别，隔离级别和数据库的性能是呈反比的，隔离级别越低，数据库性能越高，而隔离级别越高，数据库性能越差。</p><h2 id="3-1-事务并发执行会出现的问题"><a href="#3-1-事务并发执行会出现的问题" class="headerlink" title="3.1 事务并发执行会出现的问题"></a>3.1 事务并发执行会出现的问题</h2><p>我们先来看一下在不同的隔离级别下，数据库可能会出现的问题：</p><p>1、更新丢失</p><p>当有两个并发执行的事务，更新同一行数据，那么有可能一个事务会把另一个事务的更新覆盖掉。</p><p>当数据库没有加任何锁操作的情况下会发生。</p><p>2、脏读</p><p>一个事务读到另一个尚未提交的事务中的数据。</p><p>该数据可能会被回滚从而失效。 </p><p>如果第一个事务拿着失效的数据去处理那就发生错误了。</p><p>3、不可重复读</p><p>不可重复度的含义：一个事务对同一行数据读了两次，却得到了不同的结果。它具体分为如下两种情况：</p><ul><li>虚读：在事务1两次读取同一记录的过程中，事务2对该记录进行了修改，从而事务1第二次读到了不一样的记录。</li><li>幻读：事务1在两次查询的过程中，事务2对该表进行了插入、删除操作，从而事务1第二次查询的结果发生了变化。</li></ul><blockquote><p>不可重复读 与 脏读 的区别？<br>脏读读到的是尚未提交的数据，而不可重复读读到的是已经提交的数据，只不过在两次读的过程中数据被另一个事务改过了。</p></blockquote><h2 id="3-2-数据库的四种隔离级别"><a href="#3-2-数据库的四种隔离级别" class="headerlink" title="3.2 数据库的四种隔离级别"></a>3.2 数据库的四种隔离级别</h2><p>数据库一共有如下四种隔离级别：</p><p>1、Read uncommitted 读未提交</p><p>在该级别下，一个事务对一行数据修改的过程中，不允许另一个事务对该行数据进行修改，但允许另一个事务对该行数据读。 </p><p>因此本级别下，不会出现更新丢失，但会出现脏读、不可重复读。</p><p>2、Read committed 读提交</p><p>在该级别下，未提交的写事务不允许其他事务访问该行，因此不会出现脏读；但是读取数据的事务允许其他事务的访问该行数据，因此会出现不可重复读的情况。</p><p>3、Repeatable read 重复读 </p><p>在该级别下，读事务禁止写事务，但允许读事务，因此不会出现同一事务两次读到不同的数据的情况（不可重复读），且写事务禁止其他一切事务。</p><p>4、Serializable 序列化</p><p>该级别要求所有事务都必须串行执行，因此能避免一切因并发引起的问题，但效率很低。</p><p>隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。</p><h1 id="4-什么是分布式事务？"><a href="#4-什么是分布式事务？" class="headerlink" title="4. 什么是分布式事务？"></a>4. 什么是分布式事务？</h1><p>到此为止，所介绍的事务都是基于单数据库的本地事务，目前的数据库仅支持单库事务，并不支持跨库事务。而随着微服务架构的普及，一个大型业务系统往往由若干个子系统构成，这些子系统又拥有各自独立的数据库。往往一个业务流程需要由多个子系统共同完成，而且这些操作可能需要在一个事务中完成。在微服务系统中，这些业务场景是普遍存在的。此时，我们就需要在数据库之上通过某种手段，实现支持跨数据库的事务支持，这也就是大家常说的“分布式事务”。</p><p>这里举一个分布式事务的典型例子——用户下单过程。</p><p>当我们的系统采用了微服务架构后，一个电商系统往往被拆分成如下几个子系统：商品系统、订单系统、支付系统、积分系统等。整个下单的过程如下：</p><ol><li>用户通过商品系统浏览商品，他看中了某一项商品，便点击下单</li><li>此时订单系统会生成一条订单</li><li>订单创建成功后，支付系统提供支付功能</li><li>当支付完成后，由积分系统为该用户增加积分</li></ol><p>上述步骤2、3、4需要在一个事务中完成。对于传统单体应用而言，实现事务非常简单，只需将这三个步骤放在一个方法A中，再用Spring的@Transactional注解标识该方法即可。Spring通过数据库的事务支持，保证这些步骤要么全都执行完成，要么全都不执行。但在这个微服务架构中，这三个步骤涉及三个系统，涉及三个数据库，此时我们必须在数据库和应用系统之间，通过某项黑科技，实现分布式事务的支持。</p><h1 id="5-CAP理论"><a href="#5-CAP理论" class="headerlink" title="5. CAP理论"></a>5. CAP理论</h1><p>CAP理论说的是：在一个分布式系统中，最多只能满足C、A、P中的两个需求。</p><p>CAP的含义：</p><ul><li>C：Consistency 一致性</li></ul><p>同一数据的多个副本是否实时相同。</p><ul><li>A：Availability 可用性</li></ul><p>可用性：一定时间内 &amp; 系统返回一个明确的结果 则称为该系统可用。</p><ul><li>P：Partition tolerance 分区容错性</li></ul><p>将同一服务分布在多个系统中，从而保证某一个系统宕机，仍然有其他系统提供相同的服务。</p><p>CAP理论告诉我们，在分布式系统中，C、A、P三个条件中我们最多只能选择两个。那么问题来了，究竟选择哪两个条件较为合适呢？</p><p>对于一个业务系统来说，可用性和分区容错性是必须要满足的两个条件，并且这两者是相辅相成的。业务系统之所以使用分布式系统，主要原因有两个：</p><ul><li>提升整体性能</li></ul><p>当业务量猛增，单个服务器已经无法满足我们的业务需求的时候，就需要使用分布式系统，使用多个节点提供相同的功能，从而整体上提升系统的性能，这就是使用分布式系统的第一个原因。</p><ul><li>实现分区容错性</li></ul><p>单一节点 或 多个节点处于相同的网络环境下，那么会存在一定的风险，万一该机房断电、该地区发生自然灾害，那么业务系统就全面瘫痪了。为了防止这一问题，采用分布式系统，将多个子系统分布在不同的地域、不同的机房中，从而保证系统高可用性。</p><p>这说明分区容错性是分布式系统的根本，如果分区容错性不能满足，那使用分布式系统将失去意义。</p><p>此外，可用性对业务系统也尤为重要。在大谈用户体验的今天，如果业务系统时常出现“系统异常”、响应时间过长等情况，这使得用户对系统的好感度大打折扣，在互联网行业竞争激烈的今天，相同领域的竞争者不甚枚举，系统的间歇性不可用会立马导致用户流向竞争对手。因此，我们只能通过牺牲一致性来换取系统的<strong>可用性</strong>和<strong>分区容错性</strong>。这也就是下面要介绍的BASE理论。</p><h1 id="6-BASE理论"><a href="#6-BASE理论" class="headerlink" title="6. BASE理论"></a>6. BASE理论</h1><p>CAP理论告诉我们一个悲惨但不得不接受的事实——我们只能在C、A、P中选择两个条件。而对于业务系统而言，我们往往选择牺牲一致性来换取系统的可用性和分区容错性。不过这里要指出的是，所谓的“牺牲一致性”并不是完全放弃数据一致性，而是牺牲<strong>强一致性</strong>换取<strong>弱一致性</strong>。下面来介绍下BASE理论。</p><ul><li>BA：Basic Available 基本可用</li></ul><blockquote><ul><li>“一定时间”可以适当延长</li></ul><p>当举行大促时，响应时间可以适当延长</p><ul><li>给部分用户返回一个降级页面</li></ul><p>给部分用户直接返回一个降级页面，从而缓解服务器压力。但要注意，返回降级页面仍然是返回明确结果。</p><ul><li>整个系统在某些不可抗力的情况下，仍然能够保证“可用性”，即一定时间内仍然能够返回一个明确的结果。只不过“基本可用”和“高可用”的区别是：</li></ul></blockquote><ul><li>S：Soft State：柔性状态</li></ul><p>同一数据的不同副本的状态，可以不需要实时一致。</p><ul><li>E：Eventual Consisstency：最终一致性</li></ul><p>同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后仍然是一致的。</p><h1 id="7-酸碱平衡"><a href="#7-酸碱平衡" class="headerlink" title="7. 酸碱平衡"></a>7. 酸碱平衡</h1><p>ACID能够保证事务的强一致性，即数据是实时一致的。这在本地事务中是没有问题的，在分布式事务中，强一致性会极大影响分布式系统的性能，因此分布式系统中遵循BASE理论即可。但分布式系统的不同业务场景对一致性的要求也不同。如交易场景下，就要求强一致性，此时就需要遵循ACID理论，而在注册成功后发送短信验证码等场景下，并不需要实时一致，因此遵循BASE理论即可。因此要根据具体业务场景，在ACID和BASE之间寻求平衡。</p><h1 id="8-分布式事务协议"><a href="#8-分布式事务协议" class="headerlink" title="8. 分布式事务协议"></a>8. 分布式事务协议</h1><p>下面介绍几种实现分布式事务的协议。</p><h2 id="8-1-两阶段提交协议-2PC"><a href="#8-1-两阶段提交协议-2PC" class="headerlink" title="8.1 两阶段提交协议 2PC"></a>8.1 两阶段提交协议 2PC</h2><p>分布式系统的一个难点是如何保证架构下多个节点在进行事务性操作的时候保持一致性。为实现这个目的，二阶段提交算法的成立基于以下假设：</p><ul><li>该分布式系统中，存在一个节点作为协调者(Coordinator)，其他节点作为参与者(Cohorts)。且节点之间可以进行网络通信。</li><li>所有节点都采用预写式日志，且日志被写入后即被保持在可靠的存储设备上，即使节点损坏不会导致日志数据的消失。</li><li>所有节点不会永久性损坏，即使损坏后仍然可以恢复。</li></ul><h3 id="1-第一阶段（投票阶段）："><a href="#1-第一阶段（投票阶段）：" class="headerlink" title="1. 第一阶段（投票阶段）："></a>1. 第一阶段（投票阶段）：</h3><ol><li>协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。</li><li>参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）</li><li>各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。</li></ol><h3 id="2-第二阶段（提交执行阶段）："><a href="#2-第二阶段（提交执行阶段）：" class="headerlink" title="2. 第二阶段（提交执行阶段）："></a>2. 第二阶段（提交执行阶段）：</h3><p>当协调者节点从所有参与者节点获得的相应消息都为”同意”时：</p><ol><li>协调者节点向所有参与者节点发出”正式提交(commit)”的请求。</li><li>参与者节点正式完成操作，并释放在整个事务期间内占用的资源。</li><li>参与者节点向协调者节点发送”完成”消息。</li><li>协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。</li></ol><p>如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：</p><ol><li>协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。</li><li>参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。</li><li>参与者节点向协调者节点发送”回滚完成”消息。</li><li>协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。</li></ol><p>不管最后结果如何，第二阶段都会结束当前事务。</p><p>二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的：</p><ol><li>执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</li><li>参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。（没有多少容错机制）</li><li>协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。（这个可以依赖后面要讲的Paxos协议实现HA）</li><li>二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</li></ol><p>为此，Dale Skeen和Michael Stonebraker在“A Formal Model of Crash Recovery in a Distributed System”中提出了三阶段提交协议（3PC）。</p><h2 id="8-2-三阶段提交协议-3PC"><a href="#8-2-三阶段提交协议-3PC" class="headerlink" title="8.2 三阶段提交协议 3PC"></a>8.2 三阶段提交协议 3PC</h2><p>与两阶段提交不同的是，三阶段提交有两个改动点。</p><ul><li>引入超时机制。同时在协调者和参与者中都引入超时机制。</li><li>在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。</li></ul><p>也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。</p><h3 id="1-CanCommit阶段"><a href="#1-CanCommit阶段" class="headerlink" title="1. CanCommit阶段"></a>1. CanCommit阶段</h3><p>3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。</p><p>1、事务询问</p><p>协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。</p><p>2、响应反馈</p><p>参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No</p><h3 id="2-PreCommit阶段"><a href="#2-PreCommit阶段" class="headerlink" title="2. PreCommit阶段"></a>2. PreCommit阶段</h3><p>协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。</p><p>假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。</p><p>1、发送预提交请求</p><p>协调者向参与者发送PreCommit请求，并进入Prepared阶段。</p><p>2、事务预提交</p><p>参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。</p><p>3、响应反馈  </p><p>如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。</p><p>假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。</p><p>1、发送中断请求</p><p>协调者向所有参与者发送abort请求。</p><p>2、中断事务  </p><p>参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。</p><h3 id="3-doCommit阶段"><a href="#3-doCommit阶段" class="headerlink" title="3. doCommit阶段"></a>3. doCommit阶段</h3><p>该阶段进行真正的事务提交，也可以分为以下两种情况。</p><p><strong>3.1 执行提交</strong></p><p>1、发送提交请求</p><p>协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。</p><p>2、事务提交</p><p>参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。</p><p>3、响应反馈</p><p>事务提交完之后，向协调者发送Ack响应。</p><p>4、完成事务 </p><p>协调者接收到所有参与者的ack响应之后，完成事务。</p><p>3.2 中断事务 </p><p>协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。</p><p>1、发送中断请求 </p><p>协调者向所有参与者发送abort请求</p><p>2、事务回滚 </p><p>参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。</p><p>3、反馈结果 </p><p>参与者完成事务回滚之后，向协调者发送ACK消息</p><p>4、中断事务 </p><p>协调者接收到参与者反馈的ACK消息之后，执行事务的中断。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;众所周知，数据库能实现本地事务，也就是在同一个数据库中，你可以允许一组操作要么全都正确执行，要么全都不执行。这里特别强调了本地事务，也就是目前的数据库只能支持同一个数据库中的事务。但现在的系统往往采用微服务架构，业务系统拥有独立的数据库，因此就出现
      
    
    </summary>
    
      <category term="微服务" scheme="http://yoursite.com/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    
      <category term="微服务入门系列" scheme="http://yoursite.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>微服务入门系列(一)：走进微服务的世界【转载】</title>
    <link href="http://yoursite.com/2018/03/25/Into-Microservices.html"/>
    <id>http://yoursite.com/2018/03/25/Into-Microservices.html</id>
    <published>2018-03-25T03:55:31.000Z</published>
    <updated>2018-03-27T14:36:23.457Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是微服务？"><a href="#1-什么是微服务？" class="headerlink" title="1. 什么是微服务？"></a>1. 什么是微服务？</h1><p>我们首先给出微服务的定义，然后再对该定义给出详细的解释。</p><blockquote><p>微服务就是一些可独立运行、可协同工作的小的服务。</p></blockquote><p>从概念中我们可以提取三个关键词：可独立运行、可协同工作、小。这三个词高度概括了微服务的核心特性。下面我们就对这三个词作详细解释。</p><p>1、可独立运行</p><p>微服务是一个个可以独立开发、独立部署、独立运行的系统或者进程。</p><p>2、可协同工作</p><p>采用了微服务架构后，整个系统被拆分成多个微服务，这些服务之间往往不是完全独立的，在业务上存在一定的耦合，即一个服务可能需要使用另一个服务所提供的功能。这就是所谓的“可协同工作”。与单服务应用不同的是，多个微服务之间的调用时通过RPC通信来实现，而非单服务的本地调用，所以通信的成本相对要高一些，但带来的好处也是可观的。</p><p>3、小而美</p><p>微服务的思想是，将一个拥有复杂功能的庞大系统，按照业务功能，拆分成多个相互独立的子系统，这些子系统则被称为“微服务”。每个微服务只承担某一项职责，从而相对于单服务应用来说，微服务的体积是“小”的。小也就意味着每个服务承担的职责变少，根据单一职责原则，我们在系统设计时，要尽量使得每一项服务只承担一项职责，从而实现系统的“高内聚”。</p><h1 id="2-微服务的优点"><a href="#2-微服务的优点" class="headerlink" title="2. 微服务的优点"></a>2. 微服务的优点</h1><h2 id="1-易于扩展"><a href="#1-易于扩展" class="headerlink" title="1. 易于扩展"></a>1. 易于扩展</h2><p>在单服务应用中，如果目前性能到达瓶颈，无法支撑目前的业务量，此时一般采用集群模式，即增加服务器集群的节点，并将这个单服务应用“复制”到所有的节点上，从而提升整体性能。然而这种扩展的粒度是比较粗糙的。如果只是系统中某一小部分存在性能问题，在单服务应用中，也要将整个应用进行扩展，这种方式简单粗暴，无法对症下药。而当我们使用了微服务架构后，如果某一项服务的性能到达瓶颈，那么我们只需要增加该服务的节点数即可，其他服务无需变化。这种扩展更加具有针对性，能够充分利用计算机硬件/软件资源。而且只扩展单个服务影响的范围较小，从而系统出错的概率也就越低。</p><h2 id="2-部署简单"><a href="#2-部署简单" class="headerlink" title="2. 部署简单"></a>2. 部署简单</h2><p>对于单服务应用而言，所有代码均在一个项目中，从而导致任何微小的改变都需要将整个项目打包、发布、部署，而这一系列操作的代价是高昂的。长此以往，团队为了降低发布的频率，会使得每次发布都伴随着大量的修改，修改越多也就意味着出错的概率也越大。<br>当我们采用微服务架构以后，每个服务只承担少数职责，从而每次只需要发布发生修改的系统，其他系统依然能够正常运行，波及范围较小。此外，相对于单服务应用而言，每个微服务系统修改的代码相对较少，从而部署后出现错误的概率也相对较低。</p><h2 id="3-技术异构性"><a href="#3-技术异构性" class="headerlink" title="3. 技术异构性"></a>3. 技术异构性</h2><p>对于单服务应用而言，一个系统的所有模块均整合在一个项目中，所以这些模块只能选择相同的技术。但有些时候，单一技术没办法满足不同的业务需求。如对于项目的算法团队而言，函数试编程语言可能更适合算法的开发，而对于业务开发团队而言，类似于Java的强类型语言具有更高的稳定性。然而在单服务应用中只能互相权衡，选择同一种语言，而当我们使用微服务结构后，这个问题就能够引刃而解。我们将一个完整的系统拆分成了多个独立的服务，从而每个服务都可以根据各自不同的特点，选择最为合适的技术体系。</p><p>当然，并不是所有的微服务系统都具备技术异构性，要实现技术异构性，必须保证所有服务都提供通用接口。我们知道，在微服务系统中，服务之间采用RPC接口通信，而实现RPC通信的方式有很多。有一些RPC通信方式与语言强耦合，如Java的RMI技术，它就要求通信的双方都必须采用Java语言开发。当然，也有一些RPC通信方式与语言无关，如基于HTTP协议的REST。这种通信方式对通信双方所采用的语言没有做任何限制，只要通信过程中传输的数据遵循REST规范即可。当然，与语言无关也就意味着通信双方没有类型检查，从而会提高出错的概率。所以，究竟选择与语言无关的RPC通信方式，还是选择与语言强耦合的RPC通信方式，需要我们根据实际的业务场景合理地分析。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-什么是微服务？&quot;&gt;&lt;a href=&quot;#1-什么是微服务？&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是微服务？&quot;&gt;&lt;/a&gt;1. 什么是微服务？&lt;/h1&gt;&lt;p&gt;我们首先给出微服务的定义，然后再对该定义给出详细的解释。&lt;/p&gt;
&lt;blockq
      
    
    </summary>
    
      <category term="微服务" scheme="http://yoursite.com/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    
      <category term="微服务入门系列" scheme="http://yoursite.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes中的亲和性【转载】</title>
    <link href="http://yoursite.com/2018/03/24/Affinity-in-Kubernetes.html"/>
    <id>http://yoursite.com/2018/03/24/Affinity-in-Kubernetes.html</id>
    <published>2018-03-24T02:12:08.000Z</published>
    <updated>2018-03-24T03:20:43.564Z</updated>
    
    <content type="html"><![CDATA[<p>现实中应用的运行对于kubernetes在亲和性上提出了一些要求，可以归类到以下几个方面： </p><ol><li>Pod固定调度到某些节点之上 </li><li>Pod不会调度到某些节点之上 </li><li>Pod的多副本调度到相同的节点之上 </li><li>Pod的多副本调度到不同的节点之上</li></ol><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><p>下面我们将通过例子的方式来说明在kubernetes需要去设置亲和性实现上面要求．</p><h2 id="Pod调动到某些节点上"><a href="#Pod调动到某些节点上" class="headerlink" title="Pod调动到某些节点上"></a>Pod调动到某些节点上</h2><p>Pod的定义中通过nodeSelector指定label标签，pod将会只调度到具有该标签的node之上</p><pre><code>apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  nodeSelector:    disktype: ssd</code></pre><p>这个例子中pod只会调度到具有disktype=ssd的node上面．</p><h2 id="节点亲和性-反亲和性"><a href="#节点亲和性-反亲和性" class="headerlink" title="节点亲和性/反亲和性"></a>节点亲和性/反亲和性</h2><p>Affinity/anti-affinity node 相对于nodeSelector机制更加的灵活和丰富</p><ul><li>表达的语法：支持In,NotIn,Exists,DoesNotExist,Gt,Lt．</li><li>支持soft(preference)和hard(requirement),hard表示pod sheduler到某个node上，则必须满足亲和性设置．soft表示scheduler的时候，无法满足节点的时候，会选择非nodeSelector匹配的节点．</li><li><p>nodeAffinity的基础上添加多个nodeSelectorTerms字段，调度的时候Node只需要nodeSelectorTerms中的某一个符合条件就符合nodeAffinity的规则．在nodeSelectorTerms中添加matchExpressions，需要可以调度的Node是满足matchExpressions中表示的所有规则．</p><pre><code>apiVersion: v1kind: Podmetadata:  name: with-node-affinityspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: kubernetes.io/e2e-az-name            operator: In            values:            - e2e-az1            - e2e-az2      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: another-node-label-key            operator: In            values:            - another-node-label-value  containers:  - name: with-node-affinity    image: k8s.gcr.io/pause:2.0</code></pre></li></ul><h2 id="Pod间的亲和性和反亲和性"><a href="#Pod间的亲和性和反亲和性" class="headerlink" title="Pod间的亲和性和反亲和性"></a>Pod间的亲和性和反亲和性</h2><p>基于已经运行在Node 上pod的labels来决定需要新创建的Pods是否可以调度到node节点上，配置的时候可以指定那个namespace中的pod需要满足pod的亲和性．可以通过<code>topologyKey</code>来指定topology domain, 可以指定为node／cloud provider zone／cloud provider region的范围．　</p><ul><li>表达的语法：支持In, NotIn, Exists, DoesNotExist</li><li>Pod的亲和性和反亲和性可以分成<ol><li>requiredDuringSchedulingIgnoredDuringExecution　#硬要求 </li><li>preferredDuringSchedulingIgnoredDuringExecution　＃软要求 </li></ol></li></ul><p>类似上面node的亲和策略类似，<code>requiredDuringSchedulingIgnoredDuringExecution</code>亲和性可以用于约束不同服务的pod在同一个topology domain的Nod上．<code>preferredDuringSchedulingIgnoredDuringExecution</code>反亲和性可以将服务的pod分散到不同的topology domain的Node上．</p><ul><li>topologyKey可以设置成如下几种类型 <ol><li>kubernetes.io/hostname　　＃Node</li><li>failure-domain.beta.kubernetes.io/zone　＃Zone </li><li>failure-domain.beta.kubernetes.io/region #Region </li></ol></li></ul><p>可以设置node上的label的值来表示node的name,zone,region等信息，pod的规则中指定topologykey的值表示指定topology范围内的node上运行的pod满足指定规则</p><pre><code>apiVersion: v1kind: Podmetadata:  name: with-pod-affinityspec:  affinity:    podAffinity:      requiredDuringSchedulingIgnoredDuringExecution:      - labelSelector:          matchExpressions:          - key: security            operator: In            values:            - S1        topologyKey: failure-domain.beta.kubernetes.io/zone    podAntiAffinity:      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 100        podAffinityTerm:          labelSelector:            matchExpressions:            - key: security              operator: In              values:              - S2          topologyKey: kubernetes.io/hostname  containers:  - name: with-pod-affinity    image: k8s.gcr.io/pause:2.0</code></pre><p>利用社区官方的例子来进一步的说明，例子中指定了pod的亲和性和反亲和性，<br><code>preferredDuringSchedulingIgnoredDuringExecution</code>指定的规则是pod将会调度到的node尽量会满足如下条件：</p><p>node上具有failure-domain.beta.kubernetes.io/zone，并且具有相同failure-domain.beta.kubernetes.io/zone的值的node上运行有一个pod,它符合label为securtity=S1. <code>preferredDuringSchedulingIgnoredDuringExecution</code>规则表示将不会调度到node上运行有security=S2的pod．如果这里我们将topologyKey＝failure-domain.beta.kubernetes.io/zone，那么pod将不会调度到node满足的条件是：node上具有failure-domain.beta.kubernetes.io/zone相同的Value,并且这些相同zone下的node上运行有security=S2的pod.</p><p>Notice:对于topologyKey字段具有如下约束 </p><ol><li>对于亲和性以及RequiredDuringScheduling的反亲和性，topologyKey需要指定 </li><li>对于RequiredDuringScheduling的反亲和性，LimitPodHardAntiAffinityTopology的准入控制限制topologyKey为kubernetes.io/hostname,可以通过修改或者disable解除该约束 </li><li>对于PreferredDuringScheduling的反亲和性，空的topologyKey表示kubernetes.io/hostname, failure-domain.beta.kubernetes.io/zone and failure-domain.beta.kubernetes.io/region的组合． </li><li>topologyKey在遵循其他约束的基础上可以设置成其他的key.</li></ol><p>规则中可以指定匹配pod所在namespace,如果定义了但是为空，它表示所有namespace范围内的pod.</p><h1 id="常用的场景"><a href="#常用的场景" class="headerlink" title="常用的场景"></a>常用的场景</h1><p>一些更加常用的场景见例子所示 </p><p>例子一</p><pre><code>apiVersion: apps/v1kind: Deploymentmetadata:  name: redis-cachespec:  selector:    matchLabels:      app: store  replicas: 3  template:    metadata:      labels:        app: store    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - store            topologyKey: &quot;kubernetes.io/hostname&quot;      containers:      - name: redis-server        image: redis:3.2-alpine</code></pre><p>创建了一个Deployment,副本数为３，指定了反亲和规则如上所示，pod的label为app:store,那么pod调度的时候将不会调度到node上已经运行了label为app:store的pod了，这样就会使得Deployment的三副本分别部署在不同的host的node上．</p><p>例子二</p><pre><code>apiVersion: apps/v1kind: Deploymentmetadata:  name: web-serverspec:  selector:    matchLabels:      app: web-store  replicas: 3  template:    metadata:      labels:        app: web-store    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - web-store            topologyKey: &quot;kubernetes.io/hostname&quot;        podAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - store            topologyKey: &quot;kubernetes.io/hostname&quot;      containers:      - name: web-app        image: nginx:1.12-alpine</code></pre><p>在一个例子中基础之上，要求pod的亲和性满足requiredDuringSchedulingIgnoredDuringExecution中topologyKey=”kubernetes.io/hostname”,并且node上需要运行有app=store的label. </p><p>运行完例子一，例子二，那么pod的分布如下所示</p><pre><code>$kubectl get pods -o wideNAME                           READY STATUS    RESTARTS   AGE     IP           NODEredis-cache-1450370735-6dzlj   1/1   Running   0          8m      10.192.4.2   kube-node-3redis-cache-1450370735-j2j96   1/1   Running   0          8m      10.192.2.2   kube-node-1redis-cache-1450370735-z73mh   1/1   Running   0          8m      10.192.3.1   kube-node-2web-server-1287567482-5d4dz    1/1   Running   0          7m      10.192.2.3   kube-node-1web-server-1287567482-6f7v5    1/1   Running   0          7m      10.192.4.3   kube-node-3web-server-1287567482-s330j    1/1   Running   0          7m      10.192.3.2   kube-node-2</code></pre><p>例子三</p><pre><code>apiVersion: apps/v1beta1 # for versions before 1.6.0 use extensions/v1beta1kind: Deploymentmetadata:  name: web-serverspec:  replicas: 3  template:    metadata:      labels:        app: web-store    spec:      affinity:        podAffinity:          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - web-store            topologyKey: &quot;kubernetes.io/hostname&quot;      containers:      - name: web-app        image: hub.easystack.io/library/nginx:1.9.0</code></pre><p>在一些应用中，pod副本之间需要共享cache,需要将pod运行在一个节点之上</p><pre><code>web-server-77bfb4575f-bhxvg            1/1       Running   0          11s       10.233.66.79     hzc-slave2   app=web-store,pod-template-hash=3369601319web-server-77bfb4575f-mkfd9            1/1       Running   0          11s       10.233.66.80     hzc-slave2   app=web-store,pod-template-hash=3369601319web-server-77bfb4575f-wgjq6            1/1       Running   0          11s       10.233.66.78     hzc-slave2   app=web-store,pod-template-hash=3369601319</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/davidkbainbridge/demo-affinity" target="_blank" rel="noopener">https://github.com/davidkbainbridge/demo-affinity</a><br><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature</a><br><a href="https://medium.com/kokster/scheduling-in-kubernetes-part-2-pod-affinity-c2b217312ae1" target="_blank" rel="noopener">https://medium.com/kokster/scheduling-in-kubernetes-part-2-pod-affinity-c2b217312ae1</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现实中应用的运行对于kubernetes在亲和性上提出了一些要求，可以归类到以下几个方面： &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pod固定调度到某些节点之上 &lt;/li&gt;
&lt;li&gt;Pod不会调度到某些节点之上 &lt;/li&gt;
&lt;li&gt;Pod的多副本调度到相同的节点之上 &lt;/li&gt;
&lt;li
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="亲和性" scheme="http://yoursite.com/tags/%E4%BA%B2%E5%92%8C%E6%80%A7/"/>
    
  </entry>
  
  <entry>
    <title>Pipeline语法</title>
    <link href="http://yoursite.com/2018/03/23/Pipeline-syntax.html"/>
    <id>http://yoursite.com/2018/03/23/Pipeline-syntax.html</id>
    <published>2018-03-23T09:23:48.000Z</published>
    <updated>2018-03-23T10:16:46.194Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>本文基于入门介绍，仅仅是一个语法参考。至于如何在特定的例子中运用Pipeline语法，请参考<a href="https://cloudpanl.github.io/2018/03/22/Jenkins-Pipeline.html" target="_blank" rel="noopener">Jenkins Pipeline</a>。从插件Pipeline plugin的2.5版本开始，Pipeline支持两种格式的语法。对于它们之间的区别请参考语法对比。</p><p>正如在入门介绍里说的，流水线最主要的就是”步骤“。基本上，就是步骤来告诉Jenkins该干什么，它是申明式和脚本式流水线语法的基础。</p><p>你可以在流水线步骤参考中，找到一份可用的步骤列表。</p><h1 id="申明Pipeline"><a href="#申明Pipeline" class="headerlink" title="申明Pipeline"></a>申明Pipeline</h1><p>申明式流水线是最近添加到Jenkins流水线功能中的，这种语法更加简单。</p><p>所有合法的申明式流水线必须在 pipeline 代码块中，例如：</p><pre><code>pipeline {    /* insert Declarative Pipeline here */}</code></pre><p>在申明式流水线中，基本的语句和表达式是遵循 <a href="http://groovy-lang.org/syntax.html" target="_blank" rel="noopener">Groovy语法</a> ，但是有以下几个例外：</p><ul><li>流水线的顶层必须是一个代码块： pipeline { }</li><li>不需分号作为语句的分隔符。每个语句单独占一行</li><li>只能包括段落、步骤、或者赋值语句</li><li>属性引用语句被当作无参数的方法调用。例如：input会当作方法input()</li></ul><h1 id="段落"><a href="#段落" class="headerlink" title="段落"></a>段落</h1><p>在申明式流水线中，通常包括一个或者多个指令或者步骤。</p><h1 id="代理agent"><a href="#代理agent" class="headerlink" title="代理agent"></a>代理agent</h1><p>代理指定了整个流水线或者特定的阶段的运行环境。它必须在pipeline块的顶层定义，而在阶段中是可选的。</p><h1 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h1><p>为了支持多种情况的流水线使用场景，代理（agent）支持几种不同类型的参数。这些参数既可以在顶层的pipeline块也可以在每个阶段中使用。</p><h2 id="any"><a href="#any" class="headerlink" title="any"></a>any</h2><p>在任意可用的代理上执行流水线。例如： agent any</p><h2 id="none"><a href="#none" class="headerlink" title="none"></a>none</h2><p>当在顶层的pipeline块中使用时，不会有全局的代理分配给整个流水线，每个阶段都需要包含个人的代理。例如：agent none</p><h2 id="label"><a href="#label" class="headerlink" title="label"></a>label</h2><p>根据Jenkins环境中提供的标签，确定一个可用的代理来chiding流水线或者阶段。例如：agent { label ‘my-defined-label’ }</p><h2 id="node"><a href="#node" class="headerlink" title="node"></a>node</h2><p>agent { node { label ‘labelName’ } } 和 agent { label ‘labelName’ }一样，但是 node 允许增加选项（例如 customWorkspace）</p><h2 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h2><p>在指定的容器里执行流水线或者阶段，容器会被动态分配到预先配置好的基于Docker的流水线节点，或者通过label参数来匹配。docker也有一个可选参数args，该参数会直接传递给docker run来执行；还有一个alwaysPull 选项，及时镜像名已经存在了依然会强制执行docker pull。</p><p>例如：</p><pre><code>agent {     docker &apos;maven:3-alpine&apos; } </code></pre><p>或者：</p><pre><code>agent {    docker {        image &apos;maven:3-alpine&apos;        label &apos;my-defined-label&apos;        args  &apos;-v /tmp:/tmp&apos;    }}</code></pre><h2 id="dockerfile"><a href="#dockerfile" class="headerlink" title="dockerfile"></a>dockerfile</h2><p>由码线中的Dockerfile构建出来的容器，执行流水线或者阶段。为了使用该特性，Jenkinsfile 必须是在多分支流水线或者从SCM中加载。约定Dockerfile 在码线的根目录中，可以是agent { dockerfile true } 。如果Dockerfile 在另外一个目录中，可以使用参数dir ：agent { dockerfile { dir ‘someSubDir’ } } 。如果Dockerfile 有其他的名称，你可以通过参数filename 指定文件名称。你可以通过参数additionalBuildArgs 给命令docker build … 传递额外的选项，例如agent { dockerfile { additionalBuildArgs ‘–build-arg foo=bar’ } } 。例如：一个码线有文件build/Dockerfile.build，并需要一个构建参数version：</p><pre><code>agent {    // Equivalent to &quot;docker build -f Dockerfile.build --build-arg version=1.0.2 ./build/    dockerfile {        filename &apos;Dockerfile.build&apos;        dir &apos;build&apos;        label &apos;my-defined-label&apos;        additionalBuildArgs  &apos;--build-arg version=1.0.2&apos;    }</code></pre><h2 id="通用选项Common-Options"><a href="#通用选项Common-Options" class="headerlink" title="通用选项Common Options"></a>通用选项Common Options</h2><p>有一些选项可以在多种代理实现中使用。没有指定的话，就不是必须的。</p><h2 id="label-1"><a href="#label-1" class="headerlink" title="label"></a>label</h2><p>字符串。可以在流水线或者 stage上。</p><p>该选项可以在 node，docker 和 dockerfile中使用，但对于 node是必须的。</p><h2 id="customWorkspace"><a href="#customWorkspace" class="headerlink" title="customWorkspace"></a>customWorkspace</h2><p>字符串。指定工作空间，而不使用默认的。可以是相对于节点上的根工作空间，也可以是绝对路径。例如：</p><pre><code>agent {    node {        label &apos;my-defined-label&apos;        customWorkspace &apos;/some/other/path&apos;    }}</code></pre><p>该选项可以用在 node， docker 和 dockerfile。</p><h2 id="reuseNode"><a href="#reuseNode" class="headerlink" title="reuseNode"></a>reuseNode</h2><p>布尔值，默认为false。如果为true，则在相同的工作空间中运行，而不是每次创建新的。<br>该选项可以在 docker 和 dockerfile中使用，而且只有在 agent 配置到单独的 stage中才能使用。</p><p>Jenkinsfile (Declarative Pipeline)</p><pre><code>pipeline {    agent { docker &apos;maven:3-alpine&apos; }     stages {        stage(&apos;Example Build&apos;) {            steps {                sh &apos;mvn -B clean verify&apos;            }        }    }}</code></pre><p>Jenkinsfile (Declarative Pipeline)</p><pre><code>pipeline {    agent none     stages {        stage(&apos;Example Build&apos;) {            agent { docker &apos;maven:3-alpine&apos; }             steps {                echo &apos;Hello, Maven&apos;                sh &apos;mvn --version&apos;            }        }        stage(&apos;Example Test&apos;) {            agent { docker &apos;openjdk:8-jre&apos; }             steps {                echo &apos;Hello, JDK&apos;                sh &apos;java -version&apos;            }        }    }}</code></pre><ul><li>post</li><li>阶段stages</li><li>步骤steps</li><li>指令Directive</li><li>环境environment</li><li>选项options</li><li>参数parameters</li><li>触发器triggers</li><li>阶段stage</li><li>工具tools</li><li>输入input</li><li>条件when</li></ul><p>指令when 允许流水线根据条件来决定是否要执行特定的阶段。指令when 必须至少包含一个条件。如果指令when 包含多个条件，所有的条件都必须为true才可以会执行该阶段。这和allOf 条件是类似的（请参考下面的例子）。</p><p>更复杂的结构可以使用嵌套：not， allOf，或 anyOf。可以嵌套任意深度。</p><h2 id="内置条件："><a href="#内置条件：" class="headerlink" title="内置条件："></a>内置条件：</h2><h3 id="分支branch"><a href="#分支branch" class="headerlink" title="分支branch"></a>分支branch</h3><p>当匹配分支名称时执行，例如： when { branch ‘master’ }。这只有在多分支流水线中才可以使用。</p><h3 id="环境environment"><a href="#环境environment" class="headerlink" title="环境environment"></a>环境environment</h3><p>当指定的环境变量值和给定的一样时执行，例如： when { environment name: ‘DEPLOY_TO’, value: ‘production’ }</p><h3 id="表达式expression"><a href="#表达式expression" class="headerlink" title="表达式expression"></a>表达式expression</h3><p>当Groovy表达式为true时，例如： when { expression { return params.DEBUG_BUILD } }</p><h3 id="not"><a href="#not" class="headerlink" title="not"></a>not</h3><p>当嵌套条件值为false时执行。必须包含一个条件。例如： when { not { branch ‘master’ } }</p><h3 id="allOf"><a href="#allOf" class="headerlink" title="allOf"></a>allOf</h3><p>当嵌套条件为true时执行。必须至少包含一个。例如： when { allOf { branch ‘master’; environment name: ‘DEPLOY_TO’, value: ‘production’ } }</p><h3 id="anyOf"><a href="#anyOf" class="headerlink" title="anyOf"></a>anyOf</h3><p>当任意一个表达式为true时。必须至少包含一个。例如： when { anyOf { branch ‘master’; branch ‘staging’ } }</p><p>在进入阶段的代理节点之前计算when表达式</p><p>默认情况下，when 条件是在进入阶段的代理之后计算。然而，通过增加选项beforeAgent 可以改变。如果把选项beforeAgent 设置为true，就会首先计算when 条件，只有在值为true时才会进入。</p><p><strong>示例1：</strong></p><p>Jenkinsfile (Declarative Pipeline)</p><pre><code>pipeline {    agent any    stages {        stage(&apos;Example Build&apos;) {            steps {                echo &apos;Hello World&apos;            }        }        stage(&apos;Example Deploy&apos;) {            when {                branch &apos;production&apos;            }            steps {                echo &apos;Deploying&apos;            }        }    }}</code></pre><p><strong>示例2：</strong></p><pre><code>pipeline {    agent any    stages {        stage(&apos;Example Build&apos;) {            steps {                echo &apos;Hello World&apos;            }        }        stage(&apos;Example Deploy&apos;) {            when {                branch &apos;production&apos;                environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;            }            steps {                echo &apos;Deploying&apos;            }        }    }}</code></pre><p><strong>示例3：</strong></p><pre><code>pipeline {    agent any    stages {        stage(&apos;Example Build&apos;) {            steps {                echo &apos;Hello World&apos;            }        }        stage(&apos;Example Deploy&apos;) {            when {                allOf {                    branch &apos;production&apos;                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;                }            }            steps {                echo &apos;Deploying&apos;            }        }    }}</code></pre><p><strong>示例4：</strong></p><pre><code>pipeline {    agent any    stages {        stage(&apos;Example Build&apos;) {            steps {                echo &apos;Hello World&apos;            }        }        stage(&apos;Example Deploy&apos;) {            when {                branch &apos;production&apos;                anyOf {                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos;                }            }            steps {                echo &apos;Deploying&apos;            }        }    }}</code></pre><p><strong>示例5：</strong></p><pre><code>pipeline {    agent any    stages {        stage(&apos;Example Build&apos;) {            steps {                echo &apos;Hello World&apos;            }        }        stage(&apos;Example Deploy&apos;) {            when {                expression { BRANCH_NAME ==~ /(production|staging)/ }                anyOf {                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;production&apos;                    environment name: &apos;DEPLOY_TO&apos;, value: &apos;staging&apos;                }            }            steps {                echo &apos;Deploying&apos;            }        }    }}</code></pre><p><strong>示例6：</strong></p><pre><code>pipeline {    agent none    stages {        stage(&apos;Example Build&apos;) {            steps {                echo &apos;Hello World&apos;            }        }        stage(&apos;Example Deploy&apos;) {            agent {                label &quot;some-label&quot;            }            when {                beforeAgent true                branch &apos;production&apos;            }            steps {                echo &apos;Deploying&apos;            }        }    }}</code></pre><h1 id="并发parallel"><a href="#并发parallel" class="headerlink" title="并发parallel"></a>并发parallel</h1><p>阶段是可以并行执行的。注意，在阶段内必须要只能有一个steps 或 parallel。任何包含parallel 的阶段不能包括agent 或 tools，也不包括steps。</p><p>另外，当有一个任务失败后，你可以强制整个并行失败。只要设置参数failFast 为true就可以。</p><p><strong>示例：</strong></p><pre><code>pipeline {    agent any    stages {        stage(&apos;Non-Parallel Stage&apos;) {            steps {                echo &apos;This stage will be executed first.&apos;            }        }        stage(&apos;Parallel Stage&apos;) {            when {                branch &apos;master&apos;            }            failFast true            parallel {                stage(&apos;Branch A&apos;) {                    agent {                        label &quot;for-branch-a&quot;                    }                    steps {                        echo &quot;On Branch A&quot;                    }                }                stage(&apos;Branch B&apos;) {                    agent {                        label &quot;for-branch-b&quot;                    }                    steps {                        echo &quot;On Branch B&quot;                    }                }            }        }    }}</code></pre><h1 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h1><h1 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h1><p>script 可以在申明时的流水线中执行脚本时步骤。大多数情况下 script 是用不到的。</p><p><strong>示例：</strong></p><pre><code>pipeline {    agent any    stages {        stage(&apos;Example&apos;) {            steps {                echo &apos;Hello World&apos;                script {                    def browsers = [&apos;chrome&apos;, &apos;firefox&apos;]                    for (int i = 0; i &amp;lt; browsers.size(); ++i) {                        echo &quot;Testing the ${browsers[i]} browser&quot;                    }                }            }        }    }}</code></pre><h1 id="脚本化流水线"><a href="#脚本化流水线" class="headerlink" title="脚本化流水线"></a>脚本化流水线</h1><p>脚本化流水线可以使用普通的Groovy语法，因此，它可以实现很强大的功能。</p><p>在Jenkins流水线刚被开发出来时，采用Groovy作为基础。Jenkins已经很长时间内采用嵌入式的Groovy引擎提供了高级脚本功能给管理员和普通用户。也就是说，基于Groovy脚本的流水线指的就是脚本化流水线。</p><h1 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h1><h1 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h1><h1 id="语法对比"><a href="#语法对比" class="headerlink" title="语法对比"></a>语法对比</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://jenkins.io/doc/book/pipeline/syntax/" target="_blank" rel="noopener">https://jenkins.io/doc/book/pipeline/syntax/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;本文基于入门介绍，仅仅是一个语法参考。至于如何在特定的例子中运用Pipeline语法，请参考&lt;a href=&quot;https://cloudpa
      
    
    </summary>
    
      <category term="Jenkins" scheme="http://yoursite.com/categories/Jenkins/"/>
    
    
      <category term="Pipeline" scheme="http://yoursite.com/tags/Pipeline/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins Pipeline</title>
    <link href="http://yoursite.com/2018/03/22/Jenkins-Pipeline.html"/>
    <id>http://yoursite.com/2018/03/22/Jenkins-Pipeline.html</id>
    <published>2018-03-22T05:02:36.000Z</published>
    <updated>2018-03-23T09:34:23.393Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>本文介绍如何在Jenkins中使用pipeline插件。</p><p>Jenkins安装启动后，还需要安装一些插件才可以使用pipeline（流水线）的特性。你可以在系统管理–插件管理–可选插件中搜索Pipeline进行按钮；要提醒一下的是，Jenkins会自己查找依赖的插件，所以你可能看到安装的插件不只一个。Pipeline插件的wiki地址是<a href="https://wiki.jenkins.io/display/JENKINS/Pipeline+Plugin" target="_blank" rel="noopener">https://wiki.jenkins.io/display/JENKINS/Pipeline+Plugin</a>。另外，你可以通过这里<a href="https://plugins.jenkins.io/workflow-aggregator" target="_blank" rel="noopener">https://plugins.jenkins.io/workflow-aggregator</a>，查看该插件的依赖关系，并找到Pipeline插件在Github上的托管地址。</p><h1 id="什么是Pipeline"><a href="#什么是Pipeline" class="headerlink" title="什么是Pipeline"></a>什么是Pipeline</h1><p>Jenkins Pipeline是一套插件，支持实现和持续集成作为流水线应用到Jenkins。Pipeline提供了一套可扩展的工具。</p><p>Pipeline大致可以分为：节点、阶段、步骤。步骤是具体的功能表达式，例如：执行shell命令等。阶段，你可以理解为步骤的集合。而节点则是包含阶段，它规定了这些阶段（步骤）都会在哪些slave上运行。</p><p>节点，可以是一个普通的slave，也可以运行在Docker容器中。</p><h1 id="为什么要用Pipeline"><a href="#为什么要用Pipeline" class="headerlink" title="为什么要用Pipeline"></a>为什么要用Pipeline</h1><p>根本上来说，Jenkins是一个支持很多自动化模式的引擎。Pipeline增加了一套强大的工具到Jenkins中，支持用户从简单持续集成到全面的持续集成。通过模块化一些列相关的任务，用户可以利用很多Pipeline的特性。</p><p><strong>代码：</strong>Pipelines通过代码来实现，并通常可以由版本控制系统（svn、git等）来管理。</p><p><strong>可暂停：</strong>Pipelines可以暂停（停止），并且可以在运行之前接收人工输入或者等待同意。</p><h1 id="Pipeline表达式"><a href="#Pipeline表达式" class="headerlink" title="Pipeline表达式"></a>Pipeline表达式</h1><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><p>是一个单一任务，告诉Jenkins该做什么。例如，在step中执行shell命令make。当一个插件扩展了Pipeline DSL，就意味着可以使用新的step。</p><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><p>大多数工作是在一个或者多个节点（node）中完成的。</p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><p>Jenkins的流水线（pipeline）采用groovy语法来编写。逻辑判断、循环、异常等功能都是具备的，另外，熟悉groovy的人就明白这和Java的写法有一定的相似。</p><p>下面我介绍一些流水线的步骤（或者函数），首先介绍的是在插件workflow-basic-steps-plugin中的。我们从插件的名称上也能看到，这些流水线步骤大多是基础、简单的。首先，给出我研究时的版本信息：</p><pre><code>&lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt;`&lt;artifactId&gt;workflow-basic-steps&lt;/artifactId&gt;&lt;version&gt;2.7-SNAPSHOT&lt;/version&gt;</code></pre><p>以便各位依据本文可以进一步学习Jenkins流水线插件的源码。</p><h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><pre><code>node {    echo env.JENKINS_HOME    sh &apos;echo $JENKINS_HOME&apos;    echo env.JOB_NAME    echo env.NODE_NAME    echo env.NODE_LABELS    echo env.WORKSPACE    echo env.JENKINS_URL    echo env.BUILD_URL    env.SUREN_VER = &apos;12&apos;    echo env.SUREN_VER}</code></pre><p>上面的示例中，给出了如何使用内置的环境变量和自定义环境变量的做法</p><pre><code>node() {    env.JDK_HOME = &quot;${tool &apos;8u131&apos;}&quot;    env.PATH=&quot;${env.JDK_HOME}/bin:${env.PATH}&quot;    echo env.JDK_HOME    echo env.PATH    sh &apos;java -version&apos;}node(&apos;bimpm_deploytodev&apos;) {    def pass_bin = &apos;/opt/pass/bin&apos;    env.PASS_BIN = pass_bin    stage(&apos;Clean&apos;) {        sh &apos;rm -rfv $PASS_BIN&apos;    }}</code></pre><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><pre><code>node() {    tool name: &apos;JDK8_Linux&apos;, type: &apos;jdk&apos;    tool name: &apos;maven339_linux_dir&apos;, type: &apos;maven&apos;    echo &apos;hello&apos;}</code></pre><p>上面的pipeline指定需要工具jdk和maven的名称（在Global Tool Configuration中配置）。</p><p>对应的实现类为ToolStep，该类被final关键字所修饰，因此是不能做扩展的了。</p><h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><pre><code>node {    echo &apos;hello&apos;}properties([    buildDiscarder(        logRotator(            artifactDaysToKeepStr: &apos;&apos;,            artifactNumToKeepStr: &apos;&apos;,            daysToKeepStr: &apos;5&apos;,            numToKeepStr: &apos;10&apos;        )    ),    pipelineTriggers([      cron(&apos;H 3,12,17 * * *&apos;)    ])])</code></pre><h2 id="拷贝成品"><a href="#拷贝成品" class="headerlink" title="拷贝成品"></a>拷贝成品</h2><pre><code>node {       stage(&apos;Copy&apos;) {       step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true, projectName: &apos;BIM_PMJF/BIM-PMJF-BUILD/BIM_PMJF_DISCOVERY&apos;, selector: [$class: &apos;StatusBuildSelector&apos;, stable: false], target: &apos;/opt/pass/bin&apos;])   }}</code></pre><p>我们通常会在一个Job里实现工程构建，在另外的Job里做程序的部署，这时候就可以用到Jenkins的成品特性。它可以实现在多个slave之间拷贝成品。实现类为ArtifactArchiverStep。</p><h2 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h2><pre><code>node(&apos;suren&apos;) {    def dev_path = &apos;/opt/suren/bin&apos;    def services = [        [            &apos;name&apos;: &apos;admin&apos;,            &apos;project&apos;: &apos;admin&apos;,            &apos;port&apos;: &apos;7002&apos;,            &apos;jarName&apos;: &apos;admin&apos;        ]    ];    stage(&apos;Copy Artifact&apos;) {        for(service in services){            step([$class: &apos;CopyArtifact&apos;, fingerprintArtifacts: true, flatten: true,                projectName: service.project,                selector: [$class: &apos;StatusBuildSelector&apos;, stable: false],                target: dev_path + &apos;/&apos; + service.name            ])        }    }    stage(&apos;Stop Service&apos;) {        for(service in services){           sh &apos;fuser -n tcp -k &apos; + service.port + &apos; &gt; redirection &amp;&apos;        }    }    stage(&apos;Start Service&apos;) {        for(service in services){            sh &apos;cd &apos; + pass_bin + &apos;/&apos; + service.name + &apos; &amp;&amp; nohup nice java -server -Xms128m -Xmx384m \                -jar &apos; + service.jarName + &apos;.jar \                --server.port=&apos; + service.port + &apos; $&gt; initServer.log 2&gt;&amp;1 &amp;&apos;        }    }}</code></pre><p>上面的例子，展示了如何在jenkins pipeline中调用循环语句，实现批量操作。</p><h2 id="参数化构建"><a href="#参数化构建" class="headerlink" title="参数化构建"></a>参数化构建</h2><pre><code>properties([[$class: &apos;JobRestrictionProperty&apos;],    parameters([run(description: &apos;&apos;,        filter: &apos;ALL&apos;,         name: &apos;Name&apos;,         projectName: &apos;Project&apos;)]),    pipelineTriggers([])])</code></pre><p>为了能让我们的流水线定义更加具有通用性，除了可以在流水线中使用系统预定义的变量外，可以使用由用户动态输入的变量值。当流水线Job加入参数化后，在执行任务时候就必须有用户输入一系列值才可以执行。</p><h2 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h2><pre><code>node {    stage(&apos;Start Service&apos;) {        parallel &apos;test&apos;: {            echo &apos;test&apos;        }, &apos;deply&apos;: {            echo &apos;deply&apos;        }    }    parallel &apos;one&apos; : {        stage(&apos;one&apos;) {            echo &apos;one&apos;        }    }, &apos;two&apos; : {        stage(&apos;two&apos;) {            echo &apos;two&apos;        }    }}parallel &apos;one&apos;: {    node{        stage(&apos;one&apos;) {            echo &apos;one&apos;        }    }}, &apos;two&apos;: {    node {        stage(&apos;two&apos;) {            echo &apos;two&apos;        }    }}</code></pre><p>Jenkins的流水线同时支持节点（node）、阶段（stage）和步骤（step）之间的并行执行。如果多个节点并发执行的话，并发数量会少于当前可用的执行器（exector）数量。</p><h2 id="超时"><a href="#超时" class="headerlink" title="超时"></a>超时</h2><pre><code>node {    stage(&apos;stage2&apos;) {        timeout(time: 600, unit: &apos;SECONDS&apos;) {            sleep 20            echo &apos;2&apos;        }    }}</code></pre><p>遇到可能执行时间会比较长的情况，可以通过超时来约定最长的执行时间。</p><p>对应的实现类为TimeoutStep。</p><p>下面介绍的函数在插件workflow-durable-task-step-plugin中，版本信息如下：</p><pre><code>&lt;groupId&gt;org.jenkins-ci.plugins.workflow&lt;/groupId&gt;&lt;artifactId&gt;workflow-durable-task-step&lt;/artifactId&gt;&lt;version&gt;2.18-SNAPSHOT&lt;/version&gt;</code></pre><h2 id="工作空间"><a href="#工作空间" class="headerlink" title="工作空间"></a>工作空间</h2><p>当你希望在一个流水线中，对多个工程（例如git工程）做构建以及部署等操作，如果不切换工作空间的话就会发生代码错乱的问题。你可以参考下面的示例代码来解决这个问题：</p><pre><code>node{    stage(&apos;suren&apos;){        ws(&apos;suren-a-work&apos;) {            pwd        }        ws(&apos;suren-b-work&apos;) {            pwd        }    }}</code></pre><p>实现类为WorkspaceStep，使用final修饰，无法扩展。</p><h2 id="执行节点"><a href="#执行节点" class="headerlink" title="执行节点"></a>执行节点</h2><p>Jenkins里可能会配置很多节点（node），而不一定所有的节点都满足你的构建环境要求，这时候就需要来指定节点了：</p><pre><code>node(&apos;local&apos;) {    echo &apos;hello&apos;}properties([    buildDiscarder(        logRotator(            artifactDaysToKeepStr: &apos;&apos;,            artifactNumToKeepStr: &apos;&apos;,            daysToKeepStr: &apos;5&apos;,            numToKeepStr: &apos;10&apos;        )    ),    pipelineTriggers([      cron(&apos;H 3,12,17 * * *&apos;)    ])])</code></pre><p>上面的pipeline指定了运行节点的label为local。</p><p>实现类为ExecutorStep，使用final修饰，无法扩展。</p><h2 id="异常捕获"><a href="#异常捕获" class="headerlink" title="异常捕获"></a>异常捕获</h2><pre><code>node{    stage(&apos;suren&apos;){        try{            trigger        }catch(error){            echo error.getMessage()        }    }}</code></pre><p>这里调用了一个不存在的流水线函数，然后使用catch来捕获并打印错误信息。</p><h2 id="stash"><a href="#stash" class="headerlink" title="stash"></a>stash</h2><pre><code>node {    stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true)}node(&apos;jenkins-slave&apos;) {    unstash(name: &apos;test&apos;)}</code></pre><p>pipeline的文件存储（stash）这个功能，可以在流水线需要运行在多个节点（node）的情况下使用。stash和unstash会把存储的文件从一个节点转移到另一个节点上。上面给出的例子中，把所有的xml文件从master转移到了当前执行任务的slaver节点上。</p><pre><code>node(&apos;jenkins-slave&apos;){    checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]])}node {    def path = JENKINS_HOME + &apos;/jobs/&apos; + JOB_NAME + &apos;/builds/&apos; + BUILD_ID    echo path    dir(path){        stash(name: &apos;test&apos;, includes: &apos;*.xml&apos;, allowEmpty: true)    }}node(&apos;jenkins-slave&apos;) {    unstash(name: &apos;test&apos;)}</code></pre><h2 id="敏感信息"><a href="#敏感信息" class="headerlink" title="敏感信息"></a>敏感信息</h2><p>我们可以利用Jenkins的Credentials机制，在Pipeline中传递密码等敏感信息，例如：</p><pre><code>pipeline {    agent any    stages{        stage(&apos;test&apos;) {            steps{                withCredentials([usernamePassword(credentialsId: &apos;aaa&apos;, passwordVariable: &apos;passwd&apos;, usernameVariable: &apos;user&apos;)]) {                    sh &apos;&apos;&apos;echo $user $passwd&apos;&apos;&apos;                }            }        }    }}</code></pre><h2 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h2><p>很多情况下，我们需要读取文件内容。</p><p>获取pom.xml版本号，获取groupId（需要的插件Pipeline Utility Steps）：</p><pre><code>node {    stage(&apos;test&apos;){        checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;*/master&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;https://github.com/LinuxSuRen/autotest.parent&apos;]]])        pom = readMavenPom file: &apos;pom.xml&apos;        echo pom.version        echo pom.groupId        echo pom.artifactId    }}</code></pre><h2 id="使用Docker"><a href="#使用Docker" class="headerlink" title="使用Docker"></a>使用Docker</h2><p>在docker容器中执行任务：</p><pre><code>pipeline {    agent {        docker {            image &apos;eclipse/mysql&apos;            args &apos;-e MYSQL_ROOT_PASSWORD=root&apos;        }     }    stages {        stage(&apos;test&apos;) {            steps {                sh &apos;mysql&apos;            }        }    }}</code></pre><p>withDockerContainer</p><p>withDockerServer</p><p>dockerFingerprintRun</p><p>withDockerRegistry</p><p>dockerFingerprintFrom</p><h1 id="其他插件"><a href="#其他插件" class="headerlink" title="其他插件"></a>其他插件</h1><p>另外有一些比较好的Jenkins流水线插件，给出推荐：</p><p>支持从SCM加载库文件 <a href="https://github.com/suren-jenkins/workflow-remote-loader-plugin" target="_blank" rel="noopener">https://github.com/suren-jenkins/workflow-remote-loader-plugin</a></p><h1 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h1><p>安装环境：</p><pre><code>sudo apt-get install -y npmjenkins-pipeline --file test.groovy --url http://localhost:8080/jenkins/job/MyJob --credentials admin:123456</code></pre><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/spring-cloud/spring-cloud-pipelines" target="_blank" rel="noopener">https://github.com/spring-cloud/spring-cloud-pipelines</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;本文介绍如何在Jenkins中使用pipeline插件。&lt;/p&gt;
&lt;p&gt;Jenkins安装启动后，还需要安装一些插件才可以使用pipelin
      
    
    </summary>
    
      <category term="Jenkins" scheme="http://yoursite.com/categories/Jenkins/"/>
    
    
      <category term="Pipeline" scheme="http://yoursite.com/tags/Pipeline/"/>
    
  </entry>
  
  <entry>
    <title>初探Jenkins X</title>
    <link href="http://yoursite.com/2018/03/21/Preliminary-Jenkins-X.html"/>
    <id>http://yoursite.com/2018/03/21/Preliminary-Jenkins-X.html</id>
    <published>2018-03-21T13:42:25.000Z</published>
    <updated>2018-03-22T15:07:37.764Z</updated>
    
    <content type="html"><![CDATA[<p>Jenkins 于 3月21日 发布了名为Jenkins X的项目，这一项目对开发人员和云端的 CI/CD 环境之间的交互过程进行了审视和反思，结合自动化、工具链以及 DevOps 最佳实践。为开发团队提供了新的生产效率增长点。</p><h1 id="Jenkins-X是什么？"><a href="#Jenkins-X是什么？" class="headerlink" title="Jenkins X是什么？"></a>Jenkins X是什么？</h1><p>“X”注定是一个不平凡的名字，Jenkins X 对于整个Jenkins生态而言也是不平凡的存在。</p><p>简而言之，<strong>Jenkins X 是一个高度集成化的CI/CD平台，基于Jenkins和Kubernetes实现，旨在解决微服务体系架构下的云原生应用的持续交付的问题，简化整个云原生应用的开发、运行和部署过程。</strong></p><p>你猜的没错，Jenkins X 只能在Kubernetes集群上运行，这有并不意外。Kubernetes已然成为了容器编排的一枝独秀，各大厂商纷纷转向Kubernetes，发布了自己的公有云、操作系统或PaaS平台。</p><p>另外，微服务和云原生应用解决方案也日臻成熟，以Spring Boot为代表的一系列体系框架也开始走到舞台中央。</p><p>与此同时，随着应用架构的细分和服务间的解耦，服务具备了独立发布的能力，这也使得微服务架构下的持续交付成为业界所关注的热门领域，我们需要更加灵活的CI/CD自动化解决方案，以应对越发快速的交付需求。</p><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521727776/Preliminary-Jenkins-X-1.jpg" alt=""></p><blockquote><p>注：Jenkins的企业版CloudBees，已经加入CNCF（云原生计算）基金会</p></blockquote><p>看到这里，你是不是觉得Jenkins X 就是个基于Kubernetes的持续交付平台呢？</p><p>那你就大错特错了，因为Jenkins X想要实现的远非如此而已！</p><p>试想如下场景：</p><blockquote><p>越来越多的工具和实践，工程师们需要会写Kubernetes YAML，Dockerfile，Jenkinsfile，对微服务、云原生、Kubernetes和Jenkins非常熟悉。</p><p>臣妾做不到呀！</p></blockquote><p>而在Jenkins X的世界中，这一切都是通过命令完成。</p><p>可以说<strong>Jenkins X重新思考了未来云原生应用下研发工程师和CI/CD的交付方式</strong>，通过整合工具，自动化和DevOps最佳实践，改善了研发过程中的复杂环节，让研发可以专注于价值创造，其他的事情通通交给Jenkins X来帮你解决。</p><p>神奇吗？</p><p>的确，在第一次看到项目演示的时候，我也惊叹世界的变化如此之快，在Jenkins X的设计中，整合了Helm，Draft，GitOps，以及Nexus，chartmuseum，monocular等诸多新系统和工具，从而实现自动构建编译环境，生成容器镜像，流水线，自动化部署，并通过简单的Review实现不同环境间的自动发布。</p><p>这一切都被完美的封装在简单的jx命令之后。同时你也无需担心对内部实现细节的失控，因为一切都被妥善的版本控制，可以自定义和修改，可以说Jenkins X为你实现了自动化的CI/CD和DevOps最佳实践，持续交付不再是难事，进而提升生产力，实现促进企业的业务成功！</p><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521728048/preliminary-jenkins-x-2.jpg" alt=""></p><h1 id="Jenkins-X-部分新特性"><a href="#Jenkins-X-部分新特性" class="headerlink" title="Jenkins X 部分新特性"></a>Jenkins X 部分新特性</h1><h2 id="1-自动化一切：自动化CI-CD流水线"><a href="#1-自动化一切：自动化CI-CD流水线" class="headerlink" title="1. 自动化一切：自动化CI/CD流水线"></a>1. 自动化一切：自动化CI/CD流水线</h2><ul><li>选择项目类型自动生成Jenkinsfile定义流水线</li><li>自动生成Dockerfile并打包容器镜像</li><li>自动创建Helm Chart并运行在Kubernetes集群</li><li>自动关联代码库和流水线，作为代码变更自动触发（基于Webhook实现）</li><li>自动版本号自动归档</li></ul><h2 id="2-Review代码一键部署应用：基于GitOps的环境部署"><a href="#2-Review代码一键部署应用：基于GitOps的环境部署" class="headerlink" title="2. Review代码一键部署应用：基于GitOps的环境部署"></a>2. Review代码一键部署应用：基于GitOps的环境部署</h2><ul><li>所有的环境，应用列表，版本，配置信息统一放在代码库中进行版本控制</li><li>通过Pull Request实现研发和运维的协同，完成应用部署升级（Promotion）</li><li>可自动部署和手动部署，在必要的时候增加手工Review</li><li>当然这些都封装在jx命令中实现</li></ul><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521728214/preliminary-jenkins-x-3.jpg" alt=""></p><h2 id="3-自动生成预览环境和信息同步反馈"><a href="#3-自动生成预览环境和信息同步反馈" class="headerlink" title="3. 自动生成预览环境和信息同步反馈"></a>3. 自动生成预览环境和信息同步反馈</h2><ul><li>预览环境用于代码Review环节中临时创建</li><li>同Pull Request工作流程集成并实现信息同步和有效通知</li><li>验证完毕后自动清理</li><li>提交和应用状态自动同步到Github注释</li><li>自动生成release notes信息供验证</li></ul><h1 id="Jenkins-X-核心组件"><a href="#Jenkins-X-核心组件" class="headerlink" title="Jenkins X 核心组件"></a>Jenkins X 核心组件</h1><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521728362/Preliminary-Jenkins-X-4.jpg" alt=""></p><h2 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h2><p>Jenkins X不是一个全新的Jenkins。</p><p>他依然使用Jenkins作为持续交付的核心引擎，实际上Jenkins X作为Jenkins的一个子项目存在，专注于云原生应用的CI/CD实现，同时也帮助Jenkins自身完成云原生应用的转型，毕竟现在越来越多的人在诟病单体应用的设计和文件存储系统。</p><p>在之前同Jenkins创始人和核心骨干的交流中，我们也了解到Jenkins已经开始着手改变。</p><h2 id="HELM"><a href="#HELM" class="headerlink" title="HELM"></a>HELM</h2><p>Helm是用于管理Kubernetes资源对象的工具，类似APT，YUM和HOMEBREW，他通过将Kubernetes的资源对象打包成Chart的形式，完成复杂应用的部署和版本控制，是目前业界流行的解决方案</p><h2 id="DRAFT"><a href="#DRAFT" class="headerlink" title="DRAFT"></a>DRAFT</h2><p>Draft是自动化应用构建和运行在Kubernetes上面的工具，具有语言识别能力，能够自动生成构建脚本，依赖，环境并打包成docker镜像并部署在Kubernetes集群上，加快代码开发节奏，而无需关心基础设施层面的技术实现</p><h2 id="GitOps"><a href="#GitOps" class="headerlink" title="GitOps"></a>GitOps</h2><p>GitOps是weaveworks推出的天才的应用部署解决方案，他将Git作为整个应用部署的单一可信数据源（SSOT），通过类似代码开发的Pull Request流程完成应用部署的Review和自动化实现，并且将部署配置信息纳入版本控制。</p><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521728496/Preliminary-Jenkins-X-5.jpg" alt=""></p><h1 id="Jenkins-X-安装试用"><a href="#Jenkins-X-安装试用" class="headerlink" title="Jenkins X 安装试用"></a>Jenkins X 安装试用</h1><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><p><strong>工具</strong></p><ul><li>helm</li><li>kubectl</li><li>git</li></ul><p><strong>Kubernetes 集群</strong></p><ul><li>互联网连接</li><li>Tiller</li><li>公网 IP</li></ul><p><strong>github 账号</strong></p><h2 id="安装-jx"><a href="#安装-jx" class="headerlink" title="安装 jx"></a>安装 jx</h2><p><a href="http://jenkins-x.io/getting-started/install/" target="_blank" rel="noopener">http://jenkins-x.io/getting-started/install/</a> 提供了几种系统下的安装说明：</p><ul><li>OS X：<code>brew tap jenkins-x/jx &amp;&amp; brew tap jenkins-x/jx</code></li><li>Linux：<code>curl -L https://github.com/jenkins-x/jx/releases/download/v1.1.10/jx-darwin-amd64.tar.gz | tar xzv &amp;&amp; mv jx /usr/local/bin</code></li></ul><h2 id="jx-install"><a href="#jx-install" class="headerlink" title="jx install"></a>jx install</h2><p><code>jx create cluster</code> 支持多种公有云的创建。</p><p>配置好集群和对应的 kubeconfig 访问之后，就可以使用jx install进行安装了。</p><p>过程中几个需要注意的点：</p><ul><li>如果 Tiller 的 SA 权限不足，会导致安装失败，可设置相应的 ClusterRole 进行解决。</li><li>安装过程会修改 kubeconfig 文件，因此建议做好备份。</li><li>为完整体验功能，建议听从安装器建议，安装 Ingress Controller。</li><li>Jenkins X 的环境管理以及代码拉取等功能需要和 Github 进行交互，因此会提问 GitHub 的 Token。</li><li>安装过程相对较长，可以使用<code>watch kubectl get pods -n jx</code>查看进程状况。</li><li>最后步骤会显示管理密码，注意复制保存。</li></ul><h2 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h2><p>JX 会为用户建立三个环境分别是，Dev、Staging 以及 Production。</p><p>运行命令<code>jx console</code>，会打开浏览器进入 Jenkins 登录页面。</p><p>登录之后我们会看到正在进行构建，如果是一个排队状态，可能是因为正在创建 Worker Pod，可以使用kubectl查询具体情况。</p><p>构建完成，会看到这一示例中包含了拉取、构建、Helm、环境等几个步骤，可以作为工作的基础环节来进行使用。</p><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521730673/Preliminary-Jenkins-X-6.jpg" alt=""></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>接下来就可以做几个善后工作</p><ol><li>jx 支持插件，可以通过jx get addons查看支持的插件列表，进行安装。</li><li>根据实际工作需要，对缺省环境进行调整，安装所需软件。</li><li>对 Jenkins X 中的软件、集群进行安全加固。</li><li>使用import或者create spring/create quickstart，进行项目工作。</li><li>最后要注意的一点是，Jenkins X 目前的升级频率非常高。不建议生产使用。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Jenkins 于 3月21日 发布了名为Jenkins X的项目，这一项目对开发人员和云端的 CI/CD 环境之间的交互过程进行了审视和反思，结合自动化、工具链以及 DevOps 最佳实践。为开发团队提供了新的生产效率增长点。&lt;/p&gt;
&lt;h1 id=&quot;Jenkins-X是
      
    
    </summary>
    
      <category term="Jenkins" scheme="http://yoursite.com/categories/Jenkins/"/>
    
    
      <category term="Jenkins" scheme="http://yoursite.com/tags/Jenkins/"/>
    
  </entry>
  
  <entry>
    <title>浅谈服务治理、微服务与Service Mesh（二） Spring Cloud从入门到精通到放弃【转载】</title>
    <link href="http://yoursite.com/2018/03/19/spring-cloud-start-to-give-up.html"/>
    <id>http://yoursite.com/2018/03/19/spring-cloud-start-to-give-up.html</id>
    <published>2018-03-19T14:36:59.000Z</published>
    <updated>2018-03-19T15:19:45.395Z</updated>
    
    <content type="html"><![CDATA[<p>作为本系列文章的第二篇(第一篇链接请戳：<a href="https://cloudpanl.github.io/2018/03/19/dubbo-past-and-present.html" target="_blank" rel="noopener">浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生</a>)，本文主要为大家介绍下微服务概念中非常火热的Spring Cloud开发框架。由于网上关于Spring Cloud的文章多如牛毛，为了让大家阅读后能有不一样的收获，因此本文将用一个相对轻松的叙述方式来为大家讲解一下Spring Cloud框架和微服务。虽然不可能通过一篇文章让大家对Spring Cloud做到从“入门到精通到放弃”，但是希望大家通过阅读本文能对Spring Cloud和微服务有一个更加清晰的认识和了解，为后面学习Service Mesh做好一个铺垫。</p><h1 id="Spring-Cloud-之“出身名门望族”"><a href="#Spring-Cloud-之“出身名门望族”" class="headerlink" title="Spring Cloud 之“出身名门望族”"></a>Spring Cloud 之“出身名门望族”</h1><p>作为当下最火热的微服务框架，Spring Cloud的名字可以说是无人不知、无人不晓，凭借之前Spring Framework的良好群众基础和Cloud这个具有时代感的名字，Spring Cloud一出现便被大家认知。</p><p>提到Spring Cloud，便会让人想起刚刚发布了2.0版本的Spring Boot。Spring Boot和Spring Cloud都是出自Pivotal公司，Spring Boot和Spring Cloud虽然火热，但是了解Pivotal公司的人在国内却是不多。实际上Pivotal公司在云计算、大数据、虚拟化等领域都有所建树，这里先给大家简单八卦下Pivotal的情况。</p><p>Pivotal公司是由EMC和VMware联合成立的一家公司，GE（通用电气）也对Pivotal进行了股权收购，同时GE也是Pivotal的一个重要大客户。除了Spring Framework、Spring Boot和Spring Cloud之外，我们日常开发中经常使用的Reids、RabbitMQ、Greenplum、Gemfire、Cloud Foundry等，目前都是归属于Pivotal公司的产品。其中Gemfire也是被中国铁路总公司12306使用的分布式内存数据库，也就是说你过年回家买不到火车票，这个锅Pivotal的Gemfire也会跟着一起背（开个小玩笑，哈哈）。</p><h1 id="Spring-Cloud-之“入门”"><a href="#Spring-Cloud-之“入门”" class="headerlink" title="Spring Cloud 之“入门”"></a>Spring Cloud 之“入门”</h1><p>Spring Cloud作为一个微服务的开发框架，其包括了很多的组件，包括：Spring Cloud Netflix（Eureka、Hystrix、Zuul、Archaius）、Spring Cloud Config、Spring Cloud Bus、Spring Cloud Cluster、Spring Cloud Consul、Spring Cloud Security、Spring Cloud Sleuth、Spring Cloud Data Flow、Spring Cloud Stream、Spring Cloud Task、Spring Cloud ZooKeeper、Spring Cloud Connectors、Spring Cloud Starters、Spring Cloud CLI等。</p><p>在上述组件中，Spring Cloud Netflix是一套微服务的核心框架，由互联网流媒体播放商Netflix开源后并入Spring Cloud大家庭，它提供了的微服务最基础的功能：服务发现（Service Discovery）、动态路由（Dynamic Routing）、负载均衡（Load Balancing）和边缘服务器（Edge Server）等。</p><p>Spring Boot是Spring的一套快速配置脚手架，可以基于Spring Boot快速开发单个微服务。Spring Boot简化了基于Spring的应用开发，通过少量的代码就能创建一个独立的、生产级别的Spring应用。由于Spring Cloud是基于Spring Boot进行的开发，因此使用Spring Cloud就必须使用到Spring Boot。</p><p>下图是一个常见的关于Spring Cloud的架构图。下面此图为例，对Spring Cloud最常用的几个组件做一个简单的介绍：<br><img src="http://dockone.io/uploads/article/20180312/75d7b6bf5b32dbf2b364cd247e970335.png" alt=""></p><ul><li>Eureka：服务注册中心，一个基于REST的服务，用于定位服务，以实现微服务架构中服务发现和故障转移。</li><li>Hystrix：熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。</li><li>Turbine：Turbine是聚合服务器发送事件流数据的一个工具，用来监控集群下Hystrix的Metrics情况。</li><li>Zuul：API网关，Zuul是在微服务中提供动态路由、监控、弹性、安全等边缘服务的框架。</li><li>Ribbon：提供微服务中的负载均衡功能，有多种负载均衡策略可供选择，可配合服务发现和断路器使用。</li><li>Feign：Feign是一种声明式、模板化的HTTP客户端。</li><li>Spring Cloud Config：配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。</li><li>Spring Cloud Security：基于Spring Security的安全工具包，为微服务的应用程序添加安全控制。</li><li>Spring Cloud Sleuth：日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。</li></ul><p>除了上面介绍的基础组件外，常见的Spring Cloud组件还有非常多种，涉及到了微服务以及应用开发的方方面面：</p><ul><li>Spring Cloud Starters：Spring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。</li><li>Archaius：配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。</li><li>Consul：封装了Consul操作，Consul是一个服务发现与配置工具，与Docker容器可以无缝集成。</li><li>Spring Cloud Stream：数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。</li><li>Spring Cloud CLI：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。</li><li>Spring Cloud Task：提供云端计划任务管理、任务调度。</li><li>Spring Cloud Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与 Spring Cloud Config 联合实现热部署。</li><li>Spring Cloud Data Flow：大数据操作工具，作为Spring XD的替代产品，它是一个混合计算模型，结合了流数据与批量数据的处理方式。</li><li>Spring Cloud ZooKeeper：操作ZooKeeper的工具包，用于使用ZooKeeper方式的服务发现和配置管理。</li><li>Spring Cloud Connectors：便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。</li></ul><h1 id="Spring-Cloud之“精通”"><a href="#Spring-Cloud之“精通”" class="headerlink" title="Spring Cloud之“精通”"></a>Spring Cloud之“精通”</h1><p>Spring Cloud虽然集成了众多组件，可以构建一个完整的微服务应用，但是其中的各个组件却并非完美无缺，很多组件在实际应用中都存在诸多不足和缺陷。因此，需要我们对其中的一些组件进行替换和修改，方能构建一个强大、灵活、健壮的微服务架构应用。</p><h2 id="配置中心"><a href="#配置中心" class="headerlink" title="配置中心"></a>配置中心</h2><p>Spring Cloud Config可以说是Spring Cloud家族中实现最Low的一个组件，直接采用了本地存储/SVN/Git的方式进行存储。同时，Spring Cloud Config也缺乏一个完整的可视化管理查询后台，当存在比较复杂的权限管理和版本管理需求时，Spring Cloud Config会显得非常力不从心。如果需要在配置修改后，能自动进行配置信息推送的话，使用Spring Cloud Config也无法满足要求，需要自行编写代码进行实现。</p><p>目前开源社区中，已经有了很多的开源配置中心实现方案，同时很多公司也自研了自己的配置中心方案。包括淘宝的统一配置中心Diamond（已经多年未更新）、百度的分布式配置管理平台Disconf、携程的开源分布式配置中心Apollo、360的分布式配置管理工具QConf等等。目前，笔者公司采用的是自己公司自研的配置中心，没有采用开源实现的主要原因是因为需要同时适配Spring Cloud和Dubbo等多种场景的应用。</p><h2 id="注册中心"><a href="#注册中心" class="headerlink" title="注册中心"></a>注册中心</h2><p>作为Spring Cloud的服务注册中心，从分布式CAP理论来看，Eureka采用是AP型设计，强调的是注册中心的高可用性。和Dubbo常用的服务注册中心ZooKeeper相比，ZooKeeper则是采用的CP型设计，强调的是注册中心数据的一致性。</p><p>Eureka的设计确实简单易用，但是默认没有实现对注册中心数据的持久化。同时，在极端场景下，也会出现多个Eureka注册中心节点数据不一致，甚至服务注册数据丢失的情况。当然，从分布式CAP理论来看，理论上是没办法做到同时兼顾CAP三点的。目前也有一些互联网公司对Eureka进行了改造，支持了数据的持久化，但是尚不能完整的支持CAP的全部要求。</p><h2 id="API网关"><a href="#API网关" class="headerlink" title="API网关"></a>API网关</h2><p>API网关可以说是微服务需求最多，也是最有难点的一个组件。Spring Cloud中集成的Zuul应该说更多的是实现了服务的路由功能，对于负载均衡等其他功能，需要结合Ribbon等组件来实现。对于很多个性化的需求，需要开发者自己来进行编码实现。</p><p>和大部分基于Java的Web应用类似，Zuul也采用了Servlet架构，因此Zuul处理每个请求的方式是针对每个请求是用一个线程来处理。同时，由于Zuul是基于JVM的实现，因此性能也会在高并发访问场景下成为瓶颈。虽然网上一些文章评测Zuul和Nginx性能接近，但是在性能要求较高的场景下，JVM的内存管理和垃圾回收问题，仍然是一个很大的问题。所以在实际的应用场景中，通常会采用在多个Zuul几点前面再添加一层Nginx或者OpenResty来进行代理。</p><p>为了解决Zuul的性能问题，Netflix将自己的网关服务Zuul进行了升级，新的Zuul 2将HTTP请求的处理方式从同步变成了异步，并且新增诸如HTTP/2、websocket等功能。但是遗憾的是，开源版本的Zuul 2一直处于难产状态中，始终没有和大家正式见面。</p><h2 id="熔断器"><a href="#熔断器" class="headerlink" title="熔断器"></a>熔断器</h2><p>微服务中对于服务的限流、降级、熔断的需求是多种多样的，需要在API网关和各个具体服务接口中分别进行控制，才能满足复杂场景下微服务架构的应用需求。</p><p>单独使用Spring Cloud中的Hystrix无法完整的满足上述的复杂需求，需要结合API网关，并通过Kubernetes对资源、进程和命名空间来提供隔离，并通过部分自定义编码方能实现对全部服务的限流、降级、熔断等需求。</p><h2 id="监控系统"><a href="#监控系统" class="headerlink" title="监控系统"></a>监控系统</h2><p>无论是Spring Cloud中集成的Spring Cloud Sleuth，还是集成经典的ELK，都只是对日志级别的追踪和监控。在大中型微服务应用架构中，尤其是基于JVM的项目，还需要添加APM的监控机制，才能保证及时发现各种潜在的性能问题。</p><p>APM整体上主要完成3点功能：1.日志追踪、2.监控报警、3.性能统计。目前，国内外商业版本的APM方案已经有很多，开源版本的APM方案也开始丰富起来。国内开源的APM方案主要有：大众点评的CAT和Apache孵化中的SkyWalking。这里给大家重点推荐下SkyWalking，SkyWalking是针对分布式系统的应用性能监控系统，特别针对微服务、Cloud Native和容器化（Docker、Kubernetes、Mesos）架构，项目的关注度和发展速度都很快，中文文档资料也比较齐全。</p><h1 id="Spring-Cloud之“放弃”"><a href="#Spring-Cloud之“放弃”" class="headerlink" title="Spring Cloud之“放弃”"></a>Spring Cloud之“放弃”</h1><p>Spring Cloud可以说是一个完美的微服务入门框架，如果你是在一个中小型项目中应用Spring Cloud，那么你不需要太多的改造和适配，就可以实现微服务的基本功能。但是如果是在大型项目中实践微服务，可能会发现需要处理的问题还是比较多，尤其是项目中老代码比较多，没办法全部直接升级到Spring Boot框架下开发的话，你会非常希望能有一个侵入性更低的方案来实施微服务架构。在这种场景下，Service Mesh将会成为你的最佳选择，经过一段时间的发展，目前Service Mesh这个概念已经开始逐步被大家了解和认知。同时，一些Service Mesh的实现方案也逐步成熟和落地，例如Istio、Linkerd、Envoy等。在本系列文章的下一篇中，将为大家对Service Mesh概念做一个系统的介绍。但是在了解Service Mesh概念之前，还是建议大家先对微服务和Spring Cloud这些概念和框架有一个深入的了解，这样才能体会到应用Service Mesh的价值和意义。</p><h2 id="Spring-Cloud与Dubbo"><a href="#Spring-Cloud与Dubbo" class="headerlink" title="Spring Cloud与Dubbo"></a>Spring Cloud与Dubbo</h2><p>网上关于Spring Cloud和Dubbo对比的文章很多，大多数对比结果都是Spring Cloud压倒性优势战胜Dubbo，下表是对Dubbo和Spring Cloud做的一个基础功能的对比：<br><img src="http://dockone.io/uploads/article/20180312/009b0969042ea1378a8d7beb271fd386.png" alt=""></p><p>实际上，Dubbo的关注点在于服务治理，并不能算是一个真正的微服务框架。包括目前在开发中的Dubbo 3.0，也不能完整覆盖微服务的各项功能需求。而Spring Cloud一方面是针对微服务而设计，另外一方面Spring Cloud是通过集成各种组件的方式来实现微服务，因此理论上可以集成目前业内的绝大多数的微服务相关组件，从而实现微服务的全部功能。</p><p>而对Dubbo而言，如果一定要应用到微服务的使用场景中的话，上表中欠缺的大多数功能都可以通过集成第三方应用和组件的方式来实现，跟Spring Cloud相比主要的缺陷在于集成过程中的便利性和兼容性等问题。</p><h2 id="Spring-Cloud与Docker"><a href="#Spring-Cloud与Docker" class="headerlink" title="Spring Cloud与Docker"></a>Spring Cloud与Docker</h2><p>虽然网上也有很多文章写到如何使用Docker来实现微服务，但是事实上单独使用Docker是没办法完整的实现微服务的所有功能的。在实际上微服务架构中，Spring Cloud和Docker更多的是一种协作的关系，而不是一种竞争的关系。通过Docker容器化技术，可以更好的解决引入Spring Cloud微服务后带来的部署和运维的复杂性。</p><p>Spring Cloud生态圈中的Pivotal Cloud Foundry（PCF）作为PaaS实现，也提供一些类似于Docker的功能支持，但是无论上功能上还是易用性上和Docker还是存在比较大的差异。Pivotal Cloud Foundry和Docker之间的关系更多的是一种兼容关系，而不是竞争关系，Pivotal Cloud Foundry的主要竞争对手是Red Hat的OpenShift。目前，Pivotal Cloud Foundry支持的IaaS包括：AWS、AZURE、GCP、vSphere、OpenStack等。</p><h2 id="Spring-Cloud与Kubernetes"><a href="#Spring-Cloud与Kubernetes" class="headerlink" title="Spring Cloud与Kubernetes"></a>Spring Cloud与Kubernetes</h2><p>网上也有一些“Spring Cloud与Kubernetes哪个更好”，“当已经有了Kubernetes之后，还需要使用Spring Cloud么”之类的文章。首先说笔者并不认为Spring Cloud与Kubernetes是竞争关系，但是也不否认二者确实在诸多功能上存在一些重合。下图是对Spring Cloud与Kubernetes在微服务架构中的一些基础功能上的对比：<br><img src="http://dockone.io/uploads/article/20180312/39e2b549e41cca439d701b654a269cca.jpg" alt=""></p><p>通过对比可以看出，Spring Cloud和Kubernetes确实存在一些功能上的重合，但是二者的定位其实差别很大。Spring Cloud是一个基于Java语言的微服务开发框架，而Kubernetes是一个针对容器应用的自动化部署、伸缩和管理的开源系统，它兼容多种语言且提供了创建、运行、伸缩以及管理分布式系统的原语。Spring Cloud更多的是面向有Spring开发经验的Java语言开发者，而Kubernetes不是一个针对开发者的平台，它的目的是供有DevOps思想的IT人员使用。</p><p>为了区分Spring Cloud和Kubernetes两个项目的范围，下面这张图列出了几乎是端到端的微服务架构需求，从最底层的硬件，到最上层的DevOps和自服务经验，并且列出了如何关联到Spring Cloud和Kubernetes平台。</p><p><img src="http://dockone.io/uploads/article/20180312/085f0c76c18d2a944178454d05feeba0.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过Spring Cloud、Docker和Kubernetes的组合，可以构建更加完整和强大的微服务架构程序。通过三者的整合，使用Spring Boot提供应用的打包，Docker和Kubernetes提供应用的部署和调度。Spring Cloud通过Hystrix线程池提供应用内的隔离，而Kubernetes通过资源、进程和命名空间来提供隔离。Spring Cloud为每个微服务提供健康终端，而Kubernetes执行健康检查，且把流量导到健康服务。Spring Cloud外部化配置并更新它们，而Kubernetes分发配置到每个微服务。</p><p><img src="http://dockone.io/uploads/article/20180312/24702123e0fb0f8aa498764fc27faf32.png" alt=""></p><p>对于一名开发人员或者架构师来说，想要精通微服务设计与开发，能够在大中型项目中应用微服务架构，单纯掌握Spring Cloud是远远不够的，Docker和Kubernetes等都是需要学习和掌握的内容。同时，由于采用微服务架构后带来了分布式的相关问题，对于分布式系统理论也必须有一定的了解。当然，最重要的还是对系统业务的深入理解，对整体业务进行合理的规划和拆分，才能真正行之有效的应用微服务架构，构建高效、健壮、灵活、可扩展的微服务应用。</p><h1 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h1><ul><li><a href="https://springcloud.cc" target="_blank" rel="noopener">https://springcloud.cc</a></li><li><a href="https://www.kubernetes.org.cn" target="_blank" rel="noopener">https://www.kubernetes.org.cn</a></li><li><a href="https://my.oschina.net/u/3677020/blog/1570248" target="_blank" rel="noopener">https://my.oschina.net/u/3677020/blog/1570248</a></li><li><a href="http://blog.csdn.net/rickiyeat/article/details/60792925" target="_blank" rel="noopener">http://blog.csdn.net/rickiyeat/article/details/60792925</a></li><li><a href="http://www.uml.org.cn/wfw/201711271.asp" target="_blank" rel="noopener">http://www.uml.org.cn/wfw/201711271.asp</a></li><li><a href="https://projects.spring.io/spring-cloud/" target="_blank" rel="noopener">https://projects.spring.io/spring-cloud/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作为本系列文章的第二篇(第一篇链接请戳：&lt;a href=&quot;https://cloudpanl.github.io/2018/03/19/dubbo-past-and-present.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;浅谈服务治理、微服
      
    
    </summary>
    
      <category term="微服务" scheme="http://yoursite.com/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    
      <category term="Spring Cloud" scheme="http://yoursite.com/tags/Spring-Cloud/"/>
    
  </entry>
  
  <entry>
    <title>浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生【转载】</title>
    <link href="http://yoursite.com/2018/03/19/dubbo-past-and-present.html"/>
    <id>http://yoursite.com/2018/03/19/dubbo-past-and-present.html</id>
    <published>2018-03-19T09:44:57.000Z</published>
    <updated>2018-03-19T14:03:11.860Z</updated>
    
    <content type="html"><![CDATA[<p>本系列文章将为大家介绍当下最流行的服务治理、微服务等相关内容，从服务治理、SOA、微服务到最新的服务网格（Service Mesh）进行综合介绍和分析。易商阜极自2017年便积极引进微服务的先进理念，运用在项目实践中，为项目集成带来了显著效果。本文将以Dubbo为例，向为大家介绍SOA、服务治理等概念，以及Dubbo的基础知识和最新发展情况。</p><h1 id="SOA与服务治理"><a href="#SOA与服务治理" class="headerlink" title="SOA与服务治理"></a>SOA与服务治理</h1><p>SOA（面向服务的体系结构）概念由来已久，在10多年前便开始进入到我们广大软件开发者的视线中。SOA是一种粗粒度、松耦合服务架构，服务之间通过简单、精确定义接口进行通讯，不涉及底层编程接口和通讯模型。SOA可以看作是B/S模型、Web Service技术之后的自然延伸。</p><p>服务治理，也称为SOA治理，是指用来管理SOA的采用和实现的过程。以下是在2006年时IBM对于服务治理要点的总结：</p><ul><li>服务定义（服务的范围、接口和边界）</li><li>服务部署生命周期（各个生命周期阶段）</li><li>服务版本治理（包括兼容性）</li><li>服务迁移（启用和退役）</li><li>服务注册中心（依赖关系）</li><li>服务消息模型（规范数据模型）</li><li>服务监视（进行问题确定）</li><li>服务所有权（企业组织）</li><li>服务测试（重复测试）</li><li>服务安全（包括可接受的保护范围）</li></ul><p>限于当时的技术发展水平，广大软件设计与开发人员对于SOA和服务治理的技术认知还主要停留在Web Service和ESB总线等技术和规范上，并没有真正在软件开发中得以充分落地。</p><h1 id="Dubbo开源"><a href="#Dubbo开源" class="headerlink" title="Dubbo开源"></a>Dubbo开源</h1><p>直到2011年10月27日，阿里巴巴开源了自己的SOA服务化治理方案的核心框架Dubbo，服务治理和SOA的设计理念开始逐渐在国内软件行业中落地，并被广泛应用。<br>Dubbo作为阿里巴巴内部的SOA服务化治理方案的核心框架，在2012年时已经每天为2000+个服务提供3,000,000,000+次访问量支持，并被广泛应用于阿里巴巴集团的各成员站点。Dubbo自2011年开源后，已被许多非阿里系公司使用，其中既有当当网、网易考拉等互联网公司，也有中国人寿、青岛海尔等传统企业。</p><h1 id="Dubbo简介"><a href="#Dubbo简介" class="headerlink" title="Dubbo简介"></a>Dubbo简介</h1><p>Dubbo是一个高性能服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案，使得应用可通过高性能RPC实现服务的输出和输入功能，和Spring框架可以无缝集成。</p><p>作为一个分布式服务框架，以及SOA治理方案，Dubbo其功能主要包括：高性能NIO通讯及多协议集成，服务动态寻址与路由，软负载均衡与容错，依赖分析与服务降级等。Dubbo最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。</p><p>Dubbo包含远程通讯、集群容错和自动发现三个核心部分。提供透明化的远程方法调用，实现像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入。同时具备软负载均衡及容错机制，可在内网替代F5等硬件负载均衡器，降低成本，减少单点。可以实现服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者。</p><p>下图来自从Dubbo官网，描述了服务注册中心、服务提供方、服务消费方、服务监控中心之间的调用关系，具体如下图所示：<br><img src="http://dockone.io/uploads/article/20180124/11a8ad251bf0ae6577eead936add06e7.png" alt=""></p><p>节点角色说明：<br><img src="http://dockone.io/uploads/article/20180124/2c69dfd83dd060315be04574dbcc48c9.png" alt=""></p><p>调用关系说明：</p><ol><li>服务容器负责启动，加载，运行服务提供者。</li><li>服务提供者在启动时，向注册中心注册自己提供的服务。</li><li>服务消费者在启动时，向注册中心订阅自己所需的服务。</li><li>注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。</li><li>服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。</li><li>服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。</li></ol><h1 id="Dubbo总体架构"><a href="#Dubbo总体架构" class="headerlink" title="Dubbo总体架构"></a>Dubbo总体架构</h1><p>Dubbo框架设计共划分了10层，最上面的Service层是留给实际使用Dubbo开发分布式服务的开发者实现业务逻辑的接口层。图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。</p><p><img src="http://dockone.io/uploads/article/20180124/8349c5fc87fef048bb26ea6b277eee4d.png" alt=""></p><p>各层说明：</p><ul><li>Config配置层：对外配置接口，以ServiceConfig、ReferenceConfig为中心，可以直接初始化配置类，也可以通过Spring解析配置生成配置类。</li><li>Proxy服务代理层：服务接口透明代理，生成服务的客户端Stub和服务器端Skeleton，以ServiceProxy为中心，扩展接口为ProxyFactory。</li><li>Registry注册中心层：封装服务地址的注册与发现，以服务URL为中心，扩展接口为RegistryFactory、Registry、RegistryService。</li><li>Cluster路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以Invoker为中心，扩展接口为Cluster、Directory、Router、LoadBalance。</li><li>Monitor监控层：RPC调用次数和调用时间监控，以Statistics为中心，扩展接口为MonitorFactory、Monitor、MonitorService。</li><li>Protocol远程调用层：封将RPC调用，以Invocation、Result为中心，扩展接口为Protocol、Invoker、Exporter。</li><li>Exchange信息交换层：封装请求响应模式，同步转异步，以Request、Response为中心，扩展接口为Exchanger、ExchangeChannel、ExchangeClient、ExchangeServer。</li><li>Transport网络传输层：抽象MINA和Netty为统一接口，以Message为中心，扩展接口为Channel、Transporter、Client、Server、Codec。</li><li>Serialize数据序列化层：可复用的一些工具，扩展接口为Serialization、ObjectInput、ObjectOutput、ThreadPool。</li></ul><h1 id="模块分包"><a href="#模块分包" class="headerlink" title="模块分包"></a>模块分包</h1><p><img src="http://dockone.io/uploads/article/20180124/3b191f3699ee69601a87bb6f0864f18f.png" alt=""></p><p>各模块说明：</p><ul><li>dubbo-common公共逻辑模块：包括Util类和通用模型。</li><li>dubbo-remoting远程通讯模块：相当于Dubbo协议的实现，如果RPC用 RMI协议则不需要使用此包。</li><li>dubbo-rpc远程调用模块：抽象各种协议，以及动态代理，只包含一对一的调用，不关心集群的管理。</li><li>dubbo-cluster集群模块：将多个服务提供方伪装为一个提供方，包括：负载均衡、容错、路由等，集群的地址列表可以是静态配置的，也可以是由注册中心下发。</li><li>dubbo-registry注册中心模块：基于注册中心下发地址的集群方式，以及对各种注册中心的抽象。</li><li>dubbo-monitor监控模块：统计服务调用次数、调用时间的、调用链跟踪的服务。</li><li>dubbo-config配置模块：是Dubbo对外的API，用户通过Config使用Dubbo，隐藏Dubbo所有细节。</li><li>dubbo-container容器模块：是一个Standlone的容器，以简单的Main加载Spring启动，因为服务通常不需要Tomcat/JBoss等Web容器的特性，没必要用Web容器去加载服务。</li></ul><h1 id="协议支持"><a href="#协议支持" class="headerlink" title="协议支持"></a>协议支持</h1><ul><li>Dubbo协议（默认协议）</li><li>Hessian协议</li><li>HTTP协议</li><li>RMI协议</li><li>WebService协议</li><li>Thrift协议</li><li>Memcached协议</li><li>Redis协议</li></ul><h1 id="注册中心"><a href="#注册中心" class="headerlink" title="注册中心"></a>注册中心</h1><h2 id="（1）Multicast注册中心："><a href="#（1）Multicast注册中心：" class="headerlink" title="（1）Multicast注册中心："></a>（1）Multicast注册中心：</h2><p>Multicast注册中心不需要启动任何中心节点，只要广播地址一样，就可以互相发现。组播受网络结构限制，只适合小规模应用或开发阶段使用。组播地址段：224.0.0.0 - 239.255.255.255。</p><h2 id="（2）ZooKeeper注册中心（推荐）："><a href="#（2）ZooKeeper注册中心（推荐）：" class="headerlink" title="（2）ZooKeeper注册中心（推荐）："></a>（2）ZooKeeper注册中心（推荐）：</h2><p>ZooKeeper是Apacahe子项目，是一个树型的目录服务，支持变更推送，适合作为Dubbo服务的注册中心，可用于生产环境。</p><p><img src="http://dockone.io/uploads/article/20180124/fe00d79623628e628f64e13bba33616c.png" alt=""></p><p>对上图流程说明如下：</p><ol><li>服务提供者（Provider）启动时，向/dubbo/com.foo.BarService/providers目录下写入URL。</li><li>服务消费者（Consumer）启动时，订阅/dubbo/com.foo.BarService/providers目录下的URL，向/dubbo/com.foo.BarService/consumers目录下写入自己的URL。</li><li>监控中心（Monitor）启动时，订阅/dubbo/com.foo.BarService目录下的所有提供者和消费者URL。</li></ol><h2 id="（3）Redis注册中心："><a href="#（3）Redis注册中心：" class="headerlink" title="（3）Redis注册中心："></a>（3）Redis注册中心：</h2><p>阿里内部并没有采用Redis做为注册中心，而是使用自己实现的基于数据库的注册中心，即：Redis注册中心并没有在阿里内部长时间运行的可靠性保障，此Redis桥接实现只为开源版本提供，其可靠性依赖于Redis本身的可靠性。</p><h2 id="（4）Simple注册中心："><a href="#（4）Simple注册中心：" class="headerlink" title="（4）Simple注册中心："></a>（4）Simple注册中心：</h2><p>Simple注册中心本身就是一个普通的Dubbo服务，可以减少第三方依赖，使整体通讯方式一致。只是简单实现，不支持集群，可作为自定义注册中心的参考，但不适合直接用于生产环境。</p><h1 id="远程通信与信息交换"><a href="#远程通信与信息交换" class="headerlink" title="远程通信与信息交换"></a>远程通信与信息交换</h1><p>远程通信需要指定通信双方所约定的协议，在保证通信双方理解协议语义的基础上，还要保证高效、稳定的消息传输。Dubbo继承了当前主流的网络通信框架，主要包括如下几个：</p><ul><li>Mina</li><li>Netty（默认）</li><li>Grizzly</li></ul><h1 id="停止维护"><a href="#停止维护" class="headerlink" title="停止维护"></a>停止维护</h1><p>从2012年10月23日Dubbo 2.5.3发布后，在Dubbo开源将满一周年之际，阿里基本停止了对Dubbo的主要升级。只在之后的2013年和2014年更新过2次对Dubbo 2.4的维护版本，然后停止了所有维护工作。Dubbo对Srping的支持也停留在了Spring 2.5.6版本上。</p><h1 id="分支出现"><a href="#分支出现" class="headerlink" title="分支出现"></a>分支出现</h1><p>在阿里停止维护和升级Dubbo期间，当当网开始维护自己的Dubbo分支版本Dubbox，支持了新版本的Spring，并对外开源了Dubbox。同时，网易考拉也维护了自己的独立分支Dubbok，可惜并未对外开源。</p><h1 id="重获新生"><a href="#重获新生" class="headerlink" title="重获新生"></a>重获新生</h1><p>经过多年漫长的等待，随着微服务的火热兴起，在国内外开发者对阿里不再升级维护Dubbo的吐槽声中，阿里终于开始重新对Dubbo的升级和维护工作。在2017年9月7日 ，阿里发布了Dubbo的2.5.4版本，距离上一个版本2.5.3发布已经接近快5年时间了。在随后的几个月中，阿里Dubbo开发团队以差不多每月一版本的速度开始快速升级迭代，修补了Dubbo老版本多年来存在的诸多bug，并对Spring等组件的支持进行了全面升级。</p><h1 id="分支合并"><a href="#分支合并" class="headerlink" title="分支合并"></a>分支合并</h1><p>在2018年1月8日，Dubbo 2.6.0版本发布，新版本将之前当当网开源的Dubbo分支Dubbox进行了合并，实现了Dubbo版本的统一整合。</p><h1 id="Dubbo与Spring-Cloud"><a href="#Dubbo与Spring-Cloud" class="headerlink" title="Dubbo与Spring Cloud"></a>Dubbo与Spring Cloud</h1><p>阿里巴巴负责主导了 Dubbo 重启维护的研发工程师刘军在接受采访时表示：当前由于 RPC 协议、注册中心元数据不匹配等问题，在面临微服务基础框架选型时Dubbo与Spring Cloud是只能二选一，这也是为什么大家总是拿Dubbo和Spring Cloud做对比的原因之一。Dubbo之后会积极寻求适配到Spring Cloud生态，比如作为Spring Cloud的二进制通信方案来发挥Dubbo的性能优势，或者Dubbo通过模块化以及对http的支持适配到Spring Cloud。</p><h1 id="未来展望"><a href="#未来展望" class="headerlink" title="未来展望"></a>未来展望</h1><p>2018年1月8日，Dubbo创始人之一梁飞在Dubbo交流群里透露了Dubbo 3.0正在动工的消息。Dubbo 3.0内核与Dubbo 2.0完全不同，但兼容Dubbo 2.0。Dubbo 3.0将以Streaming为内核，不再是Dubbo时代的RPC，但是RPC会在Dubbo 3.0中变成远程Streaming对接的一种可选形态。Dubbo 3.0将支持可选Service Mesh，多加一层IPC，这主要是为了兼容老系统，而内部则会优先尝试内嵌模式。代理模式Ops可独立升级框架，减少业务侵入，而内嵌模式可以带业务测试、部署节点少、稳定性检测方便。同时，可以将Dubbo 3.0启动为独立进程，由dubbo-mesh进行IPC，路由、负载均衡和熔断机制将由独立进程控制。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>从 Dubbo 新版本的路线规划上可以看出，新版本的Dubbo在原有服务治理的功能基础上，将全面拥抱微服务和Service Mesh。同时，考虑到在阿里云已经有了Dubbo的商业版本，在未来一段时间内，Dubbo的更新与维护应该不会再长时间中断。在我们进行服务治理以及微服务架构设计时，新版本Dubbo对我们广大开发者来说都将会是一个不错的选择。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="http://dubbo.io" target="_blank" rel="noopener">http://dubbo.io</a></li><li><a href="https://github.com/alibaba/dubbo" target="_blank" rel="noopener">https://github.com/alibaba/dubbo</a></li><li><a href="http://shiyanjun.cn/archives/325.html" target="_blank" rel="noopener">http://shiyanjun.cn/archives/325.html</a></li><li><a href="http://mp.weixin.qq.com/s/eVYx-tUIMYtAk5wP-qkdkw" target="_blank" rel="noopener">http://mp.weixin.qq.com/s/eVYx-tUIMYtAk5wP-qkdkw</a></li></ul><p>原文链接：<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDM3NDY1NQ==&amp;mid=2651112485&amp;idx=2&amp;sn=584285b2c52e59a4957bf50079ce41e4&amp;chksm=bdb5abb58ac222a3b1a62ae9d40a66a14e05b9bc101f50a30abfcf8e38a8e0a6ac233e985a8c&amp;mpshare=1&amp;scene=1&amp;srcid=0124lduJyZ5omV9YJHbcTVSk&amp;pass_ticket=zauggU5Vx7VPre9Q8e8%2BSi0sDlvU6Gr3Jg%2F%2BrZ9BVX5EtWQ9Mf%2B%2Bch2jYuJEjOeL#rd" target="_blank" rel="noopener">浅谈服务治理、微服务与Service Mesh（一）Dubbo的前世今生</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本系列文章将为大家介绍当下最流行的服务治理、微服务等相关内容，从服务治理、SOA、微服务到最新的服务网格（Service Mesh）进行综合介绍和分析。易商阜极自2017年便积极引进微服务的先进理念，运用在项目实践中，为项目集成带来了显著效果。本文将以Dubbo为例，向为大
      
    
    </summary>
    
      <category term="微服务" scheme="http://yoursite.com/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
    
      <category term="Dubbo,Service Mesh,微服务,ZooKeeper" scheme="http://yoursite.com/tags/Dubbo-Service-Mesh-%E5%BE%AE%E6%9C%8D%E5%8A%A1-ZooKeeper/"/>
    
  </entry>
  
  <entry>
    <title>如何选择 Linux 上的跟踪器</title>
    <link href="http://yoursite.com/2018/03/18/choosing-a-linux-tracer.html"/>
    <id>http://yoursite.com/2018/03/18/choosing-a-linux-tracer.html</id>
    <published>2018-03-18T14:38:01.000Z</published>
    <updated>2018-03-18T15:28:12.985Z</updated>
    
    <content type="html"><![CDATA[<p>tracer是一个高级的性能分析和诊断工具，但是不要让这名词唬住你，如果你使用过 strace 和 tcpdump，其实你就已经使用过跟踪器了。系统跟踪器可以获取更多的系统调用和数据包。它们通常能跟踪任意的内核和应用程序。</p><p>有太多的 Linux 跟踪器可以选择。每一种都有其官方的（或非官方的）的卡通的独角兽吉祥物，足够撑起一台”儿童剧”了。</p><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521384181/choosing-a-linux-tracer-1.jpg" alt=""></p><p>那么我们应该使用哪个跟踪器呢？</p><p>可分为两类：大多数人和性能/内核工程师。</p><h1 id="对于大多数人"><a href="#对于大多数人" class="headerlink" title="对于大多数人"></a>对于大多数人</h1><p>大多数人 (开发者，系统管理员，开发管理者，运维人员，评测人员，等等) 不关心系统追踪器的细节。下面是对于追踪器你应该知道和做的：</p><h2 id="1-使用-perf-events-分析-CPU-性能"><a href="#1-使用-perf-events-分析-CPU-性能" class="headerlink" title="1. 使用 perf_events 分析 CPU 性能"></a>1. 使用 perf_events 分析 CPU 性能</h2><p>使用 perf_events 做 CPU 性能分析。性能指标可以使用 <a href="http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html" target="_blank" rel="noopener">flame graph</a> 等工具做可视化。</p><pre><code>git clone --depth 1 https://github.com/brendangregg/FlameGraphperf record -F 99 -a -g -- sleep 30perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; perf.svg</code></pre><p>Linux perf_events (又称 “perf”，同命令名) 是 Linux 用户的官方跟踪器和性能分析器。内置于内核代码，有很好维护（近来获得快速增强），通常通过 linux-tools-common 软件包安装。</p><p>perf 有很多功能，如果只能推荐一个，我选择 CPU 性能分析。尽管这只是采样，而不是从技术上追踪事件。最难的部分是获取完整的栈和信息，我为 java 和 node.js 做的一个演讲 <a href="http://www.brendangregg.com/blog/2015-02-27/linux-profiling-at-netflix.html" target="_blank" rel="noopener">Linux Profiling at Netflix</a> 中已经说过这个问题。</p><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521385307/choosing-a-linux-tracer-2.png" alt=""></p><h2 id="2-了解其他的跟踪器"><a href="#2-了解其他的跟踪器" class="headerlink" title="2. 了解其他的跟踪器"></a>2. 了解其他的跟踪器</h2><p>正如我一个朋友说的：“你不需要知道如何操作 X 射线机器，但是一旦你吞了一枚硬币，你得知道这得去做 X 射线”，你应该了解各种跟踪器都能做什么，这样就能在你工作中真正需要跟踪器的时候，你既可以选择稍后学习使用，也可以雇相应的人来完成。</p><p>简短来说：几乎所有的东西都可以使用跟踪器来进行分析和跟踪。如，文件系统内部、TCP/IP 过程、设备驱动、应用程序内部。可以看一下我的个人网站上关于 <a href="http://lwn.net/Articles/608497/" target="_blank" rel="noopener">ftrace</a> 的文章，还有我写的关于 <a href="http://www.brendangregg.com/perf.html" target="_blank" rel="noopener">perf_events</a> 文档介绍，可以做为一个追踪（或者性能分析）的例子。</p><h2 id="3-寻求前端支持工具"><a href="#3-寻求前端支持工具" class="headerlink" title="3. 寻求前端支持工具"></a>3. 寻求前端支持工具</h2><p>如果你正想买一个能支持跟踪 Linux 的性能分析工具（有许多卖这类工具的公司）。想像一下，只需要直接点击一下界面就能“洞察”整个系统内核，包括隐藏的不同堆栈位置的热图，我在 <a href="http://www.brendangregg.com/blog/2015-06-23/netflix-instance-analysis-requirements.html" target="_blank" rel="noopener">Monitorama talk</a> 中介绍了一个这样带图形界面的工具。</p><p>我开源了一些我自己开发的前端工具，尽管只是命令行界面而不是图形界面。这些工具也会让人们更加快速容易的使用跟踪器。比如下面的例子，用我的 perf_tool，跟踪一个新进程:</p><pre><code># ./execsnoopTracing exec()s. Ctrl-C to end.   PID   PPID ARGS 22898  22004 man ls 22905  22898 preconv -e UTF-8 22908  22898 pager -s 22907  22898 nroff -mandoc -rLL=164n -rLT=164n -Tutf8[...]</code></pre><p>在 Netflix 上，我们创建了一个 <a href="http://techblog.netflix.com/2015/04/introducing-vector-netflixs-on-host.html" target="_blank" rel="noopener">Vector</a>，一个分析工具的实例，同时也是 Linux 上的跟踪器的最终前端。</p><h1 id="对于性能-内核工程师"><a href="#对于性能-内核工程师" class="headerlink" title="对于性能/内核工程师"></a>对于性能/内核工程师</h1><p>我们的工作变的越来越困难，很多的人会问我们怎么样去追踪，哪种跟踪器可以用！为了正确理解一个跟踪器，你经常需要花上至少100个小时才能做到。理解所有的 linux 跟踪器去做出理性的选择是一个浩大的工程。（我可能是唯一一个快做到这件事情的人）</p><p><img src="https://res.cloudinary.com/cloudpanl/image/upload/v1521385735/choosing-a-linux-tracer-3.jpg" alt=""></p><p>这里是我的建议，可以二选其一：</p><p>A) 选中一个全能的跟踪器，并且使它标准化，这将涉及花费大量的时间去弄清楚它在测试环境中的细微差别和安全性。我现在推荐 SystemTap 的最新版本（可以从源代码构建）。我知道有些公司已经选用 LTTng，而且他们用的很好，尽管它不是非常的强大（虽然它更安全）。如果 Sysdig 可以增加追踪点tracepoint或者 kprobes，可以做为另一个候选。 </p><p>B) 遵循我上面提供的流程图，它将意味着尽可能更多的使用 ftrace 或者 perf_event， 并整合 eBPF，之后其他的跟踪器像 SystemTap/LTTng 会去填补剩下的空白。 这就是我目前在 Netflix 做的工作。</p><h1 id="对跟踪器的评价"><a href="#对跟踪器的评价" class="headerlink" title="对跟踪器的评价"></a>对跟踪器的评价</h1><h2 id="1-ftrace"><a href="#1-ftrace" class="headerlink" title="1. ftrace"></a>1. ftrace</h2><p>我喜欢用 ftrace，它是内核 hacker 的首选，内置于系统内核，可以使用跟踪点（静态检查点），能调用内核 kprobes 和 uprobes 调试工具。并且提供几个这样的功能：带可选过滤器和参数的事件追踪功能；在内核中进行统计的事件计数和定时功能；还有函数流程遍历的功能。可以看一下内核代码中 <a href="https://www.kernel.org/doc/Documentation/trace/ftrace.txt" target="_blank" rel="noopener">ftrace.txt</a> 例子了解一下。ftrace 由 /sys 控制，仅支持单一的 root 用户使用（但是你可以通过缓冲区实例改成支持多用户）。某些时候 ftrace 的操作界面非常繁琐，但是的确非常“hack”，而且它有前端界面。ftace 的主要作者 Steven Rostedt 创建了 trace-cmd 命令工具，而我创建了 perf 的工具集。我对这个工具最大的不满就是它不可编程。举例来说，你不能保存和获取时间戳，不能计算延迟，不能把这些计算结果保存成直方图的形式。你需要转储事件至用户层，并且花一些时间去处理结果。ftrace 可以通过 eBPF 变成可编程的。</p><h2 id="2-perf-events"><a href="#2-perf-events" class="headerlink" title="2. perf_events"></a>2. perf_events</h2><p>perf_events 是 Linux 用户的主要跟踪工具，它内置在内核源码中，通常通过 linux-tools-commom 安装。也称为“perf”，即其前端工具名称，它通常用来跟踪和转储信息到一个叫做 perf.data 的文件中，perf.data 文件相当于一个动态的缓冲区，用来保存之后需要处理的结果。ftrace 能做到的，perf_events 大都也可以做到，perf-events 不能做函数流程遍历，少了一点儿“hack”劲儿（但是对于安全/错误检查有更好的支持）。它可以进行 CPU 分析和性能统计，用户级堆栈解析，也可以使用对于跟踪每行局部变量产生的调试信息。它也支持多用户并发操作。和 ftrace 一样也不支持可编程。如果要我只推荐一款跟踪器，那一定是 perf 了。它能解决众多问题，并且它相对较安全。</p><h2 id="3-eBPF"><a href="#3-eBPF" class="headerlink" title="3. eBPF"></a>3. eBPF</h2><p>extended Berkeley Packet Filter（eBPF）是一个可以在事件上运行程序的高效内核虚拟机（JIT）。它可能最终会提供 ftrace 和 perf_events 的内核编程，并强化其他的跟踪器。这是 Alexei Starovoitov 目前正在开发的，还没有完全集成，但是从4.1开始已经对一些优秀的工具有足够的内核支持了，如块设备 I/O 的延迟热图。可参考其主要作者 Alexei Starovoitov 的 <a href="http://www.phoronix.com/scan.php?page=news_item&amp;px=BPF-Understanding-Kernel-VM" target="_blank" rel="noopener">BPF slides</a> 和 <a href="https://github.com/torvalds/linux/tree/master/samples/bpf" target="_blank" rel="noopener">eBPF samples</a>。</p><h2 id="4-SystemTap"><a href="#4-SystemTap" class="headerlink" title="4. SystemTap"></a>4. SystemTap</h2><p><a href="https://sourceware.org/systemtap/wiki" target="_blank" rel="noopener">SystemTap</a> 是最强大的跟踪器。它能做所有事情，如概要分析，跟踪点，探针，uprobes（来自SystemTap），USDT 和内核编程等。它将程序编译为内核模块，然后加载，这是一种获取安全的巧妙做法。它也是从 tree 发展而来，过去有很多问题（崩溃或冻结）。很多不是 SystemTap 本身的错——它常常是第一个使用某个内核追踪功能，也是第一个碰到 bug 的。SystemTap 的最新版本好多了（必须由源代码编译），但是很多人仍然会被早期版本吓到。如果你想用它，可先在测试环境中使用，并与 irc.freenode.net 上 的 #systemtap 开发人员交流。（Netflix 有容错机制，我们已经使用了 SystemTap，但是可能我们考虑的安全方面的问题比你们少。）我最大的不满是，它似乎认为你应该有内核 debug 信息，但是经常没有。实际上没有它也能做很多事情，但是缺少文档和例子（我必须自己全靠自己开始学习）。</p><h2 id="5-LTTng"><a href="#5-LTTng" class="headerlink" title="5. LTTng"></a>5. LTTng</h2><p>LTTng 优化了事件采集，这比其他跟踪器做得好，它也支持几种事件类型，包括 USTD。它从 tree 发展而来，它的核心很简单：通过一组小规模的固定指令集将事件写入追踪缓冲区，这种方式使它安全、快速，缺点是它没有内核编码的简单途径。我一直听说这不是一个大问题，因为尽管需要后期处理，它也已经优化到可以充分的度量。此外，它还首创了一个不同的分析技术，对所有关注事件的更多黑盒记录将能够稍后以 GUI 的方式进行研究。我关心的是前期没有考虑到要录制的事件缺失问题如何解决，但我真正要做的是花更多时间来看它在实践中用的怎么样。这是我花的时间最少的一个跟踪器（没有什么特殊原因）。</p><h2 id="6-Ktap"><a href="#6-Ktap" class="headerlink" title="6. Ktap"></a>6. Ktap</h2><p>ktap 是一款前景很好的跟踪器，它使用内核中的 lua 虚拟机处理，在没有调试信息的情况下在嵌入式设备上运行的很好。这让它得到了关注，并在有一段时间似乎超过了 Linux 上所有的追踪器。然后 eBPF 开始集成到内核了，而 ktap 的集成会在可以使用 eBPF 替代它自己的虚拟机后才开始。因为 eBPF 仍将持续集成几个月，ktap 开发者要继续等上一段时间。我希望今年晚些时候它能重新开发。</p><h2 id="7-dtrace4linux"><a href="#7-dtrace4linux" class="headerlink" title="7. dtrace4linux"></a>7. dtrace4linux</h2><p><a href="http://www.oschina.net/p/dtrace4linux" target="_blank" rel="noopener">dtrace4linux</a> 主要是 Paul Fox 一个人在业余时间完成的，它是 Sun DTrace 的 Linux 版本。它引人瞩目，已经有一些供应器provider可以工作，但是从某种程度上来说还不完整，更多的是一种实验性的工具（不安全）。我认为，顾忌到许可证问题，人们会小心翼翼的为 dtrace4linux 贡献代码：由于当年 Sun 开源DTrace 使用的是 CDDL 协议，而 dtrace4linux 也不大可能最终进入 Linux kernel。Paul 的方法很可能会使其成为一个 add-on。我很乐意看到 Linux 平台上的 DTrace 和这个项目的完成，我认为当我加入 Netflix 后将会花些时间来协助完成这个项目。然而，我还是要继续使用内置的跟踪器，如 ftrace 和 perf_events。</p><h2 id="8-OL-DTrace"><a href="#8-OL-DTrace" class="headerlink" title="8. OL DTrace"></a>8. OL DTrace</h2><p><a href="http://docs.oracle.com/cd/E37670_01/E38608/html/index.html" target="_blank" rel="noopener">Oracle Linux DTrace</a> 为了将 DTrace 引入 Linux，特别是为 Oracle Linux，做出了很大的努力。这些年来发布的多个版本表明了它的稳定进展。开发者们以一种对这个项目的前景看好的态度谈论着改进 DTrace 测试套件。很多有用的 供应器provider 已经完成了，如：syscall, profile, sdt, proc, sched 以及 USDT。我很期待 fbt（function boundary tracing，用于内核动态跟踪）的完成，它是 Linux 内核上非常棒的 供应器provider。OL DTrace 最终的成功将取决于人们对运行 Oracle Linux（为技术支持付费）有多大兴趣，另一方面取决于它是否完全开源：它的内核元件是开源的，而我没有看到它的用户级别代码。</p><h2 id="9-sysdig"><a href="#9-sysdig" class="headerlink" title="9. sysdig"></a>9. sysdig</h2><p><a href="http://www.sysdig.org/" target="_blank" rel="noopener">sysdig</a> 是一个使用类 tcpdump 语法在系统事件上操作的新跟踪器，它使用 lua 进行后期处理。它很优秀，它见证了系统跟踪领域的变革。它的局限性在于它只在当前进行系统调用，将所有事件转储为用户级别用于后期处理。你可以使用系统调用做很多事情，然而我还是很希望它能支持跟踪点、kprobe 和 uprobe。我还期待它能支持 eBPF 做内核摘要。目前，sysdig 开发者正在增加容器支持。留意这些内容。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;tracer是一个高级的性能分析和诊断工具，但是不要让这名词唬住你，如果你使用过 strace 和 tcpdump，其实你就已经使用过跟踪器了。系统跟踪器可以获取更多的系统调用和数据包。它们通常能跟踪任意的内核和应用程序。&lt;/p&gt;
&lt;p&gt;有太多的 Linux 跟踪器可以选择
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
</feed>
